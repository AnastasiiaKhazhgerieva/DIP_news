{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata, drive\n",
        "from datetime import date, timedelta, datetime\n",
        "from typing import List\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}"
      ],
      "metadata": {
        "id": "MRr5ESlGgEWp"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Functions for scrapping\n",
        "\n",
        "## Defining and formatting dates\n",
        "def get_last_dates(n_days=6, end_date=None):\n",
        "    if end_date is None:\n",
        "        end_date = date.today()\n",
        "    return [end_date - timedelta(days=offset) for offset in range(n_days, -1, -1)]\n",
        "\n",
        "def format_dates(dates_list, fmt=\"%Y-%m-%d\"):\n",
        "    return [d.strftime(fmt) for d in dates_list]\n",
        "\n",
        "## Getting web page soup\n",
        "def get_page_soup(url, headers=HEADERS, timeout=10):\n",
        "    resp = requests.get(url, headers=headers, timeout=timeout)\n",
        "    resp.raise_for_status()\n",
        "    return BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "## Scrapers: Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat\n",
        "\n",
        "# Kommersant scraper\n",
        "def fetch_kom(rubrics, dates, output_file,\n",
        "              base_url_template=\"https://www.kommersant.ru/archive/rubric/{rubric}/day/{date}\"):\n",
        "    results = {}\n",
        "    for rubric in rubrics:\n",
        "        daily = {}\n",
        "        for dt in dates:\n",
        "            url = base_url_template.format(rubric=rubric, date=dt)\n",
        "            print(f\"Fetching Kommersant: {url}\")\n",
        "            try:\n",
        "                soup = get_page_soup(url)\n",
        "                scripts = soup.find_all(\"script\", type=\"application/ld+json\")\n",
        "                items = []\n",
        "                for script in scripts:\n",
        "                    raw = script.string\n",
        "                    if not raw:\n",
        "                        continue\n",
        "                    try:\n",
        "                        data = json.loads(raw)\n",
        "                    except json.JSONDecodeError:\n",
        "                        continue\n",
        "                    for entry in data.get(\"itemListElement\", []):\n",
        "                        title = entry.get(\"name\") or entry.get(\"headline\")\n",
        "                        link = entry.get(\"url\")\n",
        "                        if title and link and link not in {i['url'] for i in items}:\n",
        "                            items.append({\"title\": title, \"url\": link})\n",
        "                daily[dt] = items\n",
        "            except Exception as e:\n",
        "                daily[dt] = f\"[ERROR] {e}\"\n",
        "        results[rubric] = daily\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Saved Kommersant data to {output_file}\")\n",
        "\n",
        "\n",
        "# Vedomosti scraper\n",
        "def fetch_ved(dates, output_file,\n",
        "              base_url_template=\"https://www.vedomosti.ru/newspaper/{date}\"):\n",
        "    all_news = []\n",
        "    for dt in dates:\n",
        "        url = base_url_template.format(date=dt)\n",
        "        print(f\"Fetching Vedomosti: {url}\")\n",
        "        try:\n",
        "            soup = get_page_soup(url)\n",
        "            for item in soup.select(\"li.waterfall__item\"):\n",
        "                a = item.select_one(\"a.waterfall__item-title\")\n",
        "                if not a:\n",
        "                    continue\n",
        "                title = a.get_text(strip=True)\n",
        "                href = a.get(\"href\", \"\")\n",
        "                full_url = href if href.startswith(\"http\") else f\"https://www.vedomosti.ru{href}\"\n",
        "                all_news.append({\"date\": dt, \"title\": title, \"url\": full_url})\n",
        "        except Exception as e:\n",
        "            all_news.append({\"date\": dt, \"error\": str(e)})\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_news, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Saved Vedomosti data to {output_file}\")\n",
        "\n",
        "# RBC scraper\n",
        "\n",
        "def fetch_rbc(rubrics, dates, output_file, base_url_template=\"https://www.rbc.ru/{rubric}/?utm_source=topline\"):\n",
        "    # Mapping of Russian month names (genitive case) to month numbers\n",
        "    ru_months = {\n",
        "        'января': 1, 'февраля': 2, 'марта': 3, 'апреля': 4,\n",
        "        'мая': 5, 'июня': 6, 'июля': 7, 'августа': 8,\n",
        "        'сентября': 9, 'октября': 10, 'ноября': 11, 'декабря': 12\n",
        "    }\n",
        "    today = date.today()\n",
        "    all_news = []\n",
        "    for rubric in rubrics:\n",
        "        url = base_url_template.format(rubric=rubric)\n",
        "        print(f\"Fetching RBC, {rubric}: {url}\")\n",
        "        soup = get_page_soup(url)\n",
        "        # Find all news item containers (with schema.org NewsArticle)\n",
        "        for item in soup.find_all(attrs={\"itemscope\": True}):\n",
        "            itemtype = item.get(\"itemtype\", \"\")\n",
        "            if \"NewsArticle\" not in itemtype:\n",
        "                continue\n",
        "            name_meta = item.find(\"meta\", {\"itemprop\": \"name\"})\n",
        "            url_meta = item.find(\"meta\", {\"itemprop\": \"url\"})\n",
        "            date_span = item.find(\"span\", {\"class\": \"item__category\"})\n",
        "            if not name_meta or not url_meta or not date_span:\n",
        "                continue\n",
        "            title = name_meta.get(\"content\", \"\").strip()\n",
        "            url = url_meta.get(\"content\", \"\").strip()\n",
        "            date_text = date_span.get_text(strip=True)\n",
        "            if not title or not url or not date_text:\n",
        "                continue\n",
        "            # Parse date_text to datetime.date\n",
        "            news_date = None\n",
        "            if any(month in date_text for month in ru_months):\n",
        "                # Format like \"28 мая, 17:52\" (if not today)\n",
        "                date_part = date_text.split(\",\")[0].strip()  # e.g. \"28 мая\"\n",
        "                parts = date_part.split()\n",
        "                if len(parts) >= 2:\n",
        "                    try:\n",
        "                        day = int(parts[0])\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "                    month_name = parts[1].lower()\n",
        "                    if month_name not in ru_months:\n",
        "                        continue\n",
        "                    month = ru_months[month_name]\n",
        "                    year = today.year\n",
        "                    if len(parts) >= 3:\n",
        "                        # If year is present in date_text\n",
        "                        year_str = parts[2].replace(\"г.\", \"\").strip()\n",
        "                        if year_str.isdigit():\n",
        "                            year = int(year_str)\n",
        "                    try:\n",
        "                        news_date = date(year, month, day)\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "                    # Adjust year if the date is in the future (e.g., last year's news in early January)\n",
        "                    if news_date > today:\n",
        "                        news_date = date(year - 1, month, day)\n",
        "            else:\n",
        "                # Only time given (e.g. \"17:52\"), assume today's date\n",
        "                news_date = today\n",
        "            if news_date is None:\n",
        "                continue\n",
        "            all_news.append({\"title\": title, \"url\": url, \"date\": news_date})\n",
        "    # Filter news by allowed dates\n",
        "    filtered_news = [item for item in all_news if item[\"date\"] in dates]\n",
        "    # Remove duplicates by URL\n",
        "    unique_news = []\n",
        "    seen_urls = set()\n",
        "    for item in filtered_news:\n",
        "        if item[\"url\"] not in seen_urls:\n",
        "            unique_news.append(item)\n",
        "            seen_urls.add(item[\"url\"])\n",
        "    # Convert date objects to ISO format strings for JSON serialization\n",
        "    for item in unique_news:\n",
        "        if isinstance(item.get(\"date\"), date):\n",
        "            item[\"date\"] = item[\"date\"].isoformat()\n",
        "    # Save results to JSON file\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(unique_news, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Saved RBC data to {output_file}\")\n",
        "\n",
        "# Agro investor scraper\n",
        "\n",
        "def fetch_agro(dates, output_file, base_url_template=\"https://www.agroinvestor.ru/\"):\n",
        "    \"\"\"Fetch news from Agroinvestor main page and save to JSON.\"\"\"\n",
        "    print(\"Fetching Agroinvestor: https://www.agroinvestor.ru/\")\n",
        "    soup = get_page_soup(base_url_template)\n",
        "    news_list = []\n",
        "    seen_links = set()\n",
        "    # Mapping of Russian month names (in genitive case) to month numbers\n",
        "    ru_months = {\n",
        "        \"января\": 1, \"февраля\": 2, \"марта\": 3, \"апреля\": 4,\n",
        "        \"мая\": 5, \"июня\": 6, \"июля\": 7, \"августа\": 8,\n",
        "        \"сентября\": 9, \"октября\": 10, \"ноября\": 11, \"декабря\": 12\n",
        "    }\n",
        "    # Find all news items on the main page\n",
        "    for anchor in soup.find_all(\"a\", class_=\"news__item-desc\"):\n",
        "        title = anchor.get_text(strip=True)\n",
        "        href = anchor.get(\"href\")\n",
        "        if not href:\n",
        "            continue\n",
        "        # Construct full URL for the news article\n",
        "        url = urljoin(base_url_template, href.strip())\n",
        "        # Find the date of the news (in a <time> tag following the title link)\n",
        "        time_tag = anchor.find_next(\"time\")\n",
        "        if not time_tag:\n",
        "            continue\n",
        "        date_text = time_tag.get_text(strip=True).replace(\"\\xa0\", \" \")\n",
        "        if not date_text:\n",
        "            continue\n",
        "        # Parse the date text (e.g. \"30 мая 2025\") into a datetime.date object\n",
        "        try:\n",
        "            day_str, month_str, year_str = date_text.split()\n",
        "            day = int(day_str)\n",
        "            year = int(year_str)\n",
        "        except Exception:\n",
        "            # Skip if date format is unexpected\n",
        "            continue\n",
        "        month_str = month_str.lower()\n",
        "        if month_str not in ru_months:\n",
        "            continue\n",
        "        month = ru_months[month_str]\n",
        "\n",
        "        try: date_obj = datetime.date(year, month, day)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        # Filter news by allowed dates and avoid duplicates by URL\n",
        "        if date_obj in dates and url not in seen_links:\n",
        "            news_list.append({\n",
        "                \"title\": title,\n",
        "                \"link\": url,\n",
        "                \"date\": date_obj.isoformat()\n",
        "            })\n",
        "            seen_links.add(url)\n",
        "    # Save the result to a JSON file\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(news_list, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Saved Agroinvestor data to {output_file}\")\n",
        "\n",
        "# RG.ru scraper\n",
        "\n",
        "def fetch_rg(rubrics, dates, output_file,\n",
        "             base_url_template=\"https://rg.ru/tema/ekonomika/{rubric}\"):\n",
        "    all_news = []\n",
        "    for rubric in rubrics:\n",
        "        url = base_url_template.format(rubric=rubric)\n",
        "        print(f\"Fetching RG, {rubric}: {url}\")\n",
        "        soup = get_page_soup(url)\n",
        "        for title_span in soup.find_all(\"span\", class_=\"ItemOfListStandard_title__Ajjlf\"):\n",
        "            parent_a = title_span.find_parent(\"a\")\n",
        "            if not parent_a:\n",
        "                continue\n",
        "            href = parent_a.get(\"href\", \"\").strip()\n",
        "            if not href:\n",
        "                continue\n",
        "            full_url = href if href.startswith(\"http\") else f\"https://rg.ru{href}\"\n",
        "\n",
        "            date_a = title_span.find_previous(\"a\", class_=\"ItemOfListStandard_datetime__GstJi\")\n",
        "            if not date_a:\n",
        "                continue\n",
        "            date_href = date_a.get(\"href\", \"\").strip()\n",
        "            parts = date_href.strip(\"/\").split(\"/\")  # ['2025','05','30',...]\n",
        "            if len(parts) < 3:\n",
        "                continue\n",
        "            try:\n",
        "                y, m, d = map(int, parts[:3])\n",
        "                news_date = date(y, m, d)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "            if news_date not in dates:\n",
        "                continue\n",
        "\n",
        "            all_news.append({\n",
        "                \"title\": title_span.get_text(strip=True),\n",
        "                \"url\": full_url,\n",
        "                \"date\": news_date.isoformat()\n",
        "            })\n",
        "\n",
        "    unique = []\n",
        "    seen = set()\n",
        "    for item in all_news:\n",
        "        if item[\"url\"] not in seen:\n",
        "            seen.add(item[\"url\"])\n",
        "            unique.append(item)\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(unique, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Saved RG data to {output_file}\")\n",
        "\n",
        "# RIA scraper\n",
        "\n",
        "def fetch_ria(dates, output_file, base_url_template=\"https://ria.ru/economy/\"):\n",
        "    print(\"Fetching RIA: https://ria.ru/economy/\")\n",
        "    soup = get_page_soup(base_url_template)\n",
        "    collected = []\n",
        "\n",
        "    # Each news item has <a itemprop=\"url\" href=\"...\"></a>\n",
        "    for a in soup.find_all(\"a\", itemprop=\"url\"):\n",
        "        href = a.get(\"href\", \"\").strip()\n",
        "        if not href:\n",
        "            continue\n",
        "        full_url = href if href.startswith(\"http\") else f\"https://ria.ru{href}\"\n",
        "\n",
        "        # Next meta tag with itemprop=\"name\" holds the title\n",
        "        name_meta = a.find_next(\"meta\", itemprop=\"name\")\n",
        "        if not name_meta:\n",
        "            continue\n",
        "        title = name_meta.get(\"content\", \"\").strip()\n",
        "        if not title:\n",
        "            continue\n",
        "\n",
        "        # Extract date from the URL path: \"/YYYYMMDD/...\"\n",
        "        parsed = urlparse(full_url)\n",
        "        parts = parsed.path.lstrip(\"/\").split(\"/\")\n",
        "        if not parts or len(parts[0]) != 8 or not parts[0].isdigit():\n",
        "            continue\n",
        "        y, m, d = int(parts[0][:4]), int(parts[0][4:6]), int(parts[0][6:8])\n",
        "        try:\n",
        "            news_date = date(y, m, d)\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "        # Filter by provided dates\n",
        "        if news_date in dates:\n",
        "            collected.append({\n",
        "                \"title\": title,\n",
        "                \"url\": full_url,\n",
        "                \"date\": news_date.isoformat()\n",
        "            })\n",
        "\n",
        "    # Remove duplicates by URL\n",
        "    unique = []\n",
        "    seen = set()\n",
        "    for item in collected:\n",
        "        if item[\"url\"] not in seen:\n",
        "            seen.add(item[\"url\"])\n",
        "            unique.append(item)\n",
        "\n",
        "    # Save to JSON\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(unique, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Saved RIA data to {output_file}\")\n",
        "\n",
        "\n",
        "# Autostat scraper\n",
        "\n",
        "def fetch_autostat(dates, output_file,\n",
        "                   rubrics=[21, 8, 13, 70, 71],\n",
        "                   base_url_template=\"https://m.autostat.ru/news/themes-{rubric}/\"):\n",
        "\n",
        "    if dates is None:\n",
        "        raise ValueError(\"Argument 'dates' must be provided as a list of datetime.date objects.\")\n",
        "\n",
        "    all_collected = []\n",
        "    seen_urls = set()\n",
        "\n",
        "    ru_months = {\n",
        "        'января': 1, 'февраля': 2, 'марта': 3, 'апреля': 4,\n",
        "        'мая': 5, 'июня': 6, 'июля': 7, 'августа': 8,\n",
        "        'сентября': 9, 'октября': 10, 'ноября': 11, 'декабря': 12\n",
        "    }\n",
        "    today = date.today()\n",
        "    yesterday = today - timedelta(days=1)\n",
        "\n",
        "    for rubric in rubrics:\n",
        "        url = base_url_template.format(rubric=rubric)\n",
        "        print(f\"Fetching Autostat, {rubric}: {url}\")\n",
        "        soup = get_page_soup(url)\n",
        "        if not soup:\n",
        "            print(f\"  (!) Failed to retrieve or parse page for rubric {rubric}\")\n",
        "            continue\n",
        "\n",
        "        titles = soup.find_all(\"p\", class_=\"Block-title\")\n",
        "        if not titles:\n",
        "            print(f\"    (!) No <p class='Block-title'> elements found on {url}\")\n",
        "            continue\n",
        "\n",
        "        for title_p in titles:\n",
        "            title = title_p.get_text(strip=True)\n",
        "            if not title:\n",
        "                continue\n",
        "\n",
        "            link_a = title_p.find_parent(\"a\", class_=\"Block-link\")\n",
        "            if not link_a:\n",
        "                continue\n",
        "            href = link_a.get(\"href\", \"\").strip()\n",
        "            if not href:\n",
        "                continue\n",
        "            full_url = urljoin(\"https://www.autostat.ru\", href)\n",
        "\n",
        "            date_p = title_p.find_next(\"p\", class_=\"Block-date\")\n",
        "            if not date_p:\n",
        "                continue\n",
        "            date_text = date_p.get_text(strip=True)  # e.g. \"Сегодня, 15:48\" or \"28 мая, 15:48\"\n",
        "            date_part = date_text.split(\",\")[0].strip().lower()\n",
        "\n",
        "            if date_part == \"сегодня\":\n",
        "                news_date = today\n",
        "            elif date_part == \"вчера\":\n",
        "                news_date = yesterday\n",
        "            else:\n",
        "                parts = date_part.split()\n",
        "                if len(parts) != 2:\n",
        "                    continue\n",
        "                day_str, month_str = parts\n",
        "                try:\n",
        "                    day = int(day_str)\n",
        "                    month = ru_months.get(month_str)\n",
        "                    if not month:\n",
        "                        continue\n",
        "                    news_date = date(today.year, month, day)\n",
        "                    if news_date > today:\n",
        "                        news_date = date(today.year - 1, month, day)\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "            if news_date in dates and full_url not in seen_urls:\n",
        "                all_collected.append({\n",
        "                    \"title\": title,\n",
        "                    \"url\": full_url,\n",
        "                    \"date\": news_date.isoformat()\n",
        "                })\n",
        "                seen_urls.add(full_url)\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_collected, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Saved Autostat data to {output_file}\")"
      ],
      "metadata": {
        "id": "1AjGgmgp82IO"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "days_before = 0\n",
        "dates = get_last_dates(days_before)\n",
        "dates_kom = format_dates(dates, fmt=\"%Y-%m-%d\")\n",
        "dates_ved = format_dates(dates, fmt=\"%Y/%m/%d\")\n",
        "\n",
        "rubrics_kom_rus = [3, 4, 40]\n",
        "rubrics_kom_world = [3, 5]\n",
        "rubrics_kom_prices = [41]\n",
        "rubrics_rbc = [\"economics\", \"business\", \"finances\"]\n",
        "rubrics_rg = [\"politekonom\", \"industria\", \"business\", \"finansy\", \"kazna\", \"rabota\", \"pensii\", \"vnesh\", \"apk\", \"tovary\", \"turizm\"]\n",
        "rubrics_auto = [21, 8, 13, 70, 71]"
      ],
      "metadata": {
        "id": "vN-lfRXu9OQR"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching\n",
        "fetch_kom(rubrics_kom_rus, dates_kom, \"kom_rus.json\")\n",
        "fetch_kom(rubrics_kom_world, dates_kom, \"kom_world.json\")\n",
        "fetch_kom(rubrics_kom_prices, dates_kom, \"kom_prices.json\")\n",
        "fetch_ved(dates_ved, \"ved.json\")\n",
        "fetch_rbc(rubrics_rbc, dates, \"rbc.json\")\n",
        "fetch_agro(dates, \"agro.json\")\n",
        "fetch_rg(rubrics_rg, dates, \"rg.json\")\n",
        "fetch_ria(dates, \"ria.json\")\n",
        "fetch_autostat(dates, \"autostat.json\", rubrics_auto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVVSua9C9LEq",
        "outputId": "31a5d40c-f251-479e-deb5-f9415764160e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching Kommersant: https://www.kommersant.ru/archive/rubric/3/day/2025-06-01\n",
            "Fetching Kommersant: https://www.kommersant.ru/archive/rubric/4/day/2025-06-01\n",
            "Fetching Kommersant: https://www.kommersant.ru/archive/rubric/40/day/2025-06-01\n",
            "Saved Kommersant data to kom_rus.json\n",
            "Fetching Kommersant: https://www.kommersant.ru/archive/rubric/3/day/2025-06-01\n",
            "Fetching Kommersant: https://www.kommersant.ru/archive/rubric/5/day/2025-06-01\n",
            "Saved Kommersant data to kom_world.json\n",
            "Fetching Kommersant: https://www.kommersant.ru/archive/rubric/41/day/2025-06-01\n",
            "Saved Kommersant data to kom_prices.json\n",
            "Fetching Vedomosti: https://www.vedomosti.ru/newspaper/2025/06/01\n",
            "Saved Vedomosti data to ved.json\n",
            "Fetching RBC, economics: https://www.rbc.ru/economics/?utm_source=topline\n",
            "Fetching RBC, business: https://www.rbc.ru/business/?utm_source=topline\n",
            "Fetching RBC, finances: https://www.rbc.ru/finances/?utm_source=topline\n",
            "Saved RBC data to rbc.json\n",
            "Fetching Agroinvestor: https://www.agroinvestor.ru/\n",
            "Saved Agroinvestor data to agro.json\n",
            "Fetching RG, politekonom: https://rg.ru/tema/ekonomika/politekonom\n",
            "Fetching RG, industria: https://rg.ru/tema/ekonomika/industria\n",
            "Fetching RG, business: https://rg.ru/tema/ekonomika/business\n",
            "Fetching RG, finansy: https://rg.ru/tema/ekonomika/finansy\n",
            "Fetching RG, kazna: https://rg.ru/tema/ekonomika/kazna\n",
            "Fetching RG, rabota: https://rg.ru/tema/ekonomika/rabota\n",
            "Fetching RG, pensii: https://rg.ru/tema/ekonomika/pensii\n",
            "Fetching RG, vnesh: https://rg.ru/tema/ekonomika/vnesh\n",
            "Fetching RG, apk: https://rg.ru/tema/ekonomika/apk\n",
            "Fetching RG, tovary: https://rg.ru/tema/ekonomika/tovary\n",
            "Fetching RG, turizm: https://rg.ru/tema/ekonomika/turizm\n",
            "Saved RG data to rg.json\n",
            "Fetching RIA: https://ria.ru/economy/\n",
            "Saved RIA data to ria.json\n",
            "Fetching Autostat, 21: https://m.autostat.ru/news/themes-21/\n",
            "Fetching Autostat, 8: https://m.autostat.ru/news/themes-8/\n",
            "Fetching Autostat, 13: https://m.autostat.ru/news/themes-13/\n",
            "Fetching Autostat, 70: https://m.autostat.ru/news/themes-70/\n",
            "Fetching Autostat, 71: https://m.autostat.ru/news/themes-71/\n",
            "Saved Autostat data to autostat.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat\n",
        "section_to_files = {\n",
        "    \"world\": [\n",
        "        \"kom_world.json\",\n",
        "        \"kom_rus.json\",\n",
        "        \"ved.json\",\n",
        "        \"rbc.json\",\n",
        "        \"agro.json\",\n",
        "        \"rg.json\",\n",
        "        \"ria.json\"\n",
        "    ],\n",
        "    \"rus\": [\n",
        "        \"kom_rus.json\",\n",
        "        \"ved.json\",\n",
        "        \"rbc.json\",\n",
        "        \"agro.json\",\n",
        "        \"rg.json\",\n",
        "        \"ria.json\"\n",
        "    ],\n",
        "    \"prices\": [\n",
        "        \"kom_prices.json\",\n",
        "        \"kom_rus.json\",\n",
        "        \"ved.json\",\n",
        "        \"rbc.json\",\n",
        "        \"agro.json\",\n",
        "        \"rg.json\",\n",
        "        \"ria.json\",\n",
        "        \"autostat.json\"\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "Od3Q1e3L05DG"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with open('autostat.json', encoding='utf-8') as f:\n",
        "#    data = json.load(f)\n",
        "#print(json.dumps(data, ensure_ascii=False, indent=2))"
      ],
      "metadata": {
        "id": "-8jWQlo9p6Dq",
        "collapsed": true
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = userdata.get('gemini_api_key')\n",
        "genai.configure(api_key=API_KEY)\n",
        "model_obj = genai.GenerativeModel('gemini-1.5-flash')"
      ],
      "metadata": {
        "id": "uSlnm0N1JVmC"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ixqurYvmuT-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompts\n",
        "\n",
        "file_path = '/content/drive/MyDrive/news lists, prompt beginning.txt'\n",
        "\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        propmt_list_start = f.read()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: no file found (path: {file_path})\")\n",
        "except Exception as e:\n",
        "    print(f\"Error while reading file: {e}\")\n",
        "\n",
        "file_path = '/content/drive/MyDrive/bullets, prompt beginning.txt'\n",
        "\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        prompt_bullets_start = f.read()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: no file found (path: {file_path})\")\n",
        "except Exception as e:\n",
        "    print(f\"Error while reading file: {e}\")\n",
        "\n",
        "section_to_continue_prompt = {\n",
        "    \"world\": [\n",
        "        'Пожалуйста, просмотри АБСОЛЮТНО ВСЕ НОВОСТИ в приложенном файле и отбери из них только те, что строго соответствуют критериям и могут быть включены в нумерованный список для раздела по мировой экономике.'\n",
        "    ],\n",
        "    \"rus\": [\n",
        "        'Пожалуйста, просмотри АБСОЛЮТНО ВСЕ НОВОСТИ в приложенном файле и отбери из них только те, что строго соответствуют критериям и могут быть включены в нумерованный список для раздела по россиийской экономике.'\n",
        "    ],\n",
        "    \"prices\": [\n",
        "        'Пожалуйста, просмотри АБСОЛЮТНО ВСЕ НОВОСТИ в приложенном файле и отбери из них только те, что строго соответствуют критериям и могут быть включены в нумерованный список для раздела по новостям, релевантным для динамики российских цен.'\n",
        "    ]\n",
        "}\n",
        "prompt_list_finish = 'Пришли мне оформленный в соответствии с требованиями список. ОЧЕНЬ ВАЖНО НЕ ВКЛЮЧАТЬ В ОТВЕТ НИЧЕГО ДОПОЛНИТЕЛЬНОГО, ТОЛЬКО НАЗВАНИЯ НОВОСТЕЙ И ССЫЛКИ.'\n",
        "\n",
        "section_to_finish_bullets_prompt = {\n",
        "    \"world\": [\n",
        "        'Пожалуйста, подготовь 3 буллита для раздела по мировой экономике в соответствии с требованиями и пришли итоговый результат в таком формате: сначала буллиты, потом нумерованный список.'\n",
        "    ],\n",
        "    \"rus\": [\n",
        "        'Пожалуйста, подготовь 3 буллита для раздела по россиийской экономике в соответствии с требованиями и пришли итоговый результат в таком формате: сначала буллиты, потом нумерованный список.'\n",
        "    ],\n",
        "    \"prices\": [\n",
        "        'Пожалуйста, подготовь 3 буллита для раздела по по новостям, релевантным для динамики российских цен, в соответствии с требованиями и пришли итоговый результат в таком формате: сначала буллиты, потом нумерованный список.'\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "tQGpe7e5vric"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_news_lists(section):\n",
        "    if section not in section_to_files:\n",
        "        raise ValueError(f\"Section '{section}' unknown.\")\n",
        "\n",
        "    # Если сегодня не суббота, пробуем прочитать существующий файл <section>.txt\n",
        "    if datetime.today().weekday() != 5:  # 5 = Saturday\n",
        "        drive_folder = \"/content/drive/MyDrive\"\n",
        "        file_name = f\"{section}.txt\"\n",
        "        file_path = f\"{drive_folder}/{file_name}\"\n",
        "\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                list_start = f.read()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: file not found: {file_path}\")\n",
        "            list_start = \"\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file: {e}\")\n",
        "            list_start = \"\"\n",
        "    else:\n",
        "        list_start = \"\"\n",
        "\n",
        "    # Достаём список JSON-файлов и соответствующий prompt_list_continue\n",
        "    json_files = section_to_files[section]\n",
        "    prompt_list_continue = section_to_continue_prompt[section]\n",
        "\n",
        "    combined_text_parts = []\n",
        "    for json_filename in json_files:\n",
        "        base_name, ext = os.path.splitext(json_filename)\n",
        "        if ext.lower() != \".json\":\n",
        "            print(f\"Пропускаем '{json_filename}', т.к. не .json-файл.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(json_filename, 'r', encoding='utf-8') as f:\n",
        "                news_data = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Файл '{json_filename}' не найден. Пропускаем.\")\n",
        "            continue\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Ошибка JSON в '{json_filename}': {e}. Пропускаем.\")\n",
        "            continue\n",
        "\n",
        "        news_json_string = json.dumps(news_data, ensure_ascii=False, indent=2)\n",
        "\n",
        "        raw_parts = [\n",
        "            propmt_list_start,\n",
        "            prompt_list_continue,\n",
        "            prompt_list_finish,\n",
        "            news_json_string\n",
        "        ]\n",
        "\n",
        "        prompt_parts = []\n",
        "        for part in raw_parts:\n",
        "            if isinstance(part, list):\n",
        "                # Если это список, склеиваем через переносы строк\n",
        "                prompt_parts.append(\"\\n\".join(part))\n",
        "            else:\n",
        "                prompt_parts.append(str(part))\n",
        "\n",
        "        try:\n",
        "            response = model_obj.generate_content(prompt_parts)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in model.generate_content for '{json_filename}': {e}.\")\n",
        "            continue\n",
        "\n",
        "        header = f\"=== {base_name} ({section}) ===\\n\"\n",
        "        combined_text_parts.append(header + response.text + \"\\n\\n\")\n",
        "\n",
        "    if not combined_text_parts:\n",
        "        print(f\"For section '{section}', zero JSONs were successfully processed.\")\n",
        "        return\n",
        "\n",
        "    # Объединяем прочитанное ранее (list_start) с новыми частями\n",
        "    all_text = list_start + \"\".join(combined_text_parts)\n",
        "\n",
        "    # Записываем итог в тот же файл <section>.txt на Google Drive\n",
        "    drive_folder = \"/content/drive/MyDrive\"\n",
        "    file_name = f\"{section}.txt\"\n",
        "    file_path = f\"{drive_folder}/{file_name}\"\n",
        "\n",
        "    try:\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as out_f:\n",
        "            out_f.write(all_text)\n",
        "        print(f\"File successfully written: {file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: path not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing file: {e}\")\n"
      ],
      "metadata": {
        "id": "UDWokhdm0SqA"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_news_lists(\"world\")\n",
        "time.sleep(60)\n",
        "create_news_lists(\"rus\")\n",
        "time.sleep(60)\n",
        "create_news_lists(\"prices\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "trUVpRsnKYah",
        "outputId": "7aaf39a8-6b40-4d9c-84f9-f5c3ecd5493c",
        "collapsed": true
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File successfully written: /content/drive/MyDrive/world.txt\n",
            "File successfully written: /content/drive/MyDrive/rus.txt\n",
            "File successfully written: /content/drive/MyDrive/prices.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bullets(section):\n",
        "    if section not in section_to_files:\n",
        "        raise ValueError(f\"Section '{section}' unknown.\")\n",
        "\n",
        "    # Путь к файлу списка, который нужно прочитать\n",
        "    list_file = f\"{section}.txt\"\n",
        "    file_path = f\"/content/drive/MyDrive/{list_file}\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            list_content = f.read()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: path not found: {file_path}\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return\n",
        "\n",
        "    # Берём соответствующий prompt для завершения\n",
        "    prompt_bullets_finish = section_to_finish_bullets_prompt[section]\n",
        "\n",
        "    # Формируем prompt_parts\n",
        "    raw_parts = [\n",
        "        prompt_bullets_start,\n",
        "        prompt_bullets_finish,\n",
        "        list_content\n",
        "    ]\n",
        "\n",
        "    prompt_parts = []\n",
        "    for part in raw_parts:\n",
        "            if isinstance(part, list):\n",
        "                # Если это список, склеиваем через переносы строк\n",
        "                prompt_parts.append(\"\\n\".join(part))\n",
        "            else:\n",
        "                prompt_parts.append(str(part))\n",
        "\n",
        "    try:\n",
        "        response = model_obj.generate_content(prompt_parts)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in model.generate_content: {e}\")\n",
        "        return\n",
        "\n",
        "    file_name = f\"report_{section}.txt\"\n",
        "\n",
        "    drive_folder = \"/content/drive/MyDrive/news_reports\"\n",
        "    os.makedirs(drive_folder, exist_ok=True)\n",
        "    out_path = f\"{drive_folder}/{file_name}\"\n",
        "\n",
        "    try:\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(response.text)\n",
        "        print(f\"Успешно записан файл: {out_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: path not found: {out_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing file: {e}\")"
      ],
      "metadata": {
        "id": "AoFqK2zW-AR4"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if datetime.today().weekday() == 3:\n",
        "  create_bullets(\"world\")\n",
        "  create_bullets(\"rus\")\n",
        "  create_bullets(\"prices\")"
      ],
      "metadata": {
        "id": "bsZUwtcM6ST8"
      },
      "execution_count": 59,
      "outputs": []
    }
  ]
}