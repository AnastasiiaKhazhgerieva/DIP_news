# -*- coding: utf-8 -*-
"""dip_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17KKwbfVuL28SPhRasyS-FeUXLvmP2PNo
"""

# packages

import requests
import json
import time
import datetime
import os
import pandas as pd
import google.generativeai as genai
import io
import base64
try: # google colab не запускается, когда раним через workflow, он там есть по умолчанию, поэтому имени в PyPL такого нет
    from google.colab import userdata, drive
except ImportError:
    userdata = None
    drive = None
from datetime import date, timedelta, datetime
from typing import List
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaIoBaseDownload, MediaIoBaseUpload
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials

# Auxilliary
HEADERS = {"User-Agent": "Mozilla/5.0"}

encoded_token = os.environ.get("GOOGLE_TOKEN_B64")
if not encoded_token:
    raise RuntimeError("OAuth токен не найден. Убедитесь, что переменная окружения GOOGLE_TOKEN_B64 задана.")

token_bytes = base64.b64decode(encoded_token)
token_info = json.loads(token_bytes.decode("utf-8"))

creds = Credentials.from_authorized_user_info(token_info, scopes=["https://www.googleapis.com/auth/drive"])

if creds.expired and creds.refresh_token:
    creds.refresh(Request())
    
drive_service = build("drive", "v3", credentials=creds)

print("✅ Credentials info:")
print("  - token:", creds.token[:20] + "...")
print("  - refresh_token:", bool(creds.refresh_token))
print("  - client_id:", creds.client_id)
print("  - quota_project_id:", creds.quota_project_id)
print("  - valid:", creds.valid)
print("  - expired:", creds.expired)
print("  - scopes:", creds.scopes)
# Кто залогинен?
about = drive_service.about().get(fields="user").execute()
print("✅ Авторизация от имени:", about["user"]["displayName"], about["user"]["emailAddress"])


MY_FOLDER_ID = "1BwBFMln6HcGUfBFN4-UlNueOTKUehiRe" # папка reports на google drive

# gemini api key
API_KEY = os.environ.get("GEMINI_API_KEY") # строка для запуска через workflow
#API_KEY = userdata.get('gemini_api_key') # строка для локального запуска
genai.configure(api_key=API_KEY)
model_obj = genai.GenerativeModel('gemini-2.5-pro')

### TG Schedule bot

def telegram_lists():
    url = "https://api.telegram.org/bot6245425859:AAEHTh0EX9aE6qPhvHoclFPa3a8IBVRNeSM/sendMessage"
    payload = {
        "chat_id": "-4265314101",
        "text": "Новостная записка обновлена. См. отчёты по <a href=\"https://clck.ru/3MTaGo\">ссылке</a>",
        "parse_mode": "HTML"
    }

    try:
        response = requests.post(url, data=payload)
        response.raise_for_status()
        print("Telegram message sent.")
    except requests.exceptions.RequestException as e:
        print(f"Failed to send Telegram message: {e}")

def telegram_bullets():
    url = "https://api.telegram.org/bot6245425859:AAEHTh0EX9aE6qPhvHoclFPa3a8IBVRNeSM/sendMessage"
    payload = {
        "chat_id": "-4265314101",
        "text": "Готовы буллиты к новостной записке. См. отчёты по <a href=\"https://clck.ru/3MTbwx\">ссылке</a>",
        "parse_mode": "HTML"
    }

    try:
        response = requests.post(url, data=payload)
        response.raise_for_status()
        print("Telegram message sent.")
    except requests.exceptions.RequestException as e:
        print(f"Failed to send Telegram message: {e}")

### Functions for google drive

def find_file_in_drive(file_name: str, folder_id = "1BwBFMln6HcGUfBFN4-UlNueOTKUehiRe") -> str:
    try:
        resp = drive_service.files().list(
            q=(
                f"name = '{file_name}' "
                f"and '{folder_id}' in parents "
                f"and trashed = false"
            ),
            spaces="drive",
            fields="files(id, name)",
            pageSize=1
        ).execute()
    except HttpError as e:
        raise RuntimeError(f"Error accessing Drive API: {e}")

    items = resp.get("files", [])
    if items:
        return items[0]["id"]

    raise FileNotFoundError(f"File '{file_name}' not found in folder {folder_id}.")

def download_text_file(fid: str) -> str:
    request = drive_service.files().get_media(fileId=fid)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        status, done = downloader.next_chunk()
    return fh.getvalue().decode("utf-8")

def save_to_drive(file_name: str, data, my_folder=MY_FOLDER_ID, file_format: str = "json"):
    if file_format not in ("json", "txt"):
        raise ValueError("file_format должен быть 'json' или 'txt'")

    if file_format == "txt":
        content_bytes = data.encode("utf-8") if isinstance(data, str) else str(data).encode("utf-8")
        mime_type = "text/plain"
    else:
        json_str = json.dumps(data, ensure_ascii=False, indent=2)
        content_bytes = json_str.encode("utf-8")
        mime_type = "application/json"

    # Ищем, существует ли уже файл
    existing_file_id = None
    try:
        resp = drive_service.files().list(
            q=f"name = '{file_name}' and '{my_folder}' in parents and trashed = false",
            spaces="drive",
            fields="files(id, name)",
            pageSize=1
        ).execute()
        items = resp.get("files", [])
        if items:
            existing_file_id = items[0]["id"]
    except Exception as e:
        print("Warning: can't check if the file already exists:", e)

    fh = io.BytesIO(content_bytes)
    media = MediaIoBaseUpload(fh, mimetype=mime_type, resumable=False)

    if existing_file_id:
        try:
            # Пытаемся обновить
            updated = drive_service.files().update(
                fileId=existing_file_id,
                media_body=media
            ).execute()
            print(f"File '{file_name}' updated (ID={updated['id']}).")
            return updated
        except HttpError as e:
            if e.resp.status == 403 and "storageQuotaExceeded" in str(e):
                print(f"⚠️ Quota error on update — deleting and recreating file '{file_name}'...")
                try:
                    drive_service.files().delete(fileId=existing_file_id).execute()
                    existing_file_id = None  # перейти к созданию
                except Exception as del_err:
                    print(f"Ошибка при удалении файла '{file_name}': {del_err}")
                    raise
            else:
                print(f"Ошибка при обновлении файла '{file_name}': {e}")
                raise

    # Создание нового файла
    file_metadata = {
        "name": file_name,
        "parents": [my_folder],
        "mimeType": mime_type
    }
    try:
        created = drive_service.files().create(
            body=file_metadata,
            media_body=media,
            fields="id, webViewLink"
        ).execute()
        print(f"New file created: '{file_name}', (ID={created['id']}).")
        return created
    except Exception as e:
        print(f"Ошибка при создании нового файла '{file_name}': {e}")
        raise

### Functions for scrapping

## Defining and formatting dates
def get_last_dates(n_days=6, end_date=None):
    if end_date is None:
        end_date = date.today()
    return [end_date - timedelta(days=offset) for offset in range(n_days, -1, -1)]

def format_dates(dates_list, fmt="%Y-%m-%d"):
    return [d.strftime(fmt) for d in dates_list]

## Getting web page soup
def get_page_soup(url, headers=HEADERS, timeout=10):
    resp = requests.get(url, headers=headers, timeout=timeout)
    resp.raise_for_status()
    return BeautifulSoup(resp.text, "html.parser")

## Scrapers: Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat

# Kommersant scraper
def fetch_kom(rubrics, dates, output_file,
                   base_url_template="https://www.kommersant.ru/archive/rubric/{rubric}/day/{date}"):
    all_items = []
    seen_urls = set()

    for rubric in rubrics:
        for dt in dates:
            url = base_url_template.format(rubric=rubric, date=dt)
            print(f"Fetching Kommersant: {url}")
            try:
                soup = get_page_soup(url)
                scripts = soup.find_all("script", type="application/ld+json")

                for script in scripts:
                    raw = script.string
                    if not raw:
                        continue
                    try:
                        data = json.loads(raw)
                    except json.JSONDecodeError:
                        continue

                    for entry in data.get("itemListElement", []):
                        title = entry.get("name") or entry.get("headline")
                        link = entry.get("url")
                        if title and link and link not in seen_urls:
                            seen_urls.add(link)
                            all_items.append({"title": title, "url": link})
            except Exception as e:
                print(f"[ERROR] {e} when fetching {url}")

    save_to_drive(output_file, all_items, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved Kommersant data to {output_file}")


# Vedomosti scraper
def fetch_ved(dates, output_file,
              base_url_template="https://www.vedomosti.ru/newspaper/{date}"):
    all_news = []
    for dt in dates:
        url = base_url_template.format(date=dt)
        print(f"Fetching Vedomosti: {url}")
        try:
            soup = get_page_soup(url)
            for item in soup.select("li.waterfall__item"):
                a = item.select_one("a.waterfall__item-title")
                if not a:
                    continue
                title = a.get_text(strip=True)
                href = a.get("href", "")
                full_url = href if href.startswith("http") else f"https://www.vedomosti.ru{href}"
                all_news.append({"title": title, "url": full_url})
        except Exception as e:
            all_news.append({"error": str(e)})

    save_to_drive(output_file, all_news, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved Vedomosti data to {output_file}")

# RBC scraper

from datetime import date

def fetch_rbc(rubrics, dates, output_file,
              base_url_template="https://www.rbc.ru/{rubric}/?utm_source=topline"):

    ru_months = {
        'января': 1, 'февраля': 2, 'марта': 3, 'апреля': 4,
        'мая': 5, 'июня': 6, 'июля': 7, 'августа': 8,
        'сентября': 9, 'октября': 10, 'ноября': 11, 'декабря': 12
    }
    today = date.today()
    collected = []

    for rubric in rubrics:
        page_url = base_url_template.format(rubric=rubric)
        print(f"Fetching RBC, {rubric}: {page_url}")
        soup = get_page_soup(page_url)

        anchors = soup.find_all("a", class_="news-feed__item")

        for idx, a in enumerate(anchors, start=1):
            # внутри anchor ищем span, у которого class содержит "news-feed__item__title"
            title_span = a.find(
                "span",
                class_=lambda c: c and "news-feed__item__title" in c
            )
            if not title_span:
                continue

            # Для даты: ищем span, у которого class содержит "news-feed__item__time"
            # или, если нет, "news-feed__item__date"
            date_span = a.find(
                "span",
                class_=lambda c: c and "news-feed__item__time" in c
            )
            if not date_span:
                date_span = a.find(
                    "span",
                    class_=lambda c: c and "news-feed__item__date" in c
                )
            if not date_span:
                continue

            title = title_span.get_text(strip=True)
            href = a.get("href", "").strip()
            if not href:
                continue

            full_url = href if href.startswith("http") else urljoin(page_url, href)

            # raw_date может быть вида "28 мая 17:52" или просто "17:52"
            raw_date = date_span.get_text(strip=True).replace("\xa0", " ").replace(",", "").strip()
            parts = raw_date.split()

            news_date = None
            if any(month in parts for month in ru_months):
                # формат ["28","мая","17:52"] или ["28","мая","2025","17:52"]
                try:
                    day = int(parts[0])
                except ValueError:
                    continue
                month_name = parts[1].lower()
                if month_name not in ru_months:
                    continue
                month = ru_months[month_name]
                year = today.year
                # если в parts[2] четвёрка цифр, считаем, что это год
                if len(parts) >= 3 and parts[2].isdigit() and len(parts[2]) == 4:
                    year = int(parts[2])
                try:
                    candidate = datetime.date(year, month, day)
                except ValueError:
                    continue
                # если эта дата уже в будущем, значит, год был прошлый
                if candidate > today:
                    candidate = datetime.date(year - 1, month, day)
                news_date = candidate
            else:
                # если нет названия месяца, значит raw_date = "HH:MM" сегодняшняя дата
                news_date = today

            if news_date not in dates:
                continue

            collected.append({
                "title": title,
                "url": full_url
            })

    # убираем дубликаты по URL
    unique = []
    seen = set()
    for item in collected:
        if item["url"] not in seen:
            seen.add(item["url"])
            unique.append(item)

    save_to_drive(output_file, unique, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved RBC data to {output_file}")

# Agro investor scraper - периодически ломается, поэтому пусть будет в коде вариант с отладкой

def fetch_agro(dates, output_file, base_url="https://www.agroinvestor.ru/"):
    print(f"Fetching Agroinvestor: {base_url}")

    soup = get_page_soup(base_url)

    if soup is None:
        print("❌ Failed to retrieve or parse the page.")
        return

    print("✅ Page fetched successfully.")
    news_list = []
    seen_links = set()

    ru_months = {
        "января": 1, "февраля": 2, "марта": 3, "апреля": 4,
        "мая": 5, "июня": 6, "июля": 7, "августа": 8,
        "сентября": 9, "октября": 10, "ноября": 11, "декабря": 12
    }

    print(f"Looking for dates: {dates}")

    for time_tag in soup.find_all("time"):
        date_text = time_tag.get_text(strip=True).replace("\xa0", " ")
        if not date_text:
            continue

        print(f"🕒 Found date text: '{date_text}'")
        parts = date_text.split()
        if len(parts) != 3:
            print("⚠️ Unexpected date format. Skipping.")
            continue

        day_str, month_str, year_str = parts
        try:
            day = int(day_str)
            year = int(year_str)
        except ValueError as e:
            print(f"⚠️ Could not parse day/year: {e}")
            continue

        month_str = month_str.lower()
        if month_str not in ru_months:
            print(f"⚠️ Unknown month: '{month_str}'")
            continue

        month = ru_months[month_str]
        try:
            date_obj = date(year, month, day)

        except Exception as e:
            print(f"⚠️ Failed to construct date object: {e}")
            continue

        print(f"📅 Parsed date: {date_obj}")
        if date_obj not in dates:
            print("⏩ Date not in requested range. Skipping.")
            continue

        anchor = time_tag.find_previous("a")
        if not anchor:
            print("⚠️ No previous anchor tag found.")
            continue

        title = anchor.get_text(strip=True)
        href = anchor.get("href")
        if not href or not title:
            print("⚠️ Missing title or href. Skipping.")
            continue

        url = urljoin(base_url, href.strip())
        if url in seen_links:
            print(f"🔁 Duplicate link: {url}")
            continue
        seen_links.add(url)

        print(f"✅ Added news: {title} - {url}")
        news_list.append({
            "title": title,
            "link": url
        })

    save_to_drive(output_file, news_list, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"💾 Saved {len(news_list)} news items to {output_file}")


# RG.ru scraper

def fetch_rg(rubrics, dates, output_file,
             base_url_template="https://rg.ru/tema/ekonomika/{rubric}"):
    all_news = []
    for rubric in rubrics:
        url = base_url_template.format(rubric=rubric)
        print(f"Fetching RG, {rubric}: {url}")
        soup = get_page_soup(url)
        for title_span in soup.find_all("span", class_="ItemOfListStandard_title__Ajjlf"):
            parent_a = title_span.find_parent("a")
            if not parent_a:
                continue
            href = parent_a.get("href", "").strip()
            if not href:
                continue
            full_url = href if href.startswith("http") else f"https://rg.ru{href}"

            date_a = title_span.find_previous("a", class_="ItemOfListStandard_datetime__GstJi")
            if not date_a:
                continue
            date_href = date_a.get("href", "").strip()
            parts = date_href.strip("/").split("/")  # ['2025','05','30',...]
            if len(parts) < 3:
                continue
            try:
                y, m, d = map(int, parts[:3])
                news_date = date(y, m, d)
            except ValueError:
                continue

            if news_date not in dates:
                continue

            all_news.append({
                "title": title_span.get_text(strip=True),
                "url": full_url
            })

    unique = []
    seen = set()
    for item in all_news:
        if item["url"] not in seen:
            seen.add(item["url"])
            unique.append(item)

    save_to_drive(output_file, unique, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved RG data to {output_file}")

# RIA scraper

def fetch_ria(dates, output_file, base_url_template="https://ria.ru/economy/"):
    print("Fetching RIA: https://ria.ru/economy/")
    soup = get_page_soup(base_url_template)
    collected = []

    # Each news item has <a itemprop="url" href="..."></a>
    for a in soup.find_all("a", itemprop="url"):
        href = a.get("href", "").strip()
        if not href:
            continue
        full_url = href if href.startswith("http") else f"https://ria.ru{href}"

        # Next meta tag with itemprop="name" holds the title
        name_meta = a.find_next("meta", itemprop="name")
        if not name_meta:
            continue
        title = name_meta.get("content", "").strip()
        if not title:
            continue
        parsed = urlparse(full_url)
        parts = parsed.path.lstrip("/").split("/")
        if not parts or len(parts[0]) != 8 or not parts[0].isdigit():
            continue
        y, m, d = int(parts[0][:4]), int(parts[0][4:6]), int(parts[0][6:8])
        try:
            news_date = date(y, m, d)
        except ValueError:
            continue

        if news_date in dates:
            collected.append({
                "title": title,
                "url": full_url
            })

    unique = []
    seen = set()
    for item in collected:
        if item["url"] not in seen:
            seen.add(item["url"])
            unique.append(item)

    save_to_drive(output_file, unique, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")

    print(f"Saved RIA data to {output_file}")


# Autostat scraper

def fetch_autostat(dates, output_file,
                   rubrics=[21, 8, 13, 70, 71],
                   base_url_template="https://m.autostat.ru/news/themes-{rubric}/"):

    if dates is None:
        raise ValueError("Argument 'dates' must be provided as a list of datetime.date objects.")

    all_collected = []
    seen_urls = set()

    ru_months = {
        'января': 1, 'февраля': 2, 'марта': 3, 'апреля': 4,
        'мая': 5, 'июня': 6, 'июля': 7, 'августа': 8,
        'сентября': 9, 'октября': 10, 'ноября': 11, 'декабря': 12
    }
    today = date.today()
    yesterday = today - timedelta(days=1)

    for rubric in rubrics:
        url = base_url_template.format(rubric=rubric)
        print(f"Fetching Autostat, {rubric}: {url}")
        soup = get_page_soup(url)
        if not soup:
            print(f"  (!) Failed to retrieve or parse page for rubric {rubric}")
            continue

        titles = soup.find_all("p", class_="Block-title")
        if not titles:
            print(f"    (!) No <p class='Block-title'> elements found on {url}")
            continue

        for title_p in titles:
            title = title_p.get_text(strip=True)
            if not title:
                continue

            link_a = title_p.find_parent("a", class_="Block-link")
            if not link_a:
                continue
            href = link_a.get("href", "").strip()
            if not href:
                continue
            full_url = urljoin("https://www.autostat.ru", href)

            date_p = title_p.find_next("p", class_="Block-date")
            if not date_p:
                continue
            date_text = date_p.get_text(strip=True)  # e.g. "Сегодня, 15:48" or "28 мая, 15:48"
            date_part = date_text.split(",")[0].strip().lower()

            if date_part == "сегодня":
                news_date = today
            elif date_part == "вчера":
                news_date = yesterday
            else:
                parts = date_part.split()
                if len(parts) != 2:
                    continue
                day_str, month_str = parts
                try:
                    day = int(day_str)
                    month = ru_months.get(month_str)
                    if not month:
                        continue
                    news_date = date(today.year, month, day)
                    if news_date > today:
                        news_date = date(today.year - 1, month, day)
                except Exception:
                    continue

            if news_date in dates and full_url not in seen_urls:
                all_collected.append({
                    "title": title,
                    "url": full_url
                })
                seen_urls.add(full_url)

    save_to_drive(output_file, all_collected, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")

    print(f"Saved Autostat data to {output_file}")

#with open('agro.json', encoding='utf-8') as f:
#    data = json.load(f)
#print(json.dumps(data, ensure_ascii=False, indent=2))

# Parameters
days_before = 1
dates = get_last_dates(days_before)
dates_kom = format_dates(dates, fmt="%Y-%m-%d")
dates_ved = format_dates(dates, fmt="%Y/%m/%d")

rubrics_kom_rus = [3, 4, 40]
rubrics_kom_world = [3, 5]
rubrics_kom_prices = [41]
rubrics_rbc = ["economics", "business", "finances"]
rubrics_rg = ["politekonom", "industria", "business", "finansy", "kazna", "rabota", "pensii", "vnesh", "apk", "tovary", "turizm"]
rubrics_auto = [21, 8, 13, 70, 71]

# Fetching
#fetch_kom(rubrics_kom_rus, dates_kom, "kom_rus.json")
#fetch_kom(rubrics_kom_world, dates_kom, "kom_world.json")
#fetch_kom(rubrics_kom_prices, dates_kom, "kom_prices.json")
#fetch_ved(dates_ved, "ved.json")
#fetch_rbc(rubrics_rbc, dates, "rbc.json")
#try:
#    fetch_agro(dates, "agro.json")
#except Exception as e:
#    pass
#fetch_rg(rubrics_rg, dates, "rg.json")
#fetch_ria(dates, "ria.json")
#fetch_autostat(dates, "autostat.json", rubrics_auto)

# Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat
section_to_files = {
    "world": [
        "kom_world.json",
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        #"rg.json",
        "ria.json"
    ],
    "rus": [
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        "rg.json",
        "ria.json"
    ],
    "prices": [
        "kom_prices.json",
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        #"rg.json",
        "ria.json",
        "autostat.json"
    ]
}

# drive.mount('/content/drive')

### Prompts

###
### news lists
file_id = find_file_in_drive("lists_world.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    lists_world = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    lists_world = ""

file_id = find_file_in_drive("lists_rus.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    lists_rus = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    lists_rus = ""

file_id = find_file_in_drive("lists_prices.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    lists_prices = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    lists_prices = ""

lists_prompts = {
        "world": lists_world,
        "rus": lists_rus,
        "prices": lists_prices
}

### prioritise
file_id = find_file_in_drive("prioritise_world.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    prioritise_world = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    prioritise_world = ""

file_id = find_file_in_drive("prioritise_rus.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    prioritise_rus = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    prioritise_rus = ""

file_id = find_file_in_drive("prioritise_prices.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    prioritise_prices = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    prioritise_prices = ""

prioritise_prompts = {
        "world": prioritise_world,
        "rus": prioritise_rus,
        "prices": prioritise_prices
}

### design

file_id = find_file_in_drive("design.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    prompt_design = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    prompt_design = ""

### top
file_id = find_file_in_drive("top_world.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    top_world = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    top_world = ""

file_id = find_file_in_drive("top_rus.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    top_rus = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    top_rus = ""

file_id = find_file_in_drive("top_prices.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    top_prices = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    top_prices = ""

top_prompts = {
        "world": top_world,
        "rus": top_rus,
        "prices": top_prices
}

### bullets
file_id = find_file_in_drive("bullets_world.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    bullets_world = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    bullets_world = ""

file_id = find_file_in_drive("bullets_rus.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    bullets_rus = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    bullets_rus = ""

file_id = find_file_in_drive("bullets_prices.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    bullets_prices = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    bullets_prices = ""

bullets_prompts = {
        "world": bullets_world,
        "rus": bullets_rus,
        "prices": bullets_prices
}

example = 'Пример верного оформления:\r\n1.\tРосстат зафиксировал стабилизацию выпуска базовых отраслей\r\nhttps://www.kommersant.ru/doc/7329366 \r\n2.\tСтроители просят смягчить правила распоряжения авансами\r\nhttps://www.rbc.ru/newspaper/2024/11/25/673f6abf9a7947de58a24847 \r\n3.\tВ Ульяновске открылся новый завод грузовиков Соллерс\r\nhttps://tass.ru/ekonomika/22497349 \r\n4.\t Добыча газа за 9 месяцев выросла на 8% г/г в основном за счет Газпрома\r\nhttps://www.interfax.ru/business/994801 \r\n'

def extract_json(text: str):
    """
    Пытается найти в строке `text` JSON-список или JSON-объект и вернуть его как Python-структуру.
    Сначала ищет JSON-массив [...], если не находит — JSON-объект {...}.
    Если подходящего фрагмента нет или он невалиден — возвращает None.
    """
    # 1) Пытаемся найти JSON-массив: ищем первую '[' и последнюю ']'
    start = text.find('[')
    end = text.rfind(']')
    if 0 <= start < end:
        candidate = text[start:end+1]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass

    # 2) Если не найден массив, пробуем найти JSON-объект: первую '{' и последнюю '}'
    start = text.find('{')
    end = text.rfind('}')
    if 0 <= start < end:
        candidate = text[start:end+1]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass

    # 3) Если ни то, ни другое не получилось — отдадим None
    return None

def create_news_lists(section):
    # Если сегодня не суббота, пробуем прочитать существующий файл <section>.json
    if datetime.today().weekday() != 5:  # 5 = Saturday
        try:
            existing_id = find_file_in_drive(f"{section}.json", "1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe")
            existing_text = download_text_file(existing_id)
            try:
                combined_items = json.loads(existing_text)
            except json.JSONDecodeError:
                combined_items = []
        except Exception:
            combined_items = []
    else:
        combined_items = []

    seen_urls = {item["url"] for item in combined_items if isinstance(item, dict) and "url" in item}

    # Достаём список JSON-файлов и prompt
    json_files = section_to_files[section]
    prompt_list = lists_prompts.get(section, "")

    for json_filename in json_files:
        base_name, ext = os.path.splitext(json_filename)
        if ext.lower() != ".json":
            print(f"Пропускаем '{json_filename}', т.к. не .json-файл.")
            continue

        try:
            file_id = find_file_in_drive(json_filename, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
            raw_text = download_text_file(file_id)
        except FileNotFoundError:
            print(f"Файл '{json_filename}' не найден. Пропускаем.")
            continue
        except Exception as e:
            print(f"Ошибка при скачивании '{json_filename}': {e}. Пропускаем.")
            continue

        if not isinstance(raw_text, str) or not raw_text.strip():
            print(f"JSON '{json_filename}' пустой. Пропускаем.")
            continue

        try:
            news_data = json.loads(raw_text)
        except json.JSONDecodeError as e:
            print(f"Ошибка JSON в '{json_filename}': {e}. Пропускаем.")
            continue

        if isinstance(news_data, (list, dict)) and len(news_data) == 0:
            print(f"JSON '{json_filename}' содержит пустую структуру. Пропускаем.")
            continue

        news_json_string = json.dumps(news_data, ensure_ascii=False, indent=2)

        raw_parts = [prompt_list, news_json_string]

        prompt_parts = []
        for part in raw_parts:
            if isinstance(part, list):
                prompt_parts.append("\n".join(part))
            else:
                prompt_parts.append(str(part))

        try:
            response = model_obj.generate_content(prompt_parts)
        except Exception as e:
            print(f"Ошибка при вызове модели для '{json_filename}': {e}. Пропускаем.")
            continue

        if not hasattr(response, "candidates") or not response.candidates:
            print(f"Модель не вернула кандидатов для '{json_filename}'. Пропускаем.")
            continue

        if not hasattr(response, "text") or response.text is None:
            print(f"response.text отсутствует для '{json_filename}'. Возможно, причина: finish_reason=1.")
            print("Подготовленный prompt:\n", "\n\n".join(prompt_parts)[:300], "…")
            continue

        raw_reply = response.text

        items = extract_json(raw_reply)
        if items is None:
            print(f"Ответ модели для '{json_filename}' не содержит валидный JSON:\n{raw_reply[:200]}… Пропускаем.")
            continue

        if isinstance(items, dict):
            items = [items]

        if not isinstance(items, list):
            print(f"Ответ модели для '{json_filename}' вернул не список, а {type(items)}. Пропускаем.")
            continue

        for entry in items:
            url = entry.get("url")
            title = entry.get("title")
            if not title or not url or url in seen_urls:
                continue
            seen_urls.add(url)
            combined_items.append({"title": title, "url": url})

    if not combined_items:
        print(f"For section '{section}', zero JSONs were successfully processed.")
        return

    # Сохраняем объединённый JSON в файл <section>.json
    output_file = f"{section}.json"
    save_to_drive(output_file, combined_items, my_folder="1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe")

# Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat
create_news_lists("world")
time.sleep(60)
create_news_lists("rus")
time.sleep(60)
create_news_lists("prices")

def prioritise(section):
    file_name = f"{section}.json"
    folder_id = "1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe"

    try:
        file_id = find_file_in_drive(file_name, folder_id)
        news_list_raw = download_text_file(file_id)
    except FileNotFoundError:
        print(f"❌ Файл {file_name} не найден в папке {folder_id}.")
        return
    except Exception as e:
        print(f"❌ Ошибка при загрузке файла {file_name}: {e}")
        return

    if not news_list_raw.strip():
        print(f"❌ Файл {file_name} пустой.")
        return

    prompt_prioritise = prioritise_prompts.get(section, "")

    raw_parts = [prompt_prioritise, news_list_raw]

    prompt_parts = []
    for part in raw_parts:
        if isinstance(part, list):
            prompt_parts.append("\n".join(part))
        else:
            prompt_parts.append(str(part))

    try:
        response = model_obj.generate_content(prompt_parts)
        response_text = getattr(response, "text", "").strip()
    except Exception as e:
        print(f"❌ Ошибка генерации от модели для '{file_name}': {e}")
        return

    # Убираем markdown-обертки, если есть
    if response_text.startswith("```json"):
        response_text = response_text[7:]
    if response_text.endswith("```"):
        response_text = response_text[:-3]
    response_text = response_text.strip()

    # Пытаемся распарсить ответ как JSON
    try:
        filtered_list = json.loads(response_text)
        if not isinstance(filtered_list, list):
            raise ValueError(f"Ожидался список, а пришло {type(filtered_list)}")
    except Exception as e:
        print(f"⚠️ Не удалось распарсить JSON в prioritise({section}): {e}")
        save_to_drive(file_name.replace(".json", "_prioritise_error.txt"), response_text, folder_id, file_format="txt")
        return

    # Сохраняем результат
    save_to_drive(file_name, filtered_list, folder_id, file_format="json")
    print(f"✅ prioritise({section}) — сохранён корректный JSON.")

#prioritise("world")
#time.sleep(60)
#prioritise("rus")
#time.sleep(60)
#prioritise("prices")

def design(section):
    # Получаем JSON с отфильтрованными новостями
    file_name_json = f"{section}.json"
    file_id = find_file_in_drive(file_name_json, "1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe")
    news_list_raw = download_text_file(file_id)

    raw_parts = [
        prompt_design,
        example,
        news_list_raw
    ]

    prompt_parts = []
    for part in raw_parts:
        if isinstance(part, list):
            prompt_parts.append("\n".join(part))
        else:
            prompt_parts.append(str(part))

    try:
        response = model_obj.generate_content(prompt_parts)
    except Exception as e:
        print(f"Error in model.generate_content for '{file_name_json}': {e}.")
        return

    # Записываем результат в отдельный .txt файл
    file_name_txt = f"{section}.txt"
    save_to_drive(file_name_txt, response.text, "1BwBFMln6HcGUfBFN4-UlNueOTKUehiRe", file_format="txt")

#design("world")
#time.sleep(60)
#design("rus")
#time.sleep(60)
#design("prices")
#telegram_lists()

def read_top_urls(section, max_chars=1500):

    def extract_main_text(soup, max_chars=1500, min_paragraph_len=50, max_paragraphs=5):
        paragraphs = []
        for p in soup.find_all('p'):
            text = p.get_text(" ", strip=True)
            if len(text) < min_paragraph_len:
                continue
            low = text.lower()
            if any(word in low for word in ["cookie", "subscribe", "advert", "реклама", "подпишитесь"]):
                continue
            paragraphs.append(text)
            if len(paragraphs) >= max_paragraphs:
                break

        combined_text = " ".join(paragraphs)
        if len(combined_text) > max_chars:
            combined_text = combined_text[:max_chars].rsplit(" ", 1)[0] + "..."
        return combined_text

    # Загружаем JSON с новостями после prioritise
    file_name = f"{section}.json"
    file_id = find_file_in_drive(file_name, "1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe")
    news_list = download_text_file(file_id)

    # Получаем промпт
    prompt_top = top_prompts.get(section, "")
    raw_parts = [prompt_top, news_list]

    # Генерация топ ссылок
    try:
        response = model_obj.generate_content(raw_parts)
        top_links_json = json.loads(response.text)
    except Exception as e:
        print(f"Ошибка генерации топ ссылок для {section}: {e}")
        return

    # Скачиваем и очищаем страницы
    results = []
    for item in top_links_json:
        url = item.get("url") or item.get("URL")
        title = item.get("title", "")
        if not url:
            continue
        try:
            resp = requests.get(url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(resp.text, "html.parser")
            page_text = extract_main_text(soup, max_chars=max_chars)
            results.append({
                "url": url,
                "title": title,
                "text": page_text
            })
        except Exception as e:
            print(f"Ошибка при обработке {url}: {e}")

    # Сохраняем JSON с текстами в другую папку
    save_to_drive(
        file_name,
        results,
        my_folder="17kQBohwKOQbBIwFl2yEQYWGUjuu-hf6V",
        file_format="json"
    )
    print(f"{section}: сохранено {len(results)} ссылок с текстами.")

#if datetime.today().weekday() == 3:
#read_top_urls("world")
#time.sleep(60)
#read_top_urls("rus")
#time.sleep(60)
#read_top_urls("prices")

def create_bullets(section):
    # Загружаем JSON с текстами топ-новостей
    list_file = f"{section}.json"
    file_id = find_file_in_drive(list_file, "17kQBohwKOQbBIwFl2yEQYWGUjuu-hf6V")
    list_content = download_text_file(file_id)

    # Если пришёл JSON-строкой, делаем красиво
    try:
        parsed_json = json.loads(list_content)
        pretty_json = json.dumps(parsed_json, ensure_ascii=False, indent=2)
    except json.JSONDecodeError:
        pretty_json = str(list_content)

    # Берём соответствующий prompt
    prompt_bullets = bullets_prompts.get(section, "")

    # Формируем prompt_parts
    raw_parts = [
        prompt_bullets,
        pretty_json
    ]

    prompt_parts = []
    for part in raw_parts:
        if isinstance(part, list):
            prompt_parts.append("\n".join(part))
        else:
            prompt_parts.append(str(part))

    try:
        response = model_obj.generate_content(prompt_parts)
    except Exception as e:
        print(f"Error in model.generate_content for {section}: {e}")
        return

    file_name = f"report_{section}.txt"
    save_to_drive(file_name, response.text, my_folder="18Lk31SodxZB3qgZm4ElX3BCejQihreVC", file_format="txt")
    print(f"{section}: буллиты успешно записаны.")

#if datetime.today().weekday() == 3:
#create_bullets("world")
#time.sleep(60)
#create_bullets("rus")
#time.sleep(60)
#create_bullets("prices")
#telegram_bullets()
