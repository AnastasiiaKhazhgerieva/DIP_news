# -*- coding: utf-8 -*-
"""dip_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17KKwbfVuL28SPhRasyS-FeUXLvmP2PNo
"""

# packages

import requests
import json
import time
import datetime
import os
import pandas as pd
import google.generativeai as genai
import io
import base64
try: # google colab –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è, –∫–æ–≥–¥–∞ —Ä–∞–Ω–∏–º —á–µ—Ä–µ–∑ workflow, –æ–Ω —Ç–∞–º –µ—Å—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –ø–æ—ç—Ç–æ–º—É –∏–º–µ–Ω–∏ –≤ PyPL —Ç–∞–∫–æ–≥–æ –Ω–µ—Ç
    from google.colab import userdata, drive
except ImportError:
    userdata = None
    drive = None
from datetime import date, timedelta, datetime
from typing import List
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaIoBaseDownload, MediaIoBaseUpload
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials

# Auxilliary
HEADERS = {"User-Agent": "Mozilla/5.0"}

encoded_token = os.environ.get("GOOGLE_TOKEN_B64")
if not encoded_token:
    raise RuntimeError("OAuth —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è GOOGLE_TOKEN_B64 –∑–∞–¥–∞–Ω–∞.")

token_bytes = base64.b64decode(encoded_token)
token_info = json.loads(token_bytes.decode("utf-8"))

creds = Credentials.from_authorized_user_info(token_info, scopes=["https://www.googleapis.com/auth/drive"])

if creds.expired and creds.refresh_token:
    creds.refresh(Request())
    
drive_service = build("drive", "v3", credentials=creds)

print("‚úÖ Credentials info:")
print("  - token:", creds.token[:20] + "...")
print("  - refresh_token:", bool(creds.refresh_token))
print("  - client_id:", creds.client_id)
print("  - quota_project_id:", creds.quota_project_id)
print("  - valid:", creds.valid)
print("  - expired:", creds.expired)
print("  - scopes:", creds.scopes)
# –ö—Ç–æ –∑–∞–ª–æ–≥–∏–Ω–µ–Ω?
about = drive_service.about().get(fields="user").execute()
print("‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç –∏–º–µ–Ω–∏:", about["user"]["displayName"], about["user"]["emailAddress"])


MY_FOLDER_ID = "1BwBFMln6HcGUfBFN4-UlNueOTKUehiRe" # –ø–∞–ø–∫–∞ reports –Ω–∞ google drive

# gemini api key
API_KEY = os.environ.get("GEMINI_API_KEY") # —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —á–µ—Ä–µ–∑ workflow
#API_KEY = userdata.get('gemini_api_key') # —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
genai.configure(api_key=API_KEY)
model_obj = genai.GenerativeModel('gemini-2.5-pro')

### TG Schedule bot

def telegram_lists():
    url = "https://api.telegram.org/bot6245425859:AAEHTh0EX9aE6qPhvHoclFPa3a8IBVRNeSM/sendMessage"
    payload = {
        "chat_id": "-4265314101",
        "text": "–ù–æ–≤–æ—Å—Ç–Ω–∞—è –∑–∞–ø–∏—Å–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞. –°–º. –æ—Ç—á—ë—Ç—ã –ø–æ <a href=\"https://clck.ru/3MTaGo\">—Å—Å—ã–ª–∫–µ</a>",
        "parse_mode": "HTML"
    }

    try:
        response = requests.post(url, data=payload)
        response.raise_for_status()
        print("Telegram message sent.")
    except requests.exceptions.RequestException as e:
        print(f"Failed to send Telegram message: {e}")

def telegram_bullets():
    url = "https://api.telegram.org/bot6245425859:AAEHTh0EX9aE6qPhvHoclFPa3a8IBVRNeSM/sendMessage"
    payload = {
        "chat_id": "-4265314101",
        "text": "–ì–æ—Ç–æ–≤—ã –±—É–ª–ª–∏—Ç—ã –∫ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –∑–∞–ø–∏—Å–∫–µ. –°–º. –æ—Ç—á—ë—Ç—ã –ø–æ <a href=\"https://clck.ru/3MTbwx\">—Å—Å—ã–ª–∫–µ</a>",
        "parse_mode": "HTML"
    }

    try:
        response = requests.post(url, data=payload)
        response.raise_for_status()
        print("Telegram message sent.")
    except requests.exceptions.RequestException as e:
        print(f"Failed to send Telegram message: {e}")

### Functions for google drive

def find_file_in_drive(file_name: str, folder_id = "1BwBFMln6HcGUfBFN4-UlNueOTKUehiRe") -> str:
    try:
        resp = drive_service.files().list(
            q=(
                f"name = '{file_name}' "
                f"and '{folder_id}' in parents "
                f"and trashed = false"
            ),
            spaces="drive",
            fields="files(id, name)",
            pageSize=1
        ).execute()
    except HttpError as e:
        raise RuntimeError(f"Error accessing Drive API: {e}")

    items = resp.get("files", [])
    if items:
        return items[0]["id"]

    raise FileNotFoundError(f"File '{file_name}' not found in folder {folder_id}.")

def download_text_file(fid: str) -> str:
    request = drive_service.files().get_media(fileId=fid)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        status, done = downloader.next_chunk()
    return fh.getvalue().decode("utf-8")

def save_to_drive(file_name: str, data, my_folder=MY_FOLDER_ID, file_format: str = "json"):
    if file_format not in ("json", "txt"):
        raise ValueError("file_format –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'json' –∏–ª–∏ 'txt'")

    if file_format == "txt":
        content_bytes = data.encode("utf-8") if isinstance(data, str) else str(data).encode("utf-8")
        mime_type = "text/plain"
    else:
        json_str = json.dumps(data, ensure_ascii=False, indent=2)
        content_bytes = json_str.encode("utf-8")
        mime_type = "application/json"

    # –ò—â–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ñ–∞–π–ª
    existing_file_id = None
    try:
        resp = drive_service.files().list(
            q=f"name = '{file_name}' and '{my_folder}' in parents and trashed = false",
            spaces="drive",
            fields="files(id, name)",
            pageSize=1
        ).execute()
        items = resp.get("files", [])
        if items:
            existing_file_id = items[0]["id"]
    except Exception as e:
        print("Warning: can't check if the file already exists:", e)

    fh = io.BytesIO(content_bytes)
    media = MediaIoBaseUpload(fh, mimetype=mime_type, resumable=False)

    if existing_file_id:
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –æ–±–Ω–æ–≤–∏—Ç—å
            updated = drive_service.files().update(
                fileId=existing_file_id,
                media_body=media
            ).execute()
            print(f"File '{file_name}' updated (ID={updated['id']}).")
            return updated
        except HttpError as e:
            if e.resp.status == 403 and "storageQuotaExceeded" in str(e):
                print(f"‚ö†Ô∏è Quota error on update ‚Äî deleting and recreating file '{file_name}'...")
                try:
                    drive_service.files().delete(fileId=existing_file_id).execute()
                    existing_file_id = None  # –ø–µ—Ä–µ–π—Ç–∏ –∫ —Å–æ–∑–¥–∞–Ω–∏—é
                except Exception as del_err:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ '{file_name}': {del_err}")
                    raise
            else:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ '{file_name}': {e}")
                raise

    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
    file_metadata = {
        "name": file_name,
        "parents": [my_folder],
        "mimeType": mime_type
    }
    try:
        created = drive_service.files().create(
            body=file_metadata,
            media_body=media,
            fields="id, webViewLink"
        ).execute()
        print(f"New file created: '{file_name}', (ID={created['id']}).")
        return created
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ '{file_name}': {e}")
        raise

### Functions for scrapping

## Defining and formatting dates
def get_last_dates(n_days=6, end_date=None):
    if end_date is None:
        end_date = date.today()
    return [end_date - timedelta(days=offset) for offset in range(n_days, -1, -1)]

def format_dates(dates_list, fmt="%Y-%m-%d"):
    return [d.strftime(fmt) for d in dates_list]

## Getting web page soup
def get_page_soup(url, headers=HEADERS, timeout=10):
    resp = requests.get(url, headers=headers, timeout=timeout)
    resp.raise_for_status()
    return BeautifulSoup(resp.text, "html.parser")

## Scrapers: Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat

# Kommersant scraper
def fetch_kom(rubrics, dates, output_file,
                   base_url_template="https://www.kommersant.ru/archive/rubric/{rubric}/day/{date}"):
    all_items = []
    seen_urls = set()

    for rubric in rubrics:
        for dt in dates:
            url = base_url_template.format(rubric=rubric, date=dt)
            print(f"Fetching Kommersant: {url}")
            try:
                soup = get_page_soup(url)
                scripts = soup.find_all("script", type="application/ld+json")

                for script in scripts:
                    raw = script.string
                    if not raw:
                        continue
                    try:
                        data = json.loads(raw)
                    except json.JSONDecodeError:
                        continue

                    for entry in data.get("itemListElement", []):
                        title = entry.get("name") or entry.get("headline")
                        link = entry.get("url")
                        if title and link and link not in seen_urls:
                            seen_urls.add(link)
                            all_items.append({"title": title, "url": link})
            except Exception as e:
                print(f"[ERROR] {e} when fetching {url}")

    save_to_drive(output_file, all_items, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved Kommersant data to {output_file}")


# Vedomosti scraper
def fetch_ved(dates, output_file,
              base_url_template="https://www.vedomosti.ru/newspaper/{date}"):
    all_news = []
    for dt in dates:
        url = base_url_template.format(date=dt)
        print(f"Fetching Vedomosti: {url}")
        try:
            soup = get_page_soup(url)
            for item in soup.select("li.waterfall__item"):
                a = item.select_one("a.waterfall__item-title")
                if not a:
                    continue
                title = a.get_text(strip=True)
                href = a.get("href", "")
                full_url = href if href.startswith("http") else f"https://www.vedomosti.ru{href}"
                all_news.append({"title": title, "url": full_url})
        except Exception as e:
            all_news.append({"error": str(e)})

    save_to_drive(output_file, all_news, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved Vedomosti data to {output_file}")

# RBC scraper

from datetime import date

def fetch_rbc(rubrics, dates, output_file,
              base_url_template="https://www.rbc.ru/{rubric}/?utm_source=topline"):

    ru_months = {
        '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4,
        '–º–∞—è': 5, '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8,
        '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12
    }
    today = date.today()
    collected = []

    for rubric in rubrics:
        page_url = base_url_template.format(rubric=rubric)
        print(f"Fetching RBC, {rubric}: {page_url}")
        soup = get_page_soup(page_url)

        anchors = soup.find_all("a", class_="news-feed__item")

        for idx, a in enumerate(anchors, start=1):
            # –≤–Ω—É—Ç—Ä–∏ anchor –∏—â–µ–º span, —É –∫–æ—Ç–æ—Ä–æ–≥–æ class —Å–æ–¥–µ—Ä–∂–∏—Ç "news-feed__item__title"
            title_span = a.find(
                "span",
                class_=lambda c: c and "news-feed__item__title" in c
            )
            if not title_span:
                continue

            # –î–ª—è –¥–∞—Ç—ã: –∏—â–µ–º span, —É –∫–æ—Ç–æ—Ä–æ–≥–æ class —Å–æ–¥–µ—Ä–∂–∏—Ç "news-feed__item__time"
            # –∏–ª–∏, –µ—Å–ª–∏ –Ω–µ—Ç, "news-feed__item__date"
            date_span = a.find(
                "span",
                class_=lambda c: c and "news-feed__item__time" in c
            )
            if not date_span:
                date_span = a.find(
                    "span",
                    class_=lambda c: c and "news-feed__item__date" in c
                )
            if not date_span:
                continue

            title = title_span.get_text(strip=True)
            href = a.get("href", "").strip()
            if not href:
                continue

            full_url = href if href.startswith("http") else urljoin(page_url, href)

            # raw_date –º–æ–∂–µ—Ç –±—ã—Ç—å –≤–∏–¥–∞ "28 –º–∞—è 17:52" –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ "17:52"
            raw_date = date_span.get_text(strip=True).replace("\xa0", " ").replace(",", "").strip()
            parts = raw_date.split()

            news_date = None
            if any(month in parts for month in ru_months):
                # —Ñ–æ—Ä–º–∞—Ç ["28","–º–∞—è","17:52"] –∏–ª–∏ ["28","–º–∞—è","2025","17:52"]
                try:
                    day = int(parts[0])
                except ValueError:
                    continue
                month_name = parts[1].lower()
                if month_name not in ru_months:
                    continue
                month = ru_months[month_name]
                year = today.year
                # –µ—Å–ª–∏ –≤ parts[2] —á–µ—Ç–≤—ë—Ä–∫–∞ —Ü–∏—Ñ—Ä, —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —ç—Ç–æ –≥–æ–¥
                if len(parts) >= 3 and parts[2].isdigit() and len(parts[2]) == 4:
                    year = int(parts[2])
                try:
                    candidate = datetime.date(year, month, day)
                except ValueError:
                    continue
                # –µ—Å–ª–∏ —ç—Ç–∞ –¥–∞—Ç–∞ —É–∂–µ –≤ –±—É–¥—É—â–µ–º, –∑–Ω–∞—á–∏—Ç, –≥–æ–¥ –±—ã–ª –ø—Ä–æ—à–ª—ã–π
                if candidate > today:
                    candidate = datetime.date(year - 1, month, day)
                news_date = candidate
            else:
                # –µ—Å–ª–∏ –Ω–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –º–µ—Å—è—Ü–∞, –∑–Ω–∞—á–∏—Ç raw_date = "HH:MM" —Å–µ–≥–æ–¥–Ω—è—à–Ω—è—è –¥–∞—Ç–∞
                news_date = today

            if news_date not in dates:
                continue

            collected.append({
                "title": title,
                "url": full_url
            })

    # —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
    unique = []
    seen = set()
    for item in collected:
        if item["url"] not in seen:
            seen.add(item["url"])
            unique.append(item)

    save_to_drive(output_file, unique, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved RBC data to {output_file}")

# Agro investor scraper - –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ª–æ–º–∞–µ—Ç—Å—è, –ø–æ—ç—Ç–æ–º—É –ø—É—Å—Ç—å –±—É–¥–µ—Ç –≤ –∫–æ–¥–µ –≤–∞—Ä–∏–∞–Ω—Ç —Å –æ—Ç–ª–∞–¥–∫–æ–π

def fetch_agro(dates, output_file, base_url="https://www.agroinvestor.ru/"):
    print(f"Fetching Agroinvestor: {base_url}")

    soup = get_page_soup(base_url)

    if soup is None:
        print("‚ùå Failed to retrieve or parse the page.")
        return

    print("‚úÖ Page fetched successfully.")
    news_list = []
    seen_links = set()

    ru_months = {
        "—è–Ω–≤–∞—Ä—è": 1, "—Ñ–µ–≤—Ä–∞–ª—è": 2, "–º–∞—Ä—Ç–∞": 3, "–∞–ø—Ä–µ–ª—è": 4,
        "–º–∞—è": 5, "–∏—é–Ω—è": 6, "–∏—é–ª—è": 7, "–∞–≤–≥—É—Å—Ç–∞": 8,
        "—Å–µ–Ω—Ç—è–±—Ä—è": 9, "–æ–∫—Ç—è–±—Ä—è": 10, "–Ω–æ—è–±—Ä—è": 11, "–¥–µ–∫–∞–±—Ä—è": 12
    }

    print(f"Looking for dates: {dates}")

    for time_tag in soup.find_all("time"):
        date_text = time_tag.get_text(strip=True).replace("\xa0", " ")
        if not date_text:
            continue

        print(f"üïí Found date text: '{date_text}'")
        parts = date_text.split()
        if len(parts) != 3:
            print("‚ö†Ô∏è Unexpected date format. Skipping.")
            continue

        day_str, month_str, year_str = parts
        try:
            day = int(day_str)
            year = int(year_str)
        except ValueError as e:
            print(f"‚ö†Ô∏è Could not parse day/year: {e}")
            continue

        month_str = month_str.lower()
        if month_str not in ru_months:
            print(f"‚ö†Ô∏è Unknown month: '{month_str}'")
            continue

        month = ru_months[month_str]
        try:
            date_obj = date(year, month, day)

        except Exception as e:
            print(f"‚ö†Ô∏è Failed to construct date object: {e}")
            continue

        print(f"üìÖ Parsed date: {date_obj}")
        if date_obj not in dates:
            print("‚è© Date not in requested range. Skipping.")
            continue

        anchor = time_tag.find_previous("a")
        if not anchor:
            print("‚ö†Ô∏è No previous anchor tag found.")
            continue

        title = anchor.get_text(strip=True)
        href = anchor.get("href")
        if not href or not title:
            print("‚ö†Ô∏è Missing title or href. Skipping.")
            continue

        url = urljoin(base_url, href.strip())
        if url in seen_links:
            print(f"üîÅ Duplicate link: {url}")
            continue
        seen_links.add(url)

        print(f"‚úÖ Added news: {title} - {url}")
        news_list.append({
            "title": title,
            "link": url
        })

    save_to_drive(output_file, news_list, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"üíæ Saved {len(news_list)} news items to {output_file}")


# RG.ru scraper

def fetch_rg(rubrics, dates, output_file,
             base_url_template="https://rg.ru/tema/ekonomika/{rubric}"):
    all_news = []
    for rubric in rubrics:
        url = base_url_template.format(rubric=rubric)
        print(f"Fetching RG, {rubric}: {url}")
        soup = get_page_soup(url)
        for title_span in soup.find_all("span", class_="ItemOfListStandard_title__Ajjlf"):
            parent_a = title_span.find_parent("a")
            if not parent_a:
                continue
            href = parent_a.get("href", "").strip()
            if not href:
                continue
            full_url = href if href.startswith("http") else f"https://rg.ru{href}"

            date_a = title_span.find_previous("a", class_="ItemOfListStandard_datetime__GstJi")
            if not date_a:
                continue
            date_href = date_a.get("href", "").strip()
            parts = date_href.strip("/").split("/")  # ['2025','05','30',...]
            if len(parts) < 3:
                continue
            try:
                y, m, d = map(int, parts[:3])
                news_date = date(y, m, d)
            except ValueError:
                continue

            if news_date not in dates:
                continue

            all_news.append({
                "title": title_span.get_text(strip=True),
                "url": full_url
            })

    unique = []
    seen = set()
    for item in all_news:
        if item["url"] not in seen:
            seen.add(item["url"])
            unique.append(item)

    save_to_drive(output_file, unique, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved RG data to {output_file}")

# RIA scraper

def fetch_ria(dates, output_file, base_url_template="https://ria.ru/economy/"):
    print("Fetching RIA: https://ria.ru/economy/")
    soup = get_page_soup(base_url_template)
    collected = []

    # Each news item has <a itemprop="url" href="..."></a>
    for a in soup.find_all("a", itemprop="url"):
        href = a.get("href", "").strip()
        if not href:
            continue
        full_url = href if href.startswith("http") else f"https://ria.ru{href}"

        # Next meta tag with itemprop="name" holds the title
        name_meta = a.find_next("meta", itemprop="name")
        if not name_meta:
            continue
        title = name_meta.get("content", "").strip()
        if not title:
            continue
        parsed = urlparse(full_url)
        parts = parsed.path.lstrip("/").split("/")
        if not parts or len(parts[0]) != 8 or not parts[0].isdigit():
            continue
        y, m, d = int(parts[0][:4]), int(parts[0][4:6]), int(parts[0][6:8])
        try:
            news_date = date(y, m, d)
        except ValueError:
            continue

        if news_date in dates:
            collected.append({
                "title": title,
                "url": full_url
            })

    unique = []
    seen = set()
    for item in collected:
        if item["url"] not in seen:
            seen.add(item["url"])
            unique.append(item)

    save_to_drive(output_file, unique, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")

    print(f"Saved RIA data to {output_file}")


# Autostat scraper

def fetch_autostat(dates, output_file,
                   rubrics=[21, 8, 13, 70, 71],
                   base_url_template="https://m.autostat.ru/news/themes-{rubric}/"):

    if dates is None:
        raise ValueError("Argument 'dates' must be provided as a list of datetime.date objects.")

    all_collected = []
    seen_urls = set()

    ru_months = {
        '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4,
        '–º–∞—è': 5, '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8,
        '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12
    }
    today = date.today()
    yesterday = today - timedelta(days=1)

    for rubric in rubrics:
        url = base_url_template.format(rubric=rubric)
        print(f"Fetching Autostat, {rubric}: {url}")
        soup = get_page_soup(url)
        if not soup:
            print(f"  (!) Failed to retrieve or parse page for rubric {rubric}")
            continue

        titles = soup.find_all("p", class_="Block-title")
        if not titles:
            print(f"    (!) No <p class='Block-title'> elements found on {url}")
            continue

        for title_p in titles:
            title = title_p.get_text(strip=True)
            if not title:
                continue

            link_a = title_p.find_parent("a", class_="Block-link")
            if not link_a:
                continue
            href = link_a.get("href", "").strip()
            if not href:
                continue
            full_url = urljoin("https://www.autostat.ru", href)

            date_p = title_p.find_next("p", class_="Block-date")
            if not date_p:
                continue
            date_text = date_p.get_text(strip=True)  # e.g. "–°–µ–≥–æ–¥–Ω—è, 15:48" or "28 –º–∞—è, 15:48"
            date_part = date_text.split(",")[0].strip().lower()

            if date_part == "—Å–µ–≥–æ–¥–Ω—è":
                news_date = today
            elif date_part == "–≤—á–µ—Ä–∞":
                news_date = yesterday
            else:
                parts = date_part.split()
                if len(parts) != 2:
                    continue
                day_str, month_str = parts
                try:
                    day = int(day_str)
                    month = ru_months.get(month_str)
                    if not month:
                        continue
                    news_date = date(today.year, month, day)
                    if news_date > today:
                        news_date = date(today.year - 1, month, day)
                except Exception:
                    continue

            if news_date in dates and full_url not in seen_urls:
                all_collected.append({
                    "title": title,
                    "url": full_url
                })
                seen_urls.add(full_url)

    save_to_drive(output_file, all_collected, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")

    print(f"Saved Autostat data to {output_file}")

#with open('agro.json', encoding='utf-8') as f:
#    data = json.load(f)
#print(json.dumps(data, ensure_ascii=False, indent=2))

# Parameters
days_before = 1
dates = get_last_dates(days_before)
dates_kom = format_dates(dates, fmt="%Y-%m-%d")
dates_ved = format_dates(dates, fmt="%Y/%m/%d")

rubrics_kom_rus = [3, 4, 40]
rubrics_kom_world = [3, 5]
rubrics_kom_prices = [41]
rubrics_rbc = ["economics", "business", "finances"]
rubrics_rg = ["politekonom", "industria", "business", "finansy", "kazna", "rabota", "pensii", "vnesh", "apk", "tovary", "turizm"]
rubrics_auto = [21, 8, 13, 70, 71]

# Fetching
#fetch_kom(rubrics_kom_rus, dates_kom, "kom_rus.json")
#fetch_kom(rubrics_kom_world, dates_kom, "kom_world.json")
#fetch_kom(rubrics_kom_prices, dates_kom, "kom_prices.json")
#fetch_ved(dates_ved, "ved.json")
#fetch_rbc(rubrics_rbc, dates, "rbc.json")
#try:
#    fetch_agro(dates, "agro.json")
#except Exception as e:
#    pass
#fetch_rg(rubrics_rg, dates, "rg.json")
#fetch_ria(dates, "ria.json")
#fetch_autostat(dates, "autostat.json", rubrics_auto)

# Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat
section_to_files = {
    "world": [
        "kom_world.json",
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        #"rg.json",
        "ria.json"
    ],
    "rus": [
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        "rg.json",
        "ria.json"
    ],
    "prices": [
        "kom_prices.json",
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        #"rg.json",
        "ria.json",
        "autostat.json"
    ]
}

# drive.mount('/content/drive')

### Prompts

###
### news lists
file_id = find_file_in_drive("lists_world.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    lists_world = download_text_file(file_id)
except Exception as e:
    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞:", e)
    lists_world = ""

file_id = find_file_in_drive("lists_rus.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    lists_rus = download_text_file(file_id)
except Exception as e:
    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞:", e)
    lists_rus = ""

file_id = find_file_in_drive("lists_prices.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    lists_prices = download_text_file(file_id)
except Exception as e:
    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞:", e)
    lists_prices = ""

lists_prompts = {
        "world": lists_world,
        "rus": lists_rus,
        "prices": lists_prices
}

### prioritise
file_id = find_file_in_drive("prioritise_world.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    prioritise_world = download_text_file(file_id)
except Exception as e:
    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞:", e)
    prioritise_world = ""

file_id = find_file_in_drive("prioritise_rus.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    prioritise_rus = download_text_file(file_id)
except Exception as e:
    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞:", e)
    prioritise_rus = ""

file_id = find_file_in_drive("prioritise_prices.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    prioritise_prices = download_text_file(file_id)
except Exception as e:
    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞:", e)
    prioritise_prices = ""

prioritise_prompts = {
        "world": prioritise_world,
        "rus": prioritise_rus,
        "prices": prioritise_prices
}

### design

file_id = find_file_in_drive("design.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    prompt_design = download_text_file(file_id)
except Exception as e:
    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞:", e)
    prompt_design = ""

### top
file_id = find_file_in_drive("top_world.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    top_world = download_text_file(file_id)
except Exception as e:
    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞:", e)
    top_world = ""

file_id = find_file_in_drive("top_rus.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    top_rus = download_text_file(file_id)
except Exception as e:
    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞:", e)
    top_rus = ""

file_id = find_file_in_drive("top_prices.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    top_prices = download_text_file(file_id)
except Exception as e:
    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞:", e)
    top_prices = ""

top_prompts = {
        "world": top_world,
        "rus": top_rus,
        "prices": top_prices
}

### bullets
file_id = find_file_in_drive("bullets_world.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    bullets_world = download_text_file(file_id)
except Exception as e:
    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞:", e)
    bullets_world = ""

file_id = find_file_in_drive("bullets_rus.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    bullets_rus = download_text_file(file_id)
except Exception as e:
    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞:", e)
    bullets_rus = ""

file_id = find_file_in_drive("bullets_prices.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    bullets_prices = download_text_file(file_id)
except Exception as e:
    print("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞:", e)
    bullets_prices = ""

bullets_prompts = {
        "world": bullets_world,
        "rus": bullets_rus,
        "prices": bullets_prices
}

example = '–ü—Ä–∏–º–µ—Ä –≤–µ—Ä–Ω–æ–≥–æ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è:\r\n1.\t–†–æ—Å—Å—Ç–∞—Ç –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—é –≤—ã–ø—É—Å–∫–∞ –±–∞–∑–æ–≤—ã—Ö –æ—Ç—Ä–∞—Å–ª–µ–π\r\nhttps://www.kommersant.ru/doc/7329366 \r\n2.\t–°—Ç—Ä–æ–∏—Ç–µ–ª–∏ –ø—Ä–æ—Å—è—Ç —Å–º—è–≥—á–∏—Ç—å –ø—Ä–∞–≤–∏–ª–∞ —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏—è –∞–≤–∞–Ω—Å–∞–º–∏\r\nhttps://www.rbc.ru/newspaper/2024/11/25/673f6abf9a7947de58a24847 \r\n3.\t–í –£–ª—å—è–Ω–æ–≤—Å–∫–µ –æ—Ç–∫—Ä—ã–ª—Å—è –Ω–æ–≤—ã–π –∑–∞–≤–æ–¥ –≥—Ä—É–∑–æ–≤–∏–∫–æ–≤ –°–æ–ª–ª–µ—Ä—Å\r\nhttps://tass.ru/ekonomika/22497349 \r\n4.\t –î–æ–±—ã—á–∞ –≥–∞–∑–∞ –∑–∞ 9 –º–µ—Å—è—Ü–µ–≤ –≤—ã—Ä–æ—Å–ª–∞ –Ω–∞ 8% –≥/–≥ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∑–∞ —Å—á–µ—Ç –ì–∞–∑–ø—Ä–æ–º–∞\r\nhttps://www.interfax.ru/business/994801 \r\n'

def extract_json(text: str):
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –≤ —Å—Ç—Ä–æ–∫–µ `text` JSON-—Å–ø–∏—Å–æ–∫ –∏–ª–∏ JSON-–æ–±—ä–µ–∫—Ç –∏ –≤–µ—Ä–Ω—É—Ç—å –µ–≥–æ –∫–∞–∫ Python-—Å—Ç—Ä—É–∫—Ç—É—Ä—É.
    –°–Ω–∞—á–∞–ª–∞ –∏—â–µ—Ç JSON-–º–∞—Å—Å–∏–≤ [...], –µ—Å–ª–∏ –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç ‚Äî JSON-–æ–±—ä–µ–∫—Ç {...}.
    –ï—Å–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –Ω–µ—Ç –∏–ª–∏ –æ–Ω –Ω–µ–≤–∞–ª–∏–¥–µ–Ω ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None.
    """
    # 1) –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON-–º–∞—Å—Å–∏–≤: –∏—â–µ–º –ø–µ—Ä–≤—É—é '[' –∏ –ø–æ—Å–ª–µ–¥–Ω—é—é ']'
    start = text.find('[')
    end = text.rfind(']')
    if 0 <= start < end:
        candidate = text[start:end+1]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass

    # 2) –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –º–∞—Å—Å–∏–≤, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON-–æ–±—ä–µ–∫—Ç: –ø–µ—Ä–≤—É—é '{' –∏ –ø–æ—Å–ª–µ–¥–Ω—é—é '}'
    start = text.find('{')
    end = text.rfind('}')
    if 0 <= start < end:
        candidate = text[start:end+1]
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass

    # 3) –ï—Å–ª–∏ –Ω–∏ —Ç–æ, –Ω–∏ –¥—Ä—É–≥–æ–µ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å ‚Äî –æ—Ç–¥–∞–¥–∏–º None
    return None

def create_news_lists(section):
    # –ï—Å–ª–∏ —Å–µ–≥–æ–¥–Ω—è –Ω–µ —Å—É–±–±–æ—Ç–∞, –ø—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª <section>.json
    if datetime.today().weekday() != 5:  # 5 = Saturday
        try:
            existing_id = find_file_in_drive(f"{section}.json", "1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe")
            existing_text = download_text_file(existing_id)
            try:
                combined_items = json.loads(existing_text)
            except json.JSONDecodeError:
                combined_items = []
        except Exception:
            combined_items = []
    else:
        combined_items = []

    seen_urls = {item["url"] for item in combined_items if isinstance(item, dict) and "url" in item}

    # –î–æ—Å—Ç–∞—ë–º —Å–ø–∏—Å–æ–∫ JSON-—Ñ–∞–π–ª–æ–≤ –∏ prompt
    json_files = section_to_files[section]
    prompt_list = lists_prompts.get(section, "")

    for json_filename in json_files:
        base_name, ext = os.path.splitext(json_filename)
        if ext.lower() != ".json":
            print(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º '{json_filename}', —Ç.–∫. –Ω–µ .json-—Ñ–∞–π–ª.")
            continue

        try:
            file_id = find_file_in_drive(json_filename, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
            raw_text = download_text_file(file_id)
        except FileNotFoundError:
            print(f"–§–∞–π–ª '{json_filename}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ '{json_filename}': {e}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        if not isinstance(raw_text, str) or not raw_text.strip():
            print(f"JSON '{json_filename}' –ø—É—Å—Ç–æ–π. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        try:
            news_data = json.loads(raw_text)
        except json.JSONDecodeError as e:
            print(f"–û—à–∏–±–∫–∞ JSON –≤ '{json_filename}': {e}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        if isinstance(news_data, (list, dict)) and len(news_data) == 0:
            print(f"JSON '{json_filename}' —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—É—Å—Ç—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        news_json_string = json.dumps(news_data, ensure_ascii=False, indent=2)

        raw_parts = [prompt_list, news_json_string]

        prompt_parts = []
        for part in raw_parts:
            if isinstance(part, list):
                prompt_parts.append("\n".join(part))
            else:
                prompt_parts.append(str(part))

        try:
            response = model_obj.generate_content(prompt_parts)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –º–æ–¥–µ–ª–∏ –¥–ª—è '{json_filename}': {e}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        if not hasattr(response, "candidates") or not response.candidates:
            print(f"–ú–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è '{json_filename}'. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        if not hasattr(response, "text") or response.text is None:
            print(f"response.text –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –¥–ª—è '{json_filename}'. –í–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–∏—á–∏–Ω–∞: finish_reason=1.")
            print("–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π prompt:\n", "\n\n".join(prompt_parts)[:300], "‚Ä¶")
            continue

        raw_reply = response.text

        items = extract_json(raw_reply)
        if items is None:
            print(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è '{json_filename}' –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞–ª–∏–¥–Ω—ã–π JSON:\n{raw_reply[:200]}‚Ä¶ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        if isinstance(items, dict):
            items = [items]

        if not isinstance(items, list):
            print(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è '{json_filename}' –≤–µ—Ä–Ω—É–ª –Ω–µ —Å–ø–∏—Å–æ–∫, –∞ {type(items)}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        for entry in items:
            url = entry.get("url")
            title = entry.get("title")
            if not title or not url or url in seen_urls:
                continue
            seen_urls.add(url)
            combined_items.append({"title": title, "url": url})

    if not combined_items:
        print(f"For section '{section}', zero JSONs were successfully processed.")
        return

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π JSON –≤ —Ñ–∞–π–ª <section>.json
    output_file = f"{section}.json"
    save_to_drive(output_file, combined_items, my_folder="1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe")

# Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat
create_news_lists("world")
time.sleep(60)
create_news_lists("rus")
time.sleep(60)
create_news_lists("prices")

def prioritise(section):
    file_name = f"{section}.json"
    folder_id = "1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe"

    try:
        file_id = find_file_in_drive(file_name, folder_id)
        news_list_raw = download_text_file(file_id)
    except FileNotFoundError:
        print(f"‚ùå –§–∞–π–ª {file_name} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–∞–ø–∫–µ {folder_id}.")
        return
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ {file_name}: {e}")
        return

    if not news_list_raw.strip():
        print(f"‚ùå –§–∞–π–ª {file_name} –ø—É—Å—Ç–æ–π.")
        return

    prompt_prioritise = prioritise_prompts.get(section, "")

    raw_parts = [prompt_prioritise, news_list_raw]

    prompt_parts = []
    for part in raw_parts:
        if isinstance(part, list):
            prompt_parts.append("\n".join(part))
        else:
            prompt_parts.append(str(part))

    try:
        response = model_obj.generate_content(prompt_parts)
        response_text = getattr(response, "text", "").strip()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è '{file_name}': {e}")
        return

    # –£–±–∏—Ä–∞–µ–º markdown-–æ–±–µ—Ä—Ç–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
    if response_text.startswith("```json"):
        response_text = response_text[7:]
    if response_text.endswith("```"):
        response_text = response_text[:-3]
    response_text = response_text.strip()

    # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ç–≤–µ—Ç –∫–∞–∫ JSON
    try:
        filtered_list = json.loads(response_text)
        if not isinstance(filtered_list, list):
            raise ValueError(f"–û–∂–∏–¥–∞–ª—Å—è —Å–ø–∏—Å–æ–∫, –∞ –ø—Ä–∏—à–ª–æ {type(filtered_list)}")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –≤ prioritise({section}): {e}")
        save_to_drive(file_name.replace(".json", "_prioritise_error.txt"), response_text, folder_id, file_format="txt")
        return

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    save_to_drive(file_name, filtered_list, folder_id, file_format="json")
    print(f"‚úÖ prioritise({section}) ‚Äî —Å–æ—Ö—Ä–∞–Ω—ë–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON.")

#prioritise("world")
#time.sleep(60)
#prioritise("rus")
#time.sleep(60)
#prioritise("prices")

def design(section):
    # –ü–æ–ª—É—á–∞–µ–º JSON —Å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏
    file_name_json = f"{section}.json"
    file_id = find_file_in_drive(file_name_json, "1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe")
    news_list_raw = download_text_file(file_id)

    raw_parts = [
        prompt_design,
        example,
        news_list_raw
    ]

    prompt_parts = []
    for part in raw_parts:
        if isinstance(part, list):
            prompt_parts.append("\n".join(part))
        else:
            prompt_parts.append(str(part))

    try:
        response = model_obj.generate_content(prompt_parts)
    except Exception as e:
        print(f"Error in model.generate_content for '{file_name_json}': {e}.")
        return

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π .txt —Ñ–∞–π–ª
    file_name_txt = f"{section}.txt"
    save_to_drive(file_name_txt, response.text, "1BwBFMln6HcGUfBFN4-UlNueOTKUehiRe", file_format="txt")

#design("world")
#time.sleep(60)
#design("rus")
#time.sleep(60)
#design("prices")
#telegram_lists()

def read_top_urls(section, max_chars=1500):

    def extract_main_text(soup, max_chars=1500, min_paragraph_len=50, max_paragraphs=5):
        paragraphs = []
        for p in soup.find_all('p'):
            text = p.get_text(" ", strip=True)
            if len(text) < min_paragraph_len:
                continue
            low = text.lower()
            if any(word in low for word in ["cookie", "subscribe", "advert", "—Ä–µ–∫–ª–∞–º–∞", "–ø–æ–¥–ø–∏—à–∏—Ç–µ—Å—å"]):
                continue
            paragraphs.append(text)
            if len(paragraphs) >= max_paragraphs:
                break

        combined_text = " ".join(paragraphs)
        if len(combined_text) > max_chars:
            combined_text = combined_text[:max_chars].rsplit(" ", 1)[0] + "..."
        return combined_text

    # –ó–∞–≥—Ä—É–∂–∞–µ–º JSON —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏ –ø–æ—Å–ª–µ prioritise
    file_name = f"{section}.json"
    file_id = find_file_in_drive(file_name, "1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe")
    news_list = download_text_file(file_id)

    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç
    prompt_top = top_prompts.get(section, "")
    raw_parts = [prompt_top, news_list]

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–ø —Å—Å—ã–ª–æ–∫
    try:
        response = model_obj.generate_content(raw_parts)
        top_links_json = json.loads(response.text)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–æ–ø —Å—Å—ã–ª–æ–∫ –¥–ª—è {section}: {e}")
        return

    # –°–∫–∞—á–∏–≤–∞–µ–º –∏ –æ—á–∏—â–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    results = []
    for item in top_links_json:
        url = item.get("url") or item.get("URL")
        title = item.get("title", "")
        if not url:
            continue
        try:
            resp = requests.get(url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(resp.text, "html.parser")
            page_text = extract_main_text(soup, max_chars=max_chars)
            results.append({
                "url": url,
                "title": title,
                "text": page_text
            })
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {e}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –≤ –¥—Ä—É–≥—É—é –ø–∞–ø–∫—É
    save_to_drive(
        file_name,
        results,
        my_folder="17kQBohwKOQbBIwFl2yEQYWGUjuu-hf6V",
        file_format="json"
    )
    print(f"{section}: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(results)} —Å—Å—ã–ª–æ–∫ —Å —Ç–µ–∫—Å—Ç–∞–º–∏.")

#if datetime.today().weekday() == 3:
#read_top_urls("world")
#time.sleep(60)
#read_top_urls("rus")
#time.sleep(60)
#read_top_urls("prices")

def create_bullets(section):
    # –ó–∞–≥—Ä—É–∂–∞–µ–º JSON —Å —Ç–µ–∫—Å—Ç–∞–º–∏ —Ç–æ–ø-–Ω–æ–≤–æ—Å—Ç–µ–π
    list_file = f"{section}.json"
    file_id = find_file_in_drive(list_file, "17kQBohwKOQbBIwFl2yEQYWGUjuu-hf6V")
    list_content = download_text_file(file_id)

    # –ï—Å–ª–∏ –ø—Ä–∏—à—ë–ª JSON-—Å—Ç—Ä–æ–∫–æ–π, –¥–µ–ª–∞–µ–º –∫—Ä–∞—Å–∏–≤–æ
    try:
        parsed_json = json.loads(list_content)
        pretty_json = json.dumps(parsed_json, ensure_ascii=False, indent=2)
    except json.JSONDecodeError:
        pretty_json = str(list_content)

    # –ë–µ—Ä—ë–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π prompt
    prompt_bullets = bullets_prompts.get(section, "")

    # –§–æ—Ä–º–∏—Ä—É–µ–º prompt_parts
    raw_parts = [
        prompt_bullets,
        pretty_json
    ]

    prompt_parts = []
    for part in raw_parts:
        if isinstance(part, list):
            prompt_parts.append("\n".join(part))
        else:
            prompt_parts.append(str(part))

    try:
        response = model_obj.generate_content(prompt_parts)
    except Exception as e:
        print(f"Error in model.generate_content for {section}: {e}")
        return

    file_name = f"report_{section}.txt"
    save_to_drive(file_name, response.text, my_folder="18Lk31SodxZB3qgZm4ElX3BCejQihreVC", file_format="txt")
    print(f"{section}: –±—É–ª–ª–∏—Ç—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã.")

#if datetime.today().weekday() == 3:
#create_bullets("world")
#time.sleep(60)
#create_bullets("rus")
#time.sleep(60)
#create_bullets("prices")
#telegram_bullets()
