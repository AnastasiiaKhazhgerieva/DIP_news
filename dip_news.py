# -*- coding: utf-8 -*-
"""dip_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17KKwbfVuL28SPhRasyS-FeUXLvmP2PNo
"""

# packages

import requests
import json
import time
import datetime
import os
import pandas as pd
import google.generativeai as genai
import io
try: # google colab не запускается, когда раним через workflow, он там есть по умолчанию, поэтому имени в PyPL такого нет
    from google.colab import userdata, drive
except ImportError:
    userdata = None
    drive = None
from datetime import date, timedelta, datetime
from typing import List
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaIoBaseDownload, MediaIoBaseUpload

# Auxilliary
HEADERS = {"User-Agent": "Mozilla/5.0"}

# service account credentials (to access google drive)
sa_json = os.environ.get("GCP_SA_KEY") # строка для запуска через workflow
#with open("hybrid-sunbeam-461621-s8-9191b51ccff5.json", 'r', encoding='utf-8') as f: # для локального запуска
#   sa_json = f.read()  # для локального запуска
if not sa_json:
    raise RuntimeError("Сервисный аккаунт не найден")

creds_info = json.loads(sa_json)

creds = Credentials.from_service_account_info(
    creds_info,
    scopes=["https://www.googleapis.com/auth/drive"]
)
drive_service = build("drive", "v3", credentials=creds)

MY_FOLDER_ID = "1BwBFMln6HcGUfBFN4-UlNueOTKUehiRe" # папка reports на google drive

# gemini api key
API_KEY = os.environ.get("GEMINI_API_KEY") # строка для запуска через workflow
#API_KEY = userdata.get('gemini_api_key') # строка для локального запуска
genai.configure(api_key=API_KEY)
model_obj = genai.GenerativeModel('gemini-1.5-flash')

### Functions for google drive

def find_file_in_drive(file_name: str, folder_id = "1BwBFMln6HcGUfBFN4-UlNueOTKUehiRe") -> str:
    # Ищем файл в конкретной папке folder_id
    try:
        resp = drive_service.files().list(
            q=(
                f"name = '{file_name}' "
                f"and '{folder_id}' in parents "
                f"and trashed = false"
            ),
            spaces="drive",
            fields="files(id, name)",
            pageSize=1
        ).execute()
    except HttpError as e:
        raise RuntimeError(f"Ошибка при запросе к Drive API: {e}")

    items = resp.get("files", [])
    if items:
        return items[0]["id"]

    raise FileNotFoundError(f"File '{file_name}' not found in folder {folder_id}.")

def download_text_file(fid: str) -> str:
    request = drive_service.files().get_media(fileId=fid)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        status, done = downloader.next_chunk()
    return fh.getvalue().decode("utf-8")

def save_to_drive(file_name: str, data, my_folder = MY_FOLDER_ID):
    """
    Сохраняет `data` на Google Drive в файл file_name внутри папки MY_FOLDER_ID.
    Если data — строка, файл будет сохранён как plain text;
    иначе data считается JSON-совместимой структурой и сериализуется в JSON.

    Если файл с таким именем уже есть — перезаписывает, иначе создаёт новый.
    Возвращает метаданные созданного/обновлённого файла.
    """
    # 1) Подготовим байты и mimeType в зависимости от типа data
    if isinstance(data, str):
        # Сохраняем как plain text
        content_bytes = data.encode("utf-8")
        mime_type = "text/plain"
    else:
        # Считаем, что data — это Python-структура (dict, list и т.д.), сохраняем как JSON
        json_str = json.dumps(data, ensure_ascii=False, indent=2)
        content_bytes = json_str.encode("utf-8")
        mime_type = "application/json"

    # 2) Проверим, есть ли файл с таким именем в нужной папке
    existing_file_id = None
    try:
        resp = drive_service.files().list(
            q=f"name = '{file_name}' and '{my_folder}' in parents and trashed = false",
            spaces="drive",
            fields="files(id, name)",
            pageSize=1
        ).execute()
        items = resp.get("files", [])
        if items:
            existing_file_id = items[0]["id"]
    except Exception as e:
        print("Warning: не удалось проверить существование файла в Drive:", e)

    # 3) Подготовим медиаконтент
    fh = io.BytesIO(content_bytes)
    media = MediaIoBaseUpload(fh, mimetype=mime_type, resumable=False)

    if existing_file_id:
        # 4a) Если файл уже есть — перезапишем его
        try:
            updated = drive_service.files().update(
                fileId=existing_file_id,
                media_body=media
            ).execute()
            print(f"Файл '{file_name}' обновлён (ID={updated['id']}).")
            return updated
        except Exception as e:
            print(f"Ошибка при обновлении файла '{file_name}': {e}")
            raise
    else:
        # 4b) Если файла нет — создадим новый в вашей папке
        file_metadata = {
            "name": file_name,
            "parents": [my_folder],
            "mimeType": mime_type
        }
        try:
            created = drive_service.files().create(
                body=file_metadata,
                media_body=media,
                fields="id, webViewLink"
            ).execute()
            print(f"Создан новый файл '{file_name}' (ID={created['id']}).")
            return created
        except Exception as e:
            print(f"Ошибка при создании файла '{file_name}': {e}")
            raise

### Functions for scrapping

## Defining and formatting dates
def get_last_dates(n_days=6, end_date=None):
    if end_date is None:
        end_date = date.today()
    return [end_date - timedelta(days=offset) for offset in range(n_days, -1, -1)]

def format_dates(dates_list, fmt="%Y-%m-%d"):
    return [d.strftime(fmt) for d in dates_list]

## Getting web page soup
def get_page_soup(url, headers=HEADERS, timeout=10):
    resp = requests.get(url, headers=headers, timeout=timeout)
    resp.raise_for_status()
    return BeautifulSoup(resp.text, "html.parser")

## Scrapers: Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat

# Kommersant scraper
def fetch_kom(rubrics, dates, output_file,
                   base_url_template="https://www.kommersant.ru/archive/rubric/{rubric}/day/{date}"):
    all_items = []
    seen_urls = set()

    for rubric in rubrics:
        for dt in dates:
            url = base_url_template.format(rubric=rubric, date=dt)
            print(f"Fetching Kommersant: {url}")
            try:
                soup = get_page_soup(url)
                scripts = soup.find_all("script", type="application/ld+json")

                for script in scripts:
                    raw = script.string
                    if not raw:
                        continue
                    try:
                        data = json.loads(raw)
                    except json.JSONDecodeError:
                        continue

                    for entry in data.get("itemListElement", []):
                        title = entry.get("name") or entry.get("headline")
                        link = entry.get("url")
                        if title and link and link not in seen_urls:
                            seen_urls.add(link)
                            all_items.append({"title": title, "url": link})
            except Exception as e:
                print(f"[ERROR] {e} when fetching {url}")

    #with open(output_file, "w", encoding="utf-8") as f:
    #    json.dump(all_items, f, ensure_ascii=False, indent=2)
    save_to_drive(output_file, all_items, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved Kommersant data to {output_file}")


# Vedomosti scraper
def fetch_ved(dates, output_file,
              base_url_template="https://www.vedomosti.ru/newspaper/{date}"):
    all_news = []
    for dt in dates:
        url = base_url_template.format(date=dt)
        print(f"Fetching Vedomosti: {url}")
        try:
            soup = get_page_soup(url)
            for item in soup.select("li.waterfall__item"):
                a = item.select_one("a.waterfall__item-title")
                if not a:
                    continue
                title = a.get_text(strip=True)
                href = a.get("href", "")
                full_url = href if href.startswith("http") else f"https://www.vedomosti.ru{href}"
                all_news.append({"title": title, "url": full_url})
        except Exception as e:
            all_news.append({"error": str(e)})

    #with open(output_file, 'w', encoding='utf-8') as f:
    #    json.dump(all_news, f, ensure_ascii=False, indent=2)
    save_to_drive(output_file, all_news, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved Vedomosti data to {output_file}")

# RBC scraper

from datetime import date

def fetch_rbc(rubrics, dates, output_file,
              base_url_template="https://www.rbc.ru/{rubric}/?utm_source=topline"):
    """
    Собирает новости с разделов RBC, фильтруя по датам из списка dates (datetime.date).
    Вместо жёстких class="news-feed__title" и т.п. ищем span, в котором в class есть
    подстрока "news-feed__item__title" (для заголовка) и "news-feed__item__time"
    или "news-feed__item__date" (для даты).
    Сохраняет результат [{"title","url","date"}] без дубликатов.
    """
    ru_months = {
        'января': 1, 'февраля': 2, 'марта': 3, 'апреля': 4,
        'мая': 5, 'июня': 6, 'июля': 7, 'августа': 8,
        'сентября': 9, 'октября': 10, 'ноября': 11, 'декабря': 12
    }
    today = date.today()
    collected = []

    for rubric in rubrics:
        page_url = base_url_template.format(rubric=rubric)
        print(f"Fetching RBC, {rubric}: {page_url}")
        soup = get_page_soup(page_url)

        # Ищем все <a class="news-feed__item ...">
        anchors = soup.find_all("a", class_="news-feed__item")

        for idx, a in enumerate(anchors, start=1):
            # внутри anchor ищем span, у которого class содержит "news-feed__item__title"
            title_span = a.find(
                "span",
                class_=lambda c: c and "news-feed__item__title" in c
            )
            if not title_span:
                # Пропускаем, если внутри нет span с нужной подстрокой в class
                continue

            # Для даты: ищем span, у которого class содержит "news-feed__item__time"
            # или, если нет, "news-feed__item__date"
            date_span = a.find(
                "span",
                class_=lambda c: c and "news-feed__item__time" in c
            )
            if not date_span:
                date_span = a.find(
                    "span",
                    class_=lambda c: c and "news-feed__item__date" in c
                )
            if not date_span:
                continue

            title = title_span.get_text(strip=True)
            href = a.get("href", "").strip()
            if not href:
                continue

            full_url = href if href.startswith("http") else urljoin(page_url, href)

            # raw_date может быть вида "28 мая 17:52" или просто "17:52"
            raw_date = date_span.get_text(strip=True).replace("\xa0", " ").replace(",", "").strip()
            parts = raw_date.split()

            news_date = None
            if any(month in parts for month in ru_months):
                # формат ["28","мая","17:52"] или ["28","мая","2025","17:52"]
                try:
                    day = int(parts[0])
                except ValueError:
                    continue
                month_name = parts[1].lower()
                if month_name not in ru_months:
                    continue
                month = ru_months[month_name]
                year = today.year
                # если в parts[2] четвёрка цифр, считаем, что это год
                if len(parts) >= 3 and parts[2].isdigit() and len(parts[2]) == 4:
                    year = int(parts[2])
                try:
                    candidate = datetime.date(year, month, day)
                except ValueError:
                    continue
                # если эта дата уже в будущем, значит, год был прошлый
                if candidate > today:
                    candidate = datetime.date(year - 1, month, day)
                news_date = candidate
            else:
                # если нет названия месяца, значит raw_date = "HH:MM" → сегодняшняя дата
                news_date = today

            if news_date not in dates:
                continue

            collected.append({
                "title": title,
                "url": full_url
            })

    # убираем дубликаты по URL
    unique = []
    seen = set()
    for item in collected:
        if item["url"] not in seen:
            seen.add(item["url"])
            unique.append(item)

    #with open(output_file, "w", encoding="utf-8") as f:
    #    json.dump(unique, f, ensure_ascii=False, indent=2)
    save_to_drive(output_file, unique, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved RBC data to {output_file}")

# Agro investor scraper

def fetch_agro(dates, output_file, base_url="https://www.agroinvestor.ru/"):

    print(f"Fetching Agroinvestor: {base_url}")
    soup = get_page_soup(base_url)
    news_list = []
    seen_links = set()

    # Словарь для преобразования русских месяцев в числа:
    ru_months = {
        "января": 1, "февраля": 2, "марта": 3, "апреля": 4,
        "мая": 5, "июня": 6, "июля": 7, "августа": 8,
        "сентября": 9, "октября": 10, "ноября": 11, "декабря": 12
    }

    # Шаг 1. Находим все теги <time> на странице
    for time_tag in soup.find_all("time"):
        date_text = time_tag.get_text(strip=True).replace("\xa0", " ")
        if not date_text:
            continue

        # Шаг 2. Разбираем "30 мая 2025" → (день, месяц, год)
        parts = date_text.split()
        if len(parts) != 3:
            continue  # если формат нетипичный, пропускаем

        day_str, month_str, year_str = parts
        try:
            day = int(day_str)
            year = int(year_str)
        except ValueError:
            continue

        month_str = month_str.lower()
        if month_str not in ru_months:
            continue

        month = ru_months[month_str]
        try:
            date_obj = datetime.date(year, month, day)
        except Exception:
            continue

        # Шаг 3. Проверяем, входит ли date_obj в список нужных дат
        if date_obj not in dates:
            continue

        # Шаг 4. Ищем связанный <a> перед этим <time> (может быть заголовок и ссылка)
        # Используем find_previous("a"), но иногда нужно подняться чуть выше
        anchor = time_tag.find_previous("a")
        if not anchor:
            continue

        title = anchor.get_text(strip=True)
        href = anchor.get("href")
        if not href or not title:
            continue

        # Шаг 5. Формируем полный URL
        url = urljoin(base_url, href.strip())

        # Шаг 6. Проверяем дубли по URL и добавляем в список
        if url in seen_links:
            continue
        seen_links.add(url)

        news_list.append({
            "title": title,
            "link": url
        })

    #with open(output_file, "w", encoding="utf-8") as f:
    #    json.dump(news_list, f, ensure_ascii=False, indent=2)
    save_to_drive(output_file, news_list, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")

    print(f"Saved Agroinvestor data to {output_file}")


# RG.ru scraper

def fetch_rg(rubrics, dates, output_file,
             base_url_template="https://rg.ru/tema/ekonomika/{rubric}"):
    all_news = []
    for rubric in rubrics:
        url = base_url_template.format(rubric=rubric)
        print(f"Fetching RG, {rubric}: {url}")
        soup = get_page_soup(url)
        for title_span in soup.find_all("span", class_="ItemOfListStandard_title__Ajjlf"):
            parent_a = title_span.find_parent("a")
            if not parent_a:
                continue
            href = parent_a.get("href", "").strip()
            if not href:
                continue
            full_url = href if href.startswith("http") else f"https://rg.ru{href}"

            date_a = title_span.find_previous("a", class_="ItemOfListStandard_datetime__GstJi")
            if not date_a:
                continue
            date_href = date_a.get("href", "").strip()
            parts = date_href.strip("/").split("/")  # ['2025','05','30',...]
            if len(parts) < 3:
                continue
            try:
                y, m, d = map(int, parts[:3])
                news_date = date(y, m, d)
            except ValueError:
                continue

            if news_date not in dates:
                continue

            all_news.append({
                "title": title_span.get_text(strip=True),
                "url": full_url
            })

    unique = []
    seen = set()
    for item in all_news:
        if item["url"] not in seen:
            seen.add(item["url"])
            unique.append(item)

    #with open(output_file, "w", encoding="utf-8") as f:
    #    json.dump(unique, f, ensure_ascii=False, indent=2)
    save_to_drive(output_file, unique, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved RG data to {output_file}")

# RIA scraper

def fetch_ria(dates, output_file, base_url_template="https://ria.ru/economy/"):
    print("Fetching RIA: https://ria.ru/economy/")
    soup = get_page_soup(base_url_template)
    collected = []

    # Each news item has <a itemprop="url" href="..."></a>
    for a in soup.find_all("a", itemprop="url"):
        href = a.get("href", "").strip()
        if not href:
            continue
        full_url = href if href.startswith("http") else f"https://ria.ru{href}"

        # Next meta tag with itemprop="name" holds the title
        name_meta = a.find_next("meta", itemprop="name")
        if not name_meta:
            continue
        title = name_meta.get("content", "").strip()
        if not title:
            continue

        # Extract date from the URL path: "/YYYYMMDD/..."
        parsed = urlparse(full_url)
        parts = parsed.path.lstrip("/").split("/")
        if not parts or len(parts[0]) != 8 or not parts[0].isdigit():
            continue
        y, m, d = int(parts[0][:4]), int(parts[0][4:6]), int(parts[0][6:8])
        try:
            news_date = date(y, m, d)
        except ValueError:
            continue

        # Filter by provided dates
        if news_date in dates:
            collected.append({
                "title": title,
                "url": full_url
            })

    # Remove duplicates by URL
    unique = []
    seen = set()
    for item in collected:
        if item["url"] not in seen:
            seen.add(item["url"])
            unique.append(item)

    # Save to JSON
    #with open(output_file, "w", encoding="utf-8") as f:
    #    json.dump(unique, f, ensure_ascii=False, indent=2)
    save_to_drive(output_file, unique, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")

    print(f"Saved RIA data to {output_file}")


# Autostat scraper

def fetch_autostat(dates, output_file,
                   rubrics=[21, 8, 13, 70, 71],
                   base_url_template="https://m.autostat.ru/news/themes-{rubric}/"):

    if dates is None:
        raise ValueError("Argument 'dates' must be provided as a list of datetime.date objects.")

    all_collected = []
    seen_urls = set()

    ru_months = {
        'января': 1, 'февраля': 2, 'марта': 3, 'апреля': 4,
        'мая': 5, 'июня': 6, 'июля': 7, 'августа': 8,
        'сентября': 9, 'октября': 10, 'ноября': 11, 'декабря': 12
    }
    today = date.today()
    yesterday = today - timedelta(days=1)

    for rubric in rubrics:
        url = base_url_template.format(rubric=rubric)
        print(f"Fetching Autostat, {rubric}: {url}")
        soup = get_page_soup(url)
        if not soup:
            print(f"  (!) Failed to retrieve or parse page for rubric {rubric}")
            continue

        titles = soup.find_all("p", class_="Block-title")
        if not titles:
            print(f"    (!) No <p class='Block-title'> elements found on {url}")
            continue

        for title_p in titles:
            title = title_p.get_text(strip=True)
            if not title:
                continue

            link_a = title_p.find_parent("a", class_="Block-link")
            if not link_a:
                continue
            href = link_a.get("href", "").strip()
            if not href:
                continue
            full_url = urljoin("https://www.autostat.ru", href)

            date_p = title_p.find_next("p", class_="Block-date")
            if not date_p:
                continue
            date_text = date_p.get_text(strip=True)  # e.g. "Сегодня, 15:48" or "28 мая, 15:48"
            date_part = date_text.split(",")[0].strip().lower()

            if date_part == "сегодня":
                news_date = today
            elif date_part == "вчера":
                news_date = yesterday
            else:
                parts = date_part.split()
                if len(parts) != 2:
                    continue
                day_str, month_str = parts
                try:
                    day = int(day_str)
                    month = ru_months.get(month_str)
                    if not month:
                        continue
                    news_date = date(today.year, month, day)
                    if news_date > today:
                        news_date = date(today.year - 1, month, day)
                except Exception:
                    continue

            if news_date in dates and full_url not in seen_urls:
                all_collected.append({
                    "title": title,
                    "url": full_url
                })
                seen_urls.add(full_url)

    #with open(output_file, "w", encoding="utf-8") as f:
    #    json.dump(all_collected, f, ensure_ascii=False, indent=2)
    save_to_drive(output_file, all_collected, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")

    print(f"Saved Autostat data to {output_file}")

#with open('agro.json', encoding='utf-8') as f:
#    data = json.load(f)
#print(json.dumps(data, ensure_ascii=False, indent=2))

# Parameters
#days_before = 4
dates = get_last_dates(n_days=0, end_date=date(2025, 6, 2))
dates_kom = format_dates(dates, fmt="%Y-%m-%d")
dates_ved = format_dates(dates, fmt="%Y/%m/%d")

rubrics_kom_rus = [3, 4, 40]
rubrics_kom_world = [3, 5]
rubrics_kom_prices = [41]
rubrics_rbc = ["economics", "business", "finances"]
rubrics_rg = ["politekonom", "industria", "business", "finansy", "kazna", "rabota", "pensii", "vnesh", "apk", "tovary", "turizm"]
rubrics_auto = [21, 8, 13, 70, 71]

# Fetching
fetch_kom(rubrics_kom_rus, dates_kom, "kom_rus.json")
fetch_kom(rubrics_kom_world, dates_kom, "kom_world.json")
fetch_kom(rubrics_kom_prices, dates_kom, "kom_prices.json")
fetch_ved(dates_ved, "ved.json")
fetch_rbc(rubrics_rbc, dates, "rbc.json")
fetch_agro(dates, "agro.json")
fetch_rg(rubrics_rg, dates, "rg.json")
fetch_ria(dates, "ria.json")
fetch_autostat(dates, "autostat.json", rubrics_auto)

# Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat
section_to_files = {
    "world": [
        "kom_world.json",
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        "rg.json",
        "ria.json"
    ],
    "rus": [
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        "rg.json",
        "ria.json"
    ],
    "prices": [
        "kom_prices.json",
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        "rg.json",
        "ria.json",
        "autostat.json"
    ]
}

# drive.mount('/content/drive')

### Prompts

#file_path = '/content/drive/MyDrive/news lists, prompt beginning.txt'

file_id = find_file_in_drive("news lists, prompt beginning.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")

try:
    prompt_list_start = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    prompt_list_start = ""

#try:
#    with open(file_path, 'r', encoding='utf-8') as f:
#        propmt_list_start = f.read()
#except FileNotFoundError:
#    print(f"Error: no file found (path: {file_path})")
#except Exception as e:
#    print(f"Error while reading file: {e}")

file_id = find_file_in_drive("bullets, prompt beginning.txt","1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")

try:
    prompt_bullets_start = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    prompt_bullets_start = ""

#try:
#    with open(file_path, 'r', encoding='utf-8') as f:
#        prompt_bullets_start = f.read()
#except FileNotFoundError:
#    print(f"Error: no file found (path: {file_path})")
#except Exception as e:
#    print(f"Error while reading file: {e}")

section_to_continue_prompt = {
    "world": [
        'Пожалуйста, просмотри АБСОЛЮТНО ВСЕ НОВОСТИ в приложенном файле и отбери из них только те, что СТРОГО соответствуют критериям и могут быть включены в нумерованный список для раздела по мировой экономике.'
    ],
    "rus": [
        'Пожалуйста, просмотри АБСОЛЮТНО ВСЕ НОВОСТИ в приложенном файле и отбери из них только те, что СТРОГО соответствуют критериям и могут быть включены в нумерованный список для раздела по россиийской экономике.'
    ],
    "prices": [
        'Пожалуйста, просмотри АБСОЛЮТНО ВСЕ НОВОСТИ в приложенном файле и отбери из них только те, что СТРОГО соответствуют критериям и могут быть включены в нумерованный список для раздела по новостям, релевантным для динамики российских цен.'
    ]
}
prompt_list_finish = 'Пришли мне текстовый файл с нумерованным списком новостей, СТРОГО соответствующих требованиям. Оформи нумерованный список так: новость, ниже ее URL, прикладываю пример оформления. ОЧЕНЬ ВАЖНО: В ОТВЕТ НЕ ПРИСЫЛАЙ НИЧЕГО КРОМЕ ТЕКСТОВОГО ФАЙЛА.'

section_to_finish_bullets_prompt = {
    "world": [
        'Пожалуйста, подготовь 3 буллита для раздела по мировой экономике в соответствии с требованиями и пришли итоговый результат в таком формате: сначала буллиты, потом нумерованный список (ни в коем случае ничего в нем не изменяй!).'
    ],
    "rus": [
        'Пожалуйста, подготовь 3 буллита для раздела по россиийской экономике в соответствии с требованиями и пришли итоговый результат в таком формате: сначала буллиты, потом нумерованный список (ни в коем случае ничего в нем не изменяй!).'
    ],
    "prices": [
        'Пожалуйста, подготовь 3 буллита для раздела по новостям, релевантным для динамики российских цен, в соответствии с требованиями и пришли итоговый результат в таком формате: сначала буллиты, потом нумерованный список (ни в коем случае ничего в нем не изменяй!).'
    ]
}

prompt_prioritise = 'Пожалуйста, оставь в разделе не более 30 наиболее новостей, в наибольшей степени подходящих под критерии. Пришли итоговый результат текстом в виде списка. НИКАК НЕ ИЗМЕНЯЙ ССЫЛКИ ИЛИ НАЗВАНИЯ НОВОСТЕЙ.'

example = 'Пример верного оформления:\r\n1.\tРосстат зафиксировал стабилизацию выпуска базовых отраслей\r\nhttps://www.kommersant.ru/doc/7329366 \r\n2.\tСтроители просят смягчить правила распоряжения авансами\r\nhttps://www.rbc.ru/newspaper/2024/11/25/673f6abf9a7947de58a24847 \r\n3.\tВ Ульяновске открылся новый завод грузовиков Соллерс\r\nhttps://tass.ru/ekonomika/22497349 \r\n4.\t Добыча газа за 9 месяцев выросла на 8% г/г в основном за счет Газпрома\r\nhttps://www.interfax.ru/business/994801 \r\n'

def create_news_lists(section):
    if section not in section_to_files:
        raise ValueError(f"Section '{section}' unknown.")

    file_name = f"{section}.txt"

    # Если сегодня не суббота, пробуем прочитать существующий файл <section>.txt
    if datetime.today().weekday() != 5:  # 5 = Saturday
        folder_id_input = MY_FOLDER_ID

        try:
            file_id = find_file_in_drive(file_name)
            list_start = download_text_file(file_id)
        except Exception:
            print("Warning, no file found.")
            list_start = ""
    else:
        list_start = ""

    # Достаём список JSON-файлов и соответствующий prompt_list_continue
    json_files = section_to_files[section]
    prompt_list_continue = section_to_continue_prompt[section]

    combined_text_parts = []
    for json_filename in json_files:
        base_name, ext = os.path.splitext(json_filename)
        if ext.lower() != ".json":
            print(f"Пропускаем '{json_filename}', т.к. не .json-файл.")
            continue

        try:
            file_id = find_file_in_drive(json_filename, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
            raw_text = download_text_file(file_id)
        except FileNotFoundError:
            print(f"Файл '{json_filename}' не найден. Пропускаем.")
            continue
        except Exception as e:
            print(f"Ошибка при скачивании '{json_filename}': {e}. Пропускаем.")
            continue

        # Если файл пустой или содержит только пробельные символы, пропускаем
        if not isinstance(raw_text, str) or not raw_text.strip():
            print(f"JSON '{json_filename}' пустой. Пропускаем.")
            continue

        # Парсим JSON-строку в Python-объект. Если невалидный JSON или пустой объект/список, пропускаем.
        try:
            news_data = json.loads(raw_text)
        except json.JSONDecodeError as e:
            print(f"Ошибка JSON в '{json_filename}': {e}. Пропускаем.")
            continue

        if isinstance(news_data, (list, dict)) and len(news_data) == 0:
            print(f"JSON '{json_filename}' содержит пустую структуру. Пропускаем.")
            continue

        # Преобразуем Python-объект в форматированный JSON для модели
        news_json_string = json.dumps(news_data, ensure_ascii=False, indent=2)

        raw_parts = [
            news_json_string,
            prompt_list_start,
            prompt_list_continue,
            prompt_list_finish,
            example
        ]

        prompt_parts = []
        for part in raw_parts:
            if isinstance(part, list):
                # Если это список, склеиваем через переносы строк
                prompt_parts.append("\n".join(part))
            else:
                prompt_parts.append(str(part))

        # Теперь, когда JSON точно не пуст, отправляем запрос модели
        try:
            response = model_obj.generate_content(prompt_parts)
        except Exception as e:
            print(f"Error in model.generate_content for '{json_filename}': {e}.")
            continue

        combined_text_parts.append(response.text + "\n")

    if not combined_text_parts:
        print(f"For section '{section}', zero JSONs were successfully processed.")
        return

    # Объединяем прочитанное ранее (list_start) с новыми частями
    all_text = list_start + "".join(combined_text_parts)

    # Записываем итог в тот же файл <section>.txt на Google Drive
    save_to_drive(file_name, all_text)

# Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat
create_news_lists("world")
time.sleep(60)
create_news_lists("rus")
time.sleep(60)
create_news_lists("prices")

def prioritise(section):
    if section not in section_to_files:
        raise ValueError(f"Section '{section}' unknown.")

    file_name = f"{section}.txt"
    folder_id_input = MY_FOLDER_ID
    file_id = find_file_in_drive(file_name)
    news_list = download_text_file(file_id)

    raw_parts = [
        news_list,
        prompt_list_start,
        prompt_prioritise
    ]

    prompt_parts = []
    for part in raw_parts:
        if isinstance(part, list):
            # Если это список, склеиваем через переносы строк
            prompt_parts.append("\n".join(part))
        else:
            prompt_parts.append(str(part))

    try:
        response = model_obj.generate_content(prompt_parts)
    except Exception as e:
        print(f"Error in model.generate_content for '{json_filename}': {e}.")

    # Записываем итог в тот же файл <section>.txt на Google Drive
    save_to_drive(file_name, response.text)

#prioritise("world")
#time.sleep(60)
#prioritise("rus")
#time.sleep(60)
#prioritise("prices")

def create_bullets(section):

    MY_FOLDER_ID = "18Lk31SodxZB3qgZm4ElX3BCejQihreVC"

    if section not in section_to_files:
        raise ValueError(f"Section '{section}' unknown.")

    list_file = f"{section}_bullets.txt"
    file_id = find_file_in_drive(list_file)

    try:
        list_content = download_text_file(file_id)
    except Exception as e:
        print("Ошибка при скачивании файла:", e)

    # Берём соответствующий prompt для завершения
    prompt_bullets_finish = section_to_finish_bullets_prompt[section]

    # Формируем prompt_parts
    raw_parts = [
        prompt_bullets_start,
        prompt_bullets_finish,
        list_content
    ]

    prompt_parts = []
    for part in raw_parts:
            if isinstance(part, list):
                # Если это список, склеиваем через переносы строк
                prompt_parts.append("\n".join(part))
            else:
                prompt_parts.append(str(part))

    try:
        response = model_obj.generate_content(prompt_parts)
    except Exception as e:
        print(f"Error in model.generate_content: {e}")
        return

    file_name = f"report_{section}.txt"
    save_to_drive(file_name, response.text)

if datetime.today().weekday() == 3:
  create_bullets("world")
  create_bullets("rus")
  create_bullets("prices")
