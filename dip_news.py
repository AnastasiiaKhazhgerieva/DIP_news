# -*- coding: utf-8 -*-
"""dip_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17KKwbfVuL28SPhRasyS-FeUXLvmP2PNo
"""

# packages

import requests
import json
import time
import datetime
import os
import pandas as pd
#import google.generativeai as genai
import io
import base64
import re
try: # google colab не запускается, когда раним через workflow, он там есть по умолчанию, поэтому имени в PyPL такого нет
    from google.colab import userdata, drive
except ImportError:
    userdata = None
    drive = None
from datetime import date, timedelta, datetime
from typing import List
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaIoBaseDownload, MediaIoBaseUpload
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials

# Auxilliary
HEADERS = {"User-Agent": "Mozilla/5.0"}

encoded_token = os.environ.get("GOOGLE_TOKEN_B64")
if not encoded_token:
    raise RuntimeError("OAuth токен не найден. Убедитесь, что переменная окружения GOOGLE_TOKEN_B64 задана.")

token_bytes = base64.b64decode(encoded_token)
token_info = json.loads(token_bytes.decode("utf-8"))

creds = Credentials.from_authorized_user_info(token_info, scopes=["https://www.googleapis.com/auth/drive"])

if creds.expired and creds.refresh_token:
    creds.refresh(Request())
    
drive_service = build("drive", "v3", credentials=creds)

print("✅ Credentials info:")
print("  - token:", creds.token[:20] + "...")
print("  - refresh_token:", bool(creds.refresh_token))
print("  - client_id:", creds.client_id)
print("  - quota_project_id:", creds.quota_project_id)
print("  - valid:", creds.valid)
print("  - expired:", creds.expired)
print("  - scopes:", creds.scopes)
# Кто залогинен?
about = drive_service.about().get(fields="user").execute()
print("✅ Авторизация от имени:", about["user"]["displayName"], about["user"]["emailAddress"])


MY_FOLDER_ID = "1BwBFMln6HcGUfBFN4-UlNueOTKUehiRe" # папка reports на google drive

API_KEY = os.environ.get("PERPLEXITY_API_KEY")  # для workflow
#API_KEY = userdata.get('perplexity_api_key')   # для локального запуска

if not API_KEY:
    raise ValueError("Нет переменной окружения PERPLEXITY_API_KEY!")

# Задаем эндпоинт и исходные сообщения
url = "https://api.perplexity.ai/chat/completions"

headers = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

# gemini api key
#API_KEY = os.environ.get("GEMINI_API_KEY") # строка для запуска через workflow
#API_KEY = userdata.get('gemini_api_key') # строка для локального запуска
#genai.configure(api_key=API_KEY)
#model_obj = genai.GenerativeModel(
#    model_name="gemini-2.5-pro",
#    generation_config={
#        "response_mime_type": "application/json",  # ← важно!
#    }
#)

### TG Schedule bot

def telegram_lists():
    url = "https://api.telegram.org/bot6245425859:AAEHTh0EX9aE6qPhvHoclFPa3a8IBVRNeSM/sendMessage"
    payload = {
        "chat_id": "-4265314101",
        "text": "Новостная записка обновлена. См. отчёты по <a href=\"https://clck.ru/3MTaGo\">ссылке</a>",
        "parse_mode": "HTML"
    }

    try:
        response = requests.post(url, data=payload)
        response.raise_for_status()
        print("Telegram message sent.")
    except requests.exceptions.RequestException as e:
        print(f"Failed to send Telegram message: {e}")

def telegram_bullets():
    url = "https://api.telegram.org/bot6245425859:AAEHTh0EX9aE6qPhvHoclFPa3a8IBVRNeSM/sendMessage"
    payload = {
        "chat_id": "-4265314101",
        "text": "Готовы буллиты к новостной записке. См. отчёты по <a href=\"https://clck.ru/3MTbwx\">ссылке</a>",
        "parse_mode": "HTML"
    }

    try:
        response = requests.post(url, data=payload)
        response.raise_for_status()
        print("Telegram message sent.")
    except requests.exceptions.RequestException as e:
        print(f"Failed to send Telegram message: {e}")

### Functions for google drive

def find_file_in_drive(file_name: str, folder_id = "1BwBFMln6HcGUfBFN4-UlNueOTKUehiRe") -> str:
    try:
        resp = drive_service.files().list(
            q=(
                f"name = '{file_name}' "
                f"and '{folder_id}' in parents "
                f"and trashed = false"
            ),
            spaces="drive",
            fields="files(id, name)",
            pageSize=1
        ).execute()
    except HttpError as e:
        raise RuntimeError(f"Error accessing Drive API: {e}")

    items = resp.get("files", [])
    if items:
        return items[0]["id"]

    raise FileNotFoundError(f"File '{file_name}' not found in folder {folder_id}.")

def download_text_file(fid: str) -> str:
    request = drive_service.files().get_media(fileId=fid)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        status, done = downloader.next_chunk()
    return fh.getvalue().decode("utf-8")

def save_to_drive(file_name: str, data, my_folder=MY_FOLDER_ID, file_format: str = "json"):
    if file_format not in ("json", "txt"):
        raise ValueError("file_format должен быть 'json' или 'txt'")

    if file_format == "txt":
        content_bytes = data.encode("utf-8") if isinstance(data, str) else str(data).encode("utf-8")
        mime_type = "text/plain"
    else:
        json_str = json.dumps(data, ensure_ascii=False, indent=2)
        content_bytes = json_str.encode("utf-8")
        mime_type = "application/json"

    # Ищем, существует ли уже файл
    existing_file_id = None
    try:
        resp = drive_service.files().list(
            q=f"name = '{file_name}' and '{my_folder}' in parents and trashed = false",
            spaces="drive",
            fields="files(id, name)",
            pageSize=1
        ).execute()
        items = resp.get("files", [])
        if items:
            existing_file_id = items[0]["id"]
    except Exception as e:
        print("Warning: can't check if the file already exists:", e)

    fh = io.BytesIO(content_bytes)
    media = MediaIoBaseUpload(fh, mimetype=mime_type, resumable=False)

    if existing_file_id:
        try:
            # Пытаемся обновить
            updated = drive_service.files().update(
                fileId=existing_file_id,
                media_body=media
            ).execute()
            print(f"File '{file_name}' updated (ID={updated['id']}).")
            return updated
        except HttpError as e:
            if e.resp.status == 403 and "storageQuotaExceeded" in str(e):
                print(f"⚠️ Quota error on update — deleting and recreating file '{file_name}'...")
                try:
                    drive_service.files().delete(fileId=existing_file_id).execute()
                    existing_file_id = None  # перейти к созданию
                except Exception as del_err:
                    print(f"Ошибка при удалении файла '{file_name}': {del_err}")
                    raise
            else:
                print(f"Ошибка при обновлении файла '{file_name}': {e}")
                raise

    # Создание нового файла
    file_metadata = {
        "name": file_name,
        "parents": [my_folder],
        "mimeType": mime_type
    }
    try:
        created = drive_service.files().create(
            body=file_metadata,
            media_body=media,
            fields="id, webViewLink"
        ).execute()
        print(f"New file created: '{file_name}', (ID={created['id']}).")
        return created
    except Exception as e:
        print(f"Ошибка при создании нового файла '{file_name}': {e}")
        raise

### Functions for scrapping

## Defining and formatting dates
def get_last_dates(n_days=6, end_date=None):
    if end_date is None:
        end_date = date.today()
    return [end_date - timedelta(days=offset) for offset in range(n_days, -1, -1)]

def format_dates(dates_list, fmt="%Y-%m-%d"):
    return [d.strftime(fmt) for d in dates_list]

## Getting web page soup
def get_page_soup(url, headers=HEADERS, timeout=10):
    resp = requests.get(url, headers=headers, timeout=timeout)
    resp.raise_for_status()
    return BeautifulSoup(resp.text, "html.parser")

## Scrapers: Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat

# Kommersant scraper
def fetch_kom(rubrics, dates, output_file,
                   base_url_template="https://www.kommersant.ru/archive/rubric/{rubric}/day/{date}"):
    all_items = []
    seen_urls = set()

    for rubric in rubrics:
        for dt in dates:
            url = base_url_template.format(rubric=rubric, date=dt)
            print(f"Fetching Kommersant: {url}")
            try:
                soup = get_page_soup(url)
                scripts = soup.find_all("script", type="application/ld+json")

                for script in scripts:
                    raw = script.string
                    if not raw:
                        continue
                    try:
                        data = json.loads(raw)
                    except json.JSONDecodeError:
                        continue

                    for entry in data.get("itemListElement", []):
                        title = entry.get("name") or entry.get("headline")
                        link = entry.get("url")
                        if title and link and link not in seen_urls:
                            seen_urls.add(link)
                            all_items.append({"title": title, "url": link})
            except Exception as e:
                print(f"[ERROR] {e} when fetching {url}")

    save_to_drive(output_file, all_items, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved Kommersant data to {output_file}")


# Vedomosti scraper
def fetch_ved(dates, output_file,
              base_url_template="https://www.vedomosti.ru/newspaper/{date}"):
    all_news = []
    for dt in dates:
        url = base_url_template.format(date=dt)
        print(f"Fetching Vedomosti: {url}")
        try:
            soup = get_page_soup(url)
            for item in soup.select("li.waterfall__item"):
                a = item.select_one("a.waterfall__item-title")
                if not a:
                    continue
                title = a.get_text(strip=True)
                href = a.get("href", "")
                full_url = href if href.startswith("http") else f"https://www.vedomosti.ru{href}"
                all_news.append({"title": title, "url": full_url})
        except Exception as e:
            all_news.append({"error": str(e)})

    save_to_drive(output_file, all_news, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved Vedomosti data to {output_file}")

# RBC scraper

from datetime import date

def fetch_rbc(rubrics, dates, output_file,
              base_url_template="https://www.rbc.ru/{rubric}/?utm_source=topline"):

    ru_months = {
        'января': 1, 'февраля': 2, 'марта': 3, 'апреля': 4,
        'мая': 5, 'июня': 6, 'июля': 7, 'августа': 8,
        'сентября': 9, 'октября': 10, 'ноября': 11, 'декабря': 12
    }
    today = date.today()
    collected = []

    for rubric in rubrics:
        page_url = base_url_template.format(rubric=rubric)
        print(f"Fetching RBC, {rubric}: {page_url}")
        soup = get_page_soup(page_url)

        anchors = soup.find_all("a", class_="news-feed__item")

        for idx, a in enumerate(anchors, start=1):
            # внутри anchor ищем span, у которого class содержит "news-feed__item__title"
            title_span = a.find(
                "span",
                class_=lambda c: c and "news-feed__item__title" in c
            )
            if not title_span:
                continue

            # Для даты: ищем span, у которого class содержит "news-feed__item__time"
            # или, если нет, "news-feed__item__date"
            date_span = a.find(
                "span",
                class_=lambda c: c and "news-feed__item__time" in c
            )
            if not date_span:
                date_span = a.find(
                    "span",
                    class_=lambda c: c and "news-feed__item__date" in c
                )
            if not date_span:
                continue

            title = title_span.get_text(strip=True)
            href = a.get("href", "").strip()
            if not href:
                continue

            full_url = href if href.startswith("http") else urljoin(page_url, href)

            # raw_date может быть вида "28 мая 17:52" или просто "17:52"
            raw_date = date_span.get_text(strip=True).replace("\xa0", " ").replace(",", "").strip()
            parts = raw_date.split()

            news_date = None
            if any(month in parts for month in ru_months):
                # формат ["28","мая","17:52"] или ["28","мая","2025","17:52"]
                try:
                    day = int(parts[0])
                except ValueError:
                    continue
                month_name = parts[1].lower()
                if month_name not in ru_months:
                    continue
                month = ru_months[month_name]
                year = today.year
                # если в parts[2] четвёрка цифр, считаем, что это год
                if len(parts) >= 3 and parts[2].isdigit() and len(parts[2]) == 4:
                    year = int(parts[2])
                try:
                    candidate = datetime.date(year, month, day)
                except ValueError:
                    continue
                # если эта дата уже в будущем, значит, год был прошлый
                if candidate > today:
                    candidate = datetime.date(year - 1, month, day)
                news_date = candidate
            else:
                # если нет названия месяца, значит raw_date = "HH:MM" сегодняшняя дата
                news_date = today

            if news_date not in dates:
                continue

            collected.append({
                "title": title,
                "url": full_url
            })

    # убираем дубликаты по URL
    unique = []
    seen = set()
    for item in collected:
        if item["url"] not in seen:
            seen.add(item["url"])
            unique.append(item)

    save_to_drive(output_file, unique, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved RBC data to {output_file}")

# Agro investor scraper - периодически ломается, поэтому пусть будет в коде вариант с отладкой

def fetch_agro(dates, output_file, base_url="https://www.agroinvestor.ru/"):
    print(f"Fetching Agroinvestor: {base_url}")

    soup = get_page_soup(base_url)

    if soup is None:
        print("❌ Failed to retrieve or parse the page.")
        return

    print("✅ Page fetched successfully.")
    news_list = []
    seen_links = set()

    ru_months = {
        "января": 1, "февраля": 2, "марта": 3, "апреля": 4,
        "мая": 5, "июня": 6, "июля": 7, "августа": 8,
        "сентября": 9, "октября": 10, "ноября": 11, "декабря": 12
    }

    print(f"Looking for dates: {dates}")

    for time_tag in soup.find_all("time"):
        date_text = time_tag.get_text(strip=True).replace("\xa0", " ")
        if not date_text:
            continue

        print(f"🕒 Found date text: '{date_text}'")
        parts = date_text.split()
        if len(parts) != 3:
            print("⚠️ Unexpected date format. Skipping.")
            continue

        day_str, month_str, year_str = parts
        try:
            day = int(day_str)
            year = int(year_str)
        except ValueError as e:
            print(f"⚠️ Could not parse day/year: {e}")
            continue

        month_str = month_str.lower()
        if month_str not in ru_months:
            print(f"⚠️ Unknown month: '{month_str}'")
            continue

        month = ru_months[month_str]
        try:
            date_obj = date(year, month, day)

        except Exception as e:
            print(f"⚠️ Failed to construct date object: {e}")
            continue

        print(f"📅 Parsed date: {date_obj}")
        if date_obj not in dates:
            print("⏩ Date not in requested range. Skipping.")
            continue

        anchor = time_tag.find_previous("a")
        if not anchor:
            print("⚠️ No previous anchor tag found.")
            continue

        title = anchor.get_text(strip=True)
        href = anchor.get("href")
        if not href or not title:
            print("⚠️ Missing title or href. Skipping.")
            continue

        url = urljoin(base_url, href.strip())
        if url in seen_links:
            print(f"🔁 Duplicate link: {url}")
            continue
        seen_links.add(url)

        print(f"✅ Added news: {title} - {url}")
        news_list.append({
            "title": title,
            "link": url
        })

    save_to_drive(output_file, news_list, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"💾 Saved {len(news_list)} news items to {output_file}")


# RG.ru scraper

def fetch_rg(rubrics, dates, output_file,
             base_url_template="https://rg.ru/tema/ekonomika/{rubric}"):
    all_news = []
    for rubric in rubrics:
        url = base_url_template.format(rubric=rubric)
        print(f"Fetching RG, {rubric}: {url}")
        soup = get_page_soup(url)
        for title_span in soup.find_all("span", class_="ItemOfListStandard_title__Ajjlf"):
            parent_a = title_span.find_parent("a")
            if not parent_a:
                continue
            href = parent_a.get("href", "").strip()
            if not href:
                continue
            full_url = href if href.startswith("http") else f"https://rg.ru{href}"

            date_a = title_span.find_previous("a", class_="ItemOfListStandard_datetime__GstJi")
            if not date_a:
                continue
            date_href = date_a.get("href", "").strip()
            parts = date_href.strip("/").split("/")  # ['2025','05','30',...]
            if len(parts) < 3:
                continue
            try:
                y, m, d = map(int, parts[:3])
                news_date = date(y, m, d)
            except ValueError:
                continue

            if news_date not in dates:
                continue

            all_news.append({
                "title": title_span.get_text(strip=True),
                "url": full_url
            })

    unique = []
    seen = set()
    for item in all_news:
        if item["url"] not in seen:
            seen.add(item["url"])
            unique.append(item)

    save_to_drive(output_file, unique, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
    print(f"Saved RG data to {output_file}")

# RIA scraper

def fetch_ria(dates, output_file, base_url_template="https://ria.ru/economy/"):
    print("Fetching RIA: https://ria.ru/economy/")
    soup = get_page_soup(base_url_template)
    collected = []

    # Each news item has <a itemprop="url" href="..."></a>
    for a in soup.find_all("a", itemprop="url"):
        href = a.get("href", "").strip()
        if not href:
            continue
        full_url = href if href.startswith("http") else f"https://ria.ru{href}"

        # Next meta tag with itemprop="name" holds the title
        name_meta = a.find_next("meta", itemprop="name")
        if not name_meta:
            continue
        title = name_meta.get("content", "").strip()
        if not title:
            continue
        parsed = urlparse(full_url)
        parts = parsed.path.lstrip("/").split("/")
        if not parts or len(parts[0]) != 8 or not parts[0].isdigit():
            continue
        y, m, d = int(parts[0][:4]), int(parts[0][4:6]), int(parts[0][6:8])
        try:
            news_date = date(y, m, d)
        except ValueError:
            continue

        if news_date in dates:
            collected.append({
                "title": title,
                "url": full_url
            })

    unique = []
    seen = set()
    for item in collected:
        if item["url"] not in seen:
            seen.add(item["url"])
            unique.append(item)

    save_to_drive(output_file, unique, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")

    print(f"Saved RIA data to {output_file}")


# Autostat scraper

def fetch_autostat(dates, output_file,
                   rubrics=[21, 8, 13, 70, 71],
                   base_url_template="https://m.autostat.ru/news/themes-{rubric}/"):

    if dates is None:
        raise ValueError("Argument 'dates' must be provided as a list of datetime.date objects.")

    all_collected = []
    seen_urls = set()

    ru_months = {
        'января': 1, 'февраля': 2, 'марта': 3, 'апреля': 4,
        'мая': 5, 'июня': 6, 'июля': 7, 'августа': 8,
        'сентября': 9, 'октября': 10, 'ноября': 11, 'декабря': 12
    }
    today = date.today()
    yesterday = today - timedelta(days=1)

    for rubric in rubrics:
        url = base_url_template.format(rubric=rubric)
        print(f"Fetching Autostat, {rubric}: {url}")
        soup = get_page_soup(url)
        if not soup:
            print(f"  (!) Failed to retrieve or parse page for rubric {rubric}")
            continue

        titles = soup.find_all("p", class_="Block-title")
        if not titles:
            print(f"    (!) No <p class='Block-title'> elements found on {url}")
            continue

        for title_p in titles:
            title = title_p.get_text(strip=True)
            if not title:
                continue

            link_a = title_p.find_parent("a", class_="Block-link")
            if not link_a:
                continue
            href = link_a.get("href", "").strip()
            if not href:
                continue
            full_url = urljoin("https://www.autostat.ru", href)

            date_p = title_p.find_next("p", class_="Block-date")
            if not date_p:
                continue
            date_text = date_p.get_text(strip=True)  # e.g. "Сегодня, 15:48" or "28 мая, 15:48"
            date_part = date_text.split(",")[0].strip().lower()

            if date_part == "сегодня":
                news_date = today
            elif date_part == "вчера":
                news_date = yesterday
            else:
                parts = date_part.split()
                if len(parts) != 2:
                    continue
                day_str, month_str = parts
                try:
                    day = int(day_str)
                    month = ru_months.get(month_str)
                    if not month:
                        continue
                    news_date = date(today.year, month, day)
                    if news_date > today:
                        news_date = date(today.year - 1, month, day)
                except Exception:
                    continue

            if news_date in dates and full_url not in seen_urls:
                all_collected.append({
                    "title": title,
                    "url": full_url
                })
                seen_urls.add(full_url)

    save_to_drive(output_file, all_collected, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")

    print(f"Saved Autostat data to {output_file}")

#with open('agro.json', encoding='utf-8') as f:
#    data = json.load(f)
#print(json.dumps(data, ensure_ascii=False, indent=2))

# Parameters
days_before = 1
dates = get_last_dates(days_before)
dates_kom = format_dates(dates, fmt="%Y-%m-%d")
dates_ved = format_dates(dates, fmt="%Y/%m/%d")

rubrics_kom_rus = [3, 4, 40]
rubrics_kom_world = [3, 5]
rubrics_kom_prices = [41]
rubrics_rbc = ["economics", "business", "finances"]
rubrics_rg = ["politekonom", "industria", "business", "finansy", "kazna", "rabota", "pensii", "vnesh", "apk", "tovary", "turizm"]
rubrics_auto = [21, 8, 13, 70, 71]

# Fetching
#fetch_kom(rubrics_kom_rus, dates_kom, "kom_rus.json")
#fetch_kom(rubrics_kom_world, dates_kom, "kom_world.json")
#fetch_kom(rubrics_kom_prices, dates_kom, "kom_prices.json")
#fetch_ved(dates_ved, "ved.json")

#fetch_rbc(rubrics_rbc, dates, "rbc.json")
#try:
#    fetch_agro(dates, "agro.json")
#except Exception as e:

#    pass
#fetch_rg(rubrics_rg, dates, "rg.json")
#fetch_ria(dates, "ria.json")
#fetch_autostat(dates, "autostat.json", rubrics_auto)

# Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat
section_to_files = {
    "world": [
        "kom_world.json",
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        #"rg.json",
        "ria.json"
    ],
    "rus": [
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        "rg.json",
        "ria.json"
    ],
    "prices": [
        "kom_prices.json",
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        #"rg.json",
        "ria.json",
        "autostat.json"
    ]
}

# drive.mount('/content/drive')

### Prompts

###
### news lists
file_id = find_file_in_drive("lists_world.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    lists_world = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    lists_world = ""

file_id = find_file_in_drive("lists_rus.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    lists_rus = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    lists_rus = ""

file_id = find_file_in_drive("lists_prices.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    lists_prices = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    lists_prices = ""

lists_prompts = {
        "world": lists_world,
        "rus": lists_rus,
        "prices": lists_prices
}

### prioritise
file_id = find_file_in_drive("prioritise_world.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    prioritise_world = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    prioritise_world = ""

file_id = find_file_in_drive("prioritise_rus.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    prioritise_rus = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    prioritise_rus = ""

file_id = find_file_in_drive("prioritise_prices.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    prioritise_prices = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    prioritise_prices = ""

prioritise_prompts = {
        "world": prioritise_world,
        "rus": prioritise_rus,
        "prices": prioritise_prices
}

### design

file_id = find_file_in_drive("design.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    prompt_design = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    prompt_design = ""

### top
file_id = find_file_in_drive("top_world.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    top_world = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    top_world = ""

file_id = find_file_in_drive("top_rus.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    top_rus = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    top_rus = ""

file_id = find_file_in_drive("top_prices.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    top_prices = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    top_prices = ""

top_prompts = {
        "world": top_world,
        "rus": top_rus,
        "prices": top_prices
}

### bullets
file_id = find_file_in_drive("bullets_world.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    bullets_world = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    bullets_world = ""

file_id = find_file_in_drive("bullets_rus.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    bullets_rus = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    bullets_rus = ""

file_id = find_file_in_drive("bullets_prices.txt", "1N7-qRmFebMzij2yR3nm7Edp6Hoayva-V")
try:
    bullets_prices = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    bullets_prices = ""

bullets_prompts = {
        "world": bullets_world,
        "rus": bullets_rus,
        "prices": bullets_prices
}

example = 'Пример верного оформления:\r\n1.\tРосстат зафиксировал стабилизацию выпуска базовых отраслей\r\nhttps://www.kommersant.ru/doc/7329366\r\n day: 2025-09-06 \r\n2.\tСтроители просят смягчить правила распоряжения авансами\r\nhttps://www.rbc.ru/newspaper/2024/11/25/673f6abf9a7947de58a24847\r\n day: 2025-08-12 \r\n3.\tВ Ульяновске открылся новый завод грузовиков Соллерс\r\nhttps://tass.ru/ekonomika/22497349 \r\n day: 2025-03-10 \r\n 4.\t Добыча газа за 9 месяцев выросла на 8% г/г в основном за счет Газпрома\r\nhttps://www.interfax.ru/business/994801 \r\n day: 2025-07-06 \r\n'

def extract_json(text: str):
    """
    Извлекает валидный JSON (объект или массив) из строки.
    Приоритет:
    1. Кодовые блоки: ```json [...]``` или ```[...]```
    2. Самый длинный валидный фрагмент, начинающийся с [ или { и заканчивающийся на ] или }
    3. Перебор всех возможных подстрок (на случай битого форматирования)

    Возвращает: dict | list | None
    """
    if not isinstance(text, str):
        return None

    text = text.strip()

    # Шаг 1: Ищем кодовые блоки (наиболее надёжный способ)
    code_block_match = re.search(r"```(?:json|)\s*([\s\S]+?)\s*```", text, re.IGNORECASE)
    if code_block_match:
        candidate = code_block_match.group(1).strip()
        try:
            return json.loads(candidate)
        except json.JSONDecodeError as e:
            print(f"❌ JSON в кодовом блоке невалиден: {e}")
            # Можно залогировать text[:500] для отладки

    # Шаг 2: Ищем самый длинный возможный JSON-массив или объект
    candidates = []

    # Находим все пары [..] и {..}
    brackets = []

    for i, char in enumerate(text):
        if char in '[{':
            brackets.append((i, char))
        elif char in ']}':
            brackets.append((i, char))

    # Собираем возможные валидные диапазоны
    stack = []
    ranges = []
    for pos, char in brackets:
        if char in '[{':
            stack.append((pos, char))
        elif char == ']' and stack and stack[-1][1] == '[':
            start, _ = stack.pop()
            ranges.append((start, pos))
        elif char == '}' and stack and stack[-1][1] == '{':
            start, _ = stack.pop()
            ranges.append((start, pos))

    # Сортируем по длине (сначала самые длинные)
    ranges.sort(key=lambda x: x[1] - x[0], reverse=True)

    for start, end in ranges:
        candidate = text[start:end+1]
        try:
            result = json.loads(candidate)
            # Дополнительная проверка: хотя бы один ключ или элемент
            if isinstance(result, (dict, list)) and len(result) >= 0:
                return result  # Возвращаем первый валидный (самый длинный)
        except json.JSONDecodeError:
            continue

    # Шаг 3: Фолбэк — попробовать найти хотя бы что-то похожее
    # Удаляем экранирование, если строка была "заэкранирована"
    if text.startswith('"') and text.endswith('"'):
        try:
            unescaped = text[1:-1].encode().decode('unicode_escape')
            if (unescaped.startswith('{') and unescaped.endswith('}')) or \
               (unescaped.startswith('[') and unescaped.endswith(']')):
                return json.loads(unescaped)
        except Exception:
            pass

    # Шаг 4: Полный фолбэк — перебор всех подстрок (очень медленно, только если всё сломалось)
    # Это крайний случай
    for start in range(len(text)):
        if text[start] not in '[{':
            continue
        for end in range(len(text), start, -1):
            if text[end-1] not in ']}':
                continue
            fragment = text[start:end]
            if len(fragment) < 3:
                continue
            try:
                return json.loads(fragment)
            except json.JSONDecodeError:
                continue

    return None
    
def create_news_lists(section):
    current_weekday_num = datetime.today().weekday()

    # Если сегодня не суббота — пробуем прочитать уже сохранённый <section>.json
    if current_weekday_num != 5:  # 5 = Saturday
        try:
            existing_id = find_file_in_drive(f"{section}.json", "1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe")
            existing_text = download_text_file(existing_id)
            try:
                combined_items = json.loads(existing_text)
            except json.JSONDecodeError:
                combined_items = []
        except Exception:
            combined_items = []
    else:
        combined_items = []

    # Обновляем seen_urls и подготавливаем set для сохранённых URL
    seen_urls = {item["url"] for item in combined_items if isinstance(item, dict) and "url" in item}

    # Список файлов и промпт для секции
    json_files = section_to_files[section]
    prompt_list = lists_prompts.get(section, "")

    for json_filename in json_files:
        base_name, ext = os.path.splitext(json_filename)
        if ext.lower() != ".json":
            print(f"Пропускаем '{json_filename}', т.к. не .json-файл.")
            continue

        # Загружаем JSON-файл из Google Drive
        try:
            file_id = find_file_in_drive(json_filename, "1INECa_Slues7f8Xm0eJw-c05kLbRXh0Y")
            raw_text = download_text_file(file_id)
        except FileNotFoundError:
            print(f"Файл '{json_filename}' не найден. Пропускаем.")
            continue
        except Exception as e:
            print(f"Ошибка при скачивании '{json_filename}': {e}. Пропускаем.")
            continue

        if not isinstance(raw_text, str) or not raw_text.strip():
            print(f"JSON '{json_filename}' пустой. Пропускаем.")
            continue

        try:
            news_data = json.loads(raw_text)
        except json.JSONDecodeError as e:
            print(f"Ошибка JSON в '{json_filename}': {e}. Пропускаем.")
            continue

        if isinstance(news_data, (list, dict)) and len(news_data) == 0:
            print(f"JSON '{json_filename}' содержит пустую структуру. Пропускаем.")
            continue

        # Формируем prompt для модели
        news_json_string = json.dumps(news_data, ensure_ascii=False, indent=2)
        prompt_parts = [
            str(prompt_list),
            str(news_json_string)
        ]

        # Запрос к Perplexity API
        try:
            payload = {
                "model": "sonar-pro",
                "messages": [
                    {"role": "system", "content": "Отвечай строго в формате JSON. Никогда не добавляй в списки новостей источники, найденные в интернете - отбирай новости только из приложенного списка."},
                    {"role": "user", "content": "\n".join(prompt_parts)}
                ],
                "temperature": 0.2,
                "response_mime_type": "application/json",
                "disable_search": True
            }
            response = requests.post(url, headers=headers, json=payload)
            response.raise_for_status()
            result = response.json()

            # Проверка, что модель вернула контент
            choices = result.get("choices")
            if not choices or not choices[0].get("message", {}).get("content"):
                print(f"Модель не вернула ответ для '{json_filename}'. Пропускаем.")
                continue

            assistant_json_str = choices[0]["message"]["content"]

            # Парсим JSON из строки (ожидается список словарей с ключами 'url' и 'title')
            try:
                items = json.loads(assistant_json_str)
            except json.JSONDecodeError as e:
                print(f"Ответ модели для '{json_filename}' не содержит валидный JSON: {e}")
                continue

            # Приводим к списку, если словарь
            if isinstance(items, dict):
                items = [items]

            if not isinstance(items, list):
                print(f"Ответ модели для '{json_filename}' вернул не список, а {type(items)}. Пропускаем.")
                continue

            # Фильтруем и добавляем новые новости с дополнительным полем day
            for entry in items:
                url_val = entry.get("url")
                title_val = entry.get("title")
                if not title_val or not url_val or url_val in seen_urls:
                    continue
                seen_urls.add(url_val)
                combined_items.append({
                    "title": title_val,
                    "url": url_val,
                    "day": current_weekday_num  # добавляем номер дня недели
                })

        except Exception as e:
            print(f"Ошибка при вызове модели для '{json_filename}': {e}. Пропускаем.")
            continue

    if not combined_items:
        print(f"For section '{section}', zero JSONs were successfully processed.")
        return

    # Сохраняем объединённый результат с полем day в каждом элементе
    output_file = f"{section}.json"
    save_to_drive(output_file, combined_items, my_folder="1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe")
    print(f"✅ create_news_lists({section}) — успешно обработан и сохранён файл.")

    if not combined_items:
        print(f"For section '{section}', zero JSONs were successfully processed.")
        return

    # Сохраняем объединённый результат
    output_file = f"{section}.json"
    save_to_drive(output_file, combined_items, my_folder="1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe")
    print(f"✅ create_news_lists({section}) — успешно обработан и сохранён файл.")

# Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat

#create_news_lists("world")
#time.sleep(60)
#create_news_lists("rus")
#time.sleep(60)
#create_news_lists("prices")

def prioritise(section):
    file_name = f"{section}.json"
    folder_id = "1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe"
    temp_folder_id = "12I1CB-RDDTkHUTk1wxD7qOT9bZWA8ssF"
    combined_items = []
    # Загружаем файл с новостями
    try:
        file_id = find_file_in_drive(file_name, folder_id)
        news_list_raw = download_text_file(file_id)
    except FileNotFoundError:
        print(f"❌ Файл {file_name} не найден в папке {folder_id}.")
        return
    except Exception as e:
        print(f"❌ Ошибка при загрузке файла {file_name}: {e}")
        return
    if not news_list_raw.strip():
        print(f"❌ Файл {file_name} пустой.")
        return
    # Готовим prompt
    prompt_prioritise = prioritise_prompts.get(section, "")
    prompt_text = "\n".join([str(prompt_prioritise), news_list_raw])
    try:
        payload = {
            "model": "sonar-pro",
            "messages": [
                {"role": "system", "content": "Отвечай строго в формате JSON. Никогда не добавляй в списки новостей источники, найденные в интернете - отбирай новости только из приложенного списка."},
                {"role": "user", "content": prompt_text}
            ],
            "temperature": 0.2,
            "response_mime_type": "application/json",
            "disable_search": True
        }
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        result = response.json()
        # Проверка ответа
        choices = result.get("choices")
        if not choices or not choices[0].get("message", {}).get("content"):
            print(f"❌ Модель не вернула ответ для '{file_name}'.")
            return
        
        assistant_json_str = choices[0]["message"]["content"]
        # Отладка - вывод ответа модели перед парсингом JSON
        print("DEBUG: Ответ модели (первые 3000 символов):")
        print(assistant_json_str[:3000])
        
        try:
            items = json.loads(assistant_json_str)
        except json.JSONDecodeError as e:
            print(f"❌ Ответ модели для '{file_name}' не содержит валидный JSON: {e}")
            return
        if isinstance(items, dict):
            items = [items]
        if not isinstance(items, list):
            print(f"❌ Ответ модели для '{file_name}' вернул не список, а {type(items)}.")
            return
        
        # Сохраняем полный ответ в отдельную папку
        save_to_drive(file_name, items, temp_folder_id, file_format="json")
        # Обработка с grade
        if all(isinstance(entry, dict) and "grade" in entry for entry in items):
            items_sorted = sorted(items, key=lambda x: x["grade"], reverse=True)
            items_top40 = items_sorted[:40]
            combined_items = [
                {"title": e.get("title"), "url": e.get("url"), "day": e.get("day")}
                for e in items_top40 if e.get("url")
            ]
        else:
            # Нет grade — берем первые 40 записей с валидным url
            combined_items = [
                {"title": e.get("title"), "url": e.get("url"), "day": e.get("day")}
                for e in items if e.get("url")
            ][:40]
    except Exception as e:
        print(f"❌ Ошибка при вызове модели для '{file_name}': {e}")
        return
    # Сохраняем итоговый результат в исходную папку
    save_to_drive(file_name, combined_items, folder_id, file_format="json")
    print(f"✅ prioritise({section}) — сохранён корректный JSON.")

#prioritise("world")
#time.sleep(60)
# prioritise("rus")
#time.sleep(60)
#prioritise("prices")

#time.sleep(60)

def design_wo_llm(section):
    file_name_json = f"{section}.json"
    try:
        file_id = find_file_in_drive(file_name_json, "1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe")
        news_list_raw = download_text_file(file_id)
    except FileNotFoundError:
        print(f"Файл {file_name_json} не найден в папке.")
        return
    except Exception as e:
        print(f"Ошибка при загрузке файла {file_name_json}: {e}")
        return
    # Парсим входной JSON
    try:
        news_items = json.loads(news_list_raw)
    except json.JSONDecodeError as e:
        print(f"Ошибка парсинга JSON: {e}")
        return
    # Формируем нумерованный список, как в примере, с сохранением day
    formatted_lines = []
    for i, item in enumerate(news_items, 1):
        title = item.get("title", "").strip()
        url = item.get("url", "").strip()
        day = item.get("day")  # сохраняем поле day, если есть
        if not title or not url:
            continue
        if day is not None:
            line = f"{i}.\t{title} (day: {day})\n{url}"
        else:
            line = f"{i}.\t{title}\n{url}"
        formatted_lines.append(line)

    # Склеиваем результаты с переводом строки между ними
    result_text = "\r\n".join(formatted_lines) + "\r\n" if formatted_lines else ""
    file_name_txt = f"{section}.txt"
    save_to_drive(file_name_txt, result_text, "1BwBFMln6HcGUfBFN4-UlNueOTKUehiRe", file_format="txt")
    print(f"✅ design({section}) — успешно сохранён файл с текстом.")


def design(section):
    file_name_json = f"{section}.json"
    try:
        file_id = find_file_in_drive(file_name_json, "1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe")
        news_list_raw = download_text_file(file_id)
    except FileNotFoundError:
        print(f"Файл {file_name_json} не найден в папке.")
        return
    except Exception as e:
        print(f"Ошибка при загрузке файла {file_name_json}: {e}")
        return
    raw_parts = [prompt_design, example, news_list_raw]
    prompt_parts = []
    for part in raw_parts:
        if isinstance(part, list):
            prompt_parts.append("\n".join(part))
        else:
            prompt_parts.append(str(part))
    prompt_text = "\n".join(prompt_parts)
    try:
        payload = {
            "model": "sonar-pro",
            "messages": [
                {"role": "system", "content": "Отвечай лаконично и информативно. Никогда не добавляй в списки новостей источники, найденные в интернете - отбирай новости только из приложенного списка."},
                {"role": "user", "content": prompt_text}
            ],
            "temperature": 0.7,
            "disable_search": True
        }
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        result = response.json()
        choices = result.get("choices")
        if not choices or not choices[0].get("message", {}).get("content"):
            print(f"Модель не вернула ответ для '{file_name_json}'.")
            return
        assistant_text = choices[0]["message"]["content"]
        file_name_txt = f"{section}.txt"
        save_to_drive(file_name_txt, assistant_text, "1BwBFMln6HcGUfBFN4-UlNueOTKUehiRe", file_format="txt")
        print(f"✅ design({section}) — успешно сохранён файл с текстом.")
    except Exception as e:
        print(f"Ошибка при вызове модели для '{file_name_json}': {e}")
        return


#for section in ["world", "rus", "prices"]:
#    try:
#        design_wo_llm(section)
#    except Exception as e:
#        print(f"⚠️ Ошибка в design_wo_llm для '{section}': {e}. Пробую через LLM.")
#        design(section)
#        time.sleep(60)
#telegram_lists()

def choose_top_urls(section, max_chars=1500):
    import json
    import requests

    file_name = f"{section}.json"
    folder_id = "1Wo6zk7T8EllL7ceA5AwaPeBCaEUeiSYe"

    try:
        file_id = find_file_in_drive(file_name, folder_id)
        news_list_raw = download_text_file(file_id)
    except FileNotFoundError:
        print(f"❌ Файл {file_name} не найден в папке {folder_id}.")
        return
    except Exception as e:
        print(f"❌ Ошибка при загрузке файла {file_name}: {e}")
        return

    if not news_list_raw.strip():
        print(f"❌ Файл {file_name} пустой.")
        return

    prompt_top = top_prompts.get(section, "")
    prompt_text = "\n".join([str(prompt_top), news_list_raw])

    try:
        payload = {
            "model": "sonar-pro",
            "messages": [
                {
                    "role": "system",
                    "content": "Отвечай строго в формате JSON. Никогда не добавляй в списки новостей источники, найденные в интернете — выбирай только из приложенного списка."
                },
                {
                    "role": "user",
                    "content": prompt_text
                }
            ],
            "temperature": 0.2,
            "response_mime_type": "application/json",
            "disable_search": True
        }
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        result = response.json()

        print("=== Полный ответ API ===")
        print(json.dumps(result, ensure_ascii=False, indent=2))
        print("========================")

        choices = result.get("choices")
        if not choices:
            print("❌ В ответе API нет поля 'choices'.")
            return

        first_choice = choices[0]
        message = first_choice.get("message", {})
        content = message.get("content")
        finish_reason = first_choice.get("finish_reason")

        print(f"Finish reason: {finish_reason}")
        if content is None or content.strip() == "":
            print("❌ Модель вернула пустой ответ (content пустой).")
            return

        assistant_json_str = content

        try:
            items = json.loads(assistant_json_str)
        except json.JSONDecodeError as e:
            print(f"❌ Ответ модели для '{file_name}' не содержит валидный JSON: {e}")
            print("=== Начало ответа модели ===")
            print(assistant_json_str)
            print("=== Конец ответа модели ===")
            return

        if isinstance(items, dict):
            items = [items]
        if not isinstance(items, list):
            print(f"❌ Ответ модели для '{file_name}' вернул не список, а {type(items)}.")
            return

        combined_items = []
        current_len = 0
        for entry in items:
            url_val = entry.get("url")
            title_val = entry.get("title")
            theme_val = entry.get("theme") or entry.get("тема") or "undefined"
            json_entry = {"title": title_val, "url": url_val, "theme": theme_val}
            entry_len = len(json.dumps(json_entry, ensure_ascii=False))
            if current_len + entry_len > max_chars:
                break
            if url_val and title_val:
                combined_items.append(json_entry)
                current_len += entry_len

    except Exception as e:
        print(f"❌ Ошибка при вызове модели для '{file_name}': {e}")
        return

    output_folder_id = "17kQBohwKOQbBIwFl2yEQYWGUjuu-hf6V"
    save_to_drive(file_name, combined_items, output_folder_id, file_format="json")
    print(f"✅ choose_top_urls({section}) — сохранён корректный JSON.")

#if datetime.today().weekday() == 3:
choose_top_urls("world")
time.sleep(60)
#    choose_top_urls("rus")
#    time.sleep(60)
#    choose_top_urls("prices")

def read_top_urls(section, max_chars=3000):
    def extract_main_text(soup, max_chars=3000, min_paragraph_len=50, max_paragraphs=5):
        paragraphs = []
        for p in soup.find_all('p'):
            text = p.get_text(" ", strip=True)
            if len(text) < min_paragraph_len:
                continue
            low = text.lower()
            # Фильтр по рекламе/подпискам
            if any(word in low for word in ["cookie", "subscribe", "advert", "реклама", "подпишитесь"]):
                continue
            paragraphs.append(text)
            if len(paragraphs) >= max_paragraphs:
                break
        combined_text = " ".join(paragraphs)
        if len(combined_text) > max_chars:
            combined_text = combined_text[:max_chars].rsplit(" ", 1)[0] + "..."
        return combined_text

    import json
    import requests
    from bs4 import BeautifulSoup

    # Имя файла с топ ссылками для секции, например "world.json"
    file_name = f"{section}.json"

    # Находим ID файла в папке с топами (17kQBohwKOQbBIwFl2yEQYWGUjuu-hf6V)
    file_id = find_file_in_drive(file_name, folder_id="17kQBohwKOQbBIwFl2yEQYWGUjuu-hf6V")

    # Скачиваем содержимое файла — список словарей с title, url и темой
    json_text = download_text_file(file_id)
    try:
        items = json.loads(json_text)
    except Exception as e:
        print(f"Ошибка чтения JSON файла {file_name}: {e}")
        return

    results = []
    for item in items:
        url = item.get("url") or item.get("URL")
        title = item.get("title", "")
        theme = item.get("theme") or item.get("тема") or "undefined"
        if not url:
            continue
        try:
            resp = requests.get(url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            soup = BeautifulSoup(resp.text, "html.parser")
            page_text = extract_main_text(soup, max_chars=max_chars)
            results.append({
                "title": title,
                "url": url,
                "theme": theme,
                "text": page_text
            })
        except Exception as e:
            print(f"Ошибка при обработке {url}: {e}")

    # Сохраняем результат в другую папку с текстами (13KDzhQ0Y6GzKzEaMZggHoF38bglN358r)
    save_to_drive(
        file_name,
        results,
        my_folder="13KDzhQ0Y6GzKzEaMZggHoF38bglN358r",
        file_format="json"
    )
    print(f"{section}: сохранено {len(results)} ссылок с текстами.")

#if datetime.today().weekday() == 3:
#read_top_urls("world")
#    read_top_urls("rus")
#    read_top_urls("prices")

def create_bullets(section):
    list_file = f"{section}.json"
    try:
        file_id = find_file_in_drive(list_file, "13KDzhQ0Y6GzKzEaMZggHoF38bglN358r")
        list_content = download_text_file(file_id)
    except Exception as e:
        print(f"Ошибка загрузки файла {list_file}: {e}")
        return

    # Если пришёл JSON, делаем красиво (отступы для читаемости)
    try:
        parsed_json = json.loads(list_content)
        pretty_json = json.dumps(parsed_json, ensure_ascii=False, indent=2)
    except json.JSONDecodeError:
        pretty_json = str(list_content)

    prompt_bullets = bullets_prompts.get(section, "")
    prompt_text = "\n".join([str(prompt_bullets), pretty_json])

    try:
        payload = {
            "model": "sonar-pro",
            "messages": [
                {"role": "system", "content": "Отвечай лаконично и информативно."},
                {"role": "user", "content": prompt_text}
            ],
            "temperature": 0.7,
        }

        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        result = response.json()

        choices = result.get("choices")
        if not choices or not choices[0].get("message", {}).get("content"):
            print(f"Модель не вернула ответ для {section}.")
            return

        assistant_text = choices[0]["message"]["content"]

        file_name = f"report_{section}.txt"
        save_to_drive(file_name, assistant_text, my_folder="18Lk31SodxZB3qgZm4ElX3BCejQihreVC", file_format="txt")
        print(f"{section}: буллиты успешно записаны.")

    except Exception as e:
        print(f"Ошибка при вызове модели для {section}: {e}")
        return

#if datetime.today().weekday() == 3:
#create_bullets("world")
#    time.sleep(60)
 #   create_bullets("rus")
  #  time.sleep(60)
   # create_bullets("prices")
    #telegram_bullets()
