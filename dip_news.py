# -*- coding: utf-8 -*-
"""dip_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17KKwbfVuL28SPhRasyS-FeUXLvmP2PNo
"""

# packages

import requests
import json
import time
import datetime
import os
import pandas as pd
import google.generativeai as genai
import io
try: # google colab не запускается, когда раним через workflow, он там есть по умолчанию, поэтому имени в PyPL такого нет
    from google.colab import userdata, drive
except ImportError:
    userdata = None
    drive = None
from datetime import date, timedelta, datetime
from typing import List
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaIoBaseDownload, MediaIoBaseUpload

# Auxilliary
HEADERS = {"User-Agent": "Mozilla/5.0"}

# service account credentials (to access google drive)
sa_json = os.environ.get("GCP_SA_KEY") # строка для запуска через workflow
# with open("hybrid-sunbeam-461621-s8-9191b51ccff5.json", 'r', encoding='utf-8') as f: # для локального запуска
#   sa_json = f.read()  # для локального запуска
if not sa_json:
    raise RuntimeError("Сервисный аккаунт не найден")

creds_info = json.loads(sa_json)

creds = Credentials.from_service_account_info(
    creds_info,
    scopes=["https://www.googleapis.com/auth/drive"]
)
drive_service = build("drive", "v3", credentials=creds)

MY_FOLDER_ID = "1BwBFMln6HcGUfBFN4-UlNueOTKUehiRe" # папка reports на google drive

# gemini api key
API_KEY = os.environ.get("GEMINI_API_KEY") # строка для запуска через workflow
# API_KEY = userdata.get('gemini_api_key') # строка для локального запуска
genai.configure(api_key=API_KEY)
model_obj = genai.GenerativeModel('gemini-1.5-flash')

### Functions for scrapping

## Defining and formatting dates
def get_last_dates(n_days=6, end_date=None):
    if end_date is None:
        end_date = date.today()
    return [end_date - timedelta(days=offset) for offset in range(n_days, -1, -1)]

def format_dates(dates_list, fmt="%Y-%m-%d"):
    return [d.strftime(fmt) for d in dates_list]

## Getting web page soup
def get_page_soup(url, headers=HEADERS, timeout=10):
    resp = requests.get(url, headers=headers, timeout=timeout)
    resp.raise_for_status()
    return BeautifulSoup(resp.text, "html.parser")

## Scrapers: Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat

# Kommersant scraper
def fetch_kom(rubrics, dates, output_file,
              base_url_template="https://www.kommersant.ru/archive/rubric/{rubric}/day/{date}"):
    results = {}
    for rubric in rubrics:
        daily = {}
        for dt in dates:
            url = base_url_template.format(rubric=rubric, date=dt)
            print(f"Fetching Kommersant: {url}")
            try:
                soup = get_page_soup(url)
                scripts = soup.find_all("script", type="application/ld+json")
                items = []
                for script in scripts:
                    raw = script.string
                    if not raw:
                        continue
                    try:
                        data = json.loads(raw)
                    except json.JSONDecodeError:
                        continue
                    for entry in data.get("itemListElement", []):
                        title = entry.get("name") or entry.get("headline")
                        link = entry.get("url")
                        if title and link and link not in {i['url'] for i in items}:
                            items.append({"title": title, "url": link})
                daily[dt] = items
            except Exception as e:
                daily[dt] = f"[ERROR] {e}"
        results[rubric] = daily

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"Saved Kommersant data to {output_file}")


# Vedomosti scraper
def fetch_ved(dates, output_file,
              base_url_template="https://www.vedomosti.ru/newspaper/{date}"):
    all_news = []
    for dt in dates:
        url = base_url_template.format(date=dt)
        print(f"Fetching Vedomosti: {url}")
        try:
            soup = get_page_soup(url)
            for item in soup.select("li.waterfall__item"):
                a = item.select_one("a.waterfall__item-title")
                if not a:
                    continue
                title = a.get_text(strip=True)
                href = a.get("href", "")
                full_url = href if href.startswith("http") else f"https://www.vedomosti.ru{href}"
                all_news.append({"date": dt, "title": title, "url": full_url})
        except Exception as e:
            all_news.append({"date": dt, "error": str(e)})

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_news, f, ensure_ascii=False, indent=2)
    print(f"Saved Vedomosti data to {output_file}")

# RBC scraper

def fetch_rbc(rubrics, dates, output_file, base_url_template="https://www.rbc.ru/{rubric}/?utm_source=topline"):
    # Mapping of Russian month names (genitive case) to month numbers
    ru_months = {
        'января': 1, 'февраля': 2, 'марта': 3, 'апреля': 4,
        'мая': 5, 'июня': 6, 'июля': 7, 'августа': 8,
        'сентября': 9, 'октября': 10, 'ноября': 11, 'декабря': 12
    }
    today = date.today()
    all_news = []
    for rubric in rubrics:
        url = base_url_template.format(rubric=rubric)
        print(f"Fetching RBC, {rubric}: {url}")
        soup = get_page_soup(url)
        # Find all news item containers (with schema.org NewsArticle)
        for item in soup.find_all(attrs={"itemscope": True}):
            itemtype = item.get("itemtype", "")
            if "NewsArticle" not in itemtype:
                continue
            name_meta = item.find("meta", {"itemprop": "name"})
            url_meta = item.find("meta", {"itemprop": "url"})
            date_span = item.find("span", {"class": "item__category"})
            if not name_meta or not url_meta or not date_span:
                continue
            title = name_meta.get("content", "").strip()
            url = url_meta.get("content", "").strip()
            date_text = date_span.get_text(strip=True)
            if not title or not url or not date_text:
                continue
            # Parse date_text to datetime.date
            news_date = None
            if any(month in date_text for month in ru_months):
                # Format like "28 мая, 17:52" (if not today)
                date_part = date_text.split(",")[0].strip()  # e.g. "28 мая"
                parts = date_part.split()
                if len(parts) >= 2:
                    try:
                        day = int(parts[0])
                    except ValueError:
                        continue
                    month_name = parts[1].lower()
                    if month_name not in ru_months:
                        continue
                    month = ru_months[month_name]
                    year = today.year
                    if len(parts) >= 3:
                        # If year is present in date_text
                        year_str = parts[2].replace("г.", "").strip()
                        if year_str.isdigit():
                            year = int(year_str)
                    try:
                        news_date = date(year, month, day)
                    except ValueError:
                        continue
                    # Adjust year if the date is in the future (e.g., last year's news in early January)
                    if news_date > today:
                        news_date = date(year - 1, month, day)
            else:
                # Only time given (e.g. "17:52"), assume today's date
                news_date = today
            if news_date is None:
                continue
            all_news.append({"title": title, "url": url, "date": news_date})
    # Filter news by allowed dates
    filtered_news = [item for item in all_news if item["date"] in dates]
    # Remove duplicates by URL
    unique_news = []
    seen_urls = set()
    for item in filtered_news:
        if item["url"] not in seen_urls:
            unique_news.append(item)
            seen_urls.add(item["url"])
    # Convert date objects to ISO format strings for JSON serialization
    for item in unique_news:
        if isinstance(item.get("date"), date):
            item["date"] = item["date"].isoformat()
    # Save results to JSON file
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(unique_news, f, ensure_ascii=False, indent=2)
    print(f"Saved RBC data to {output_file}")

# Agro investor scraper

def fetch_agro(dates, output_file, base_url_template="https://www.agroinvestor.ru/"):
    """Fetch news from Agroinvestor main page and save to JSON."""
    print("Fetching Agroinvestor: https://www.agroinvestor.ru/")
    soup = get_page_soup(base_url_template)
    news_list = []
    seen_links = set()
    # Mapping of Russian month names (in genitive case) to month numbers
    ru_months = {
        "января": 1, "февраля": 2, "марта": 3, "апреля": 4,
        "мая": 5, "июня": 6, "июля": 7, "августа": 8,
        "сентября": 9, "октября": 10, "ноября": 11, "декабря": 12
    }
    # Find all news items on the main page
    for anchor in soup.find_all("a", class_="news__item-desc"):
        title = anchor.get_text(strip=True)
        href = anchor.get("href")
        if not href:
            continue
        # Construct full URL for the news article
        url = urljoin(base_url_template, href.strip())
        # Find the date of the news (in a <time> tag following the title link)
        time_tag = anchor.find_next("time")
        if not time_tag:
            continue
        date_text = time_tag.get_text(strip=True).replace("\xa0", " ")
        if not date_text:
            continue
        # Parse the date text (e.g. "30 мая 2025") into a datetime.date object
        try:
            day_str, month_str, year_str = date_text.split()
            day = int(day_str)
            year = int(year_str)
        except Exception:
            # Skip if date format is unexpected
            continue
        month_str = month_str.lower()
        if month_str not in ru_months:
            continue
        month = ru_months[month_str]

        try: date_obj = datetime.date(year, month, day)
        except Exception:
            continue

        # Filter news by allowed dates and avoid duplicates by URL
        if date_obj in dates and url not in seen_links:
            news_list.append({
                "title": title,
                "link": url,
                "date": date_obj.isoformat()
            })
            seen_links.add(url)
    # Save the result to a JSON file
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(news_list, f, ensure_ascii=False, indent=2)
    print(f"Saved Agroinvestor data to {output_file}")

# RG.ru scraper

def fetch_rg(rubrics, dates, output_file,
             base_url_template="https://rg.ru/tema/ekonomika/{rubric}"):
    all_news = []
    for rubric in rubrics:
        url = base_url_template.format(rubric=rubric)
        print(f"Fetching RG, {rubric}: {url}")
        soup = get_page_soup(url)
        for title_span in soup.find_all("span", class_="ItemOfListStandard_title__Ajjlf"):
            parent_a = title_span.find_parent("a")
            if not parent_a:
                continue
            href = parent_a.get("href", "").strip()
            if not href:
                continue
            full_url = href if href.startswith("http") else f"https://rg.ru{href}"

            date_a = title_span.find_previous("a", class_="ItemOfListStandard_datetime__GstJi")
            if not date_a:
                continue
            date_href = date_a.get("href", "").strip()
            parts = date_href.strip("/").split("/")  # ['2025','05','30',...]
            if len(parts) < 3:
                continue
            try:
                y, m, d = map(int, parts[:3])
                news_date = date(y, m, d)
            except ValueError:
                continue

            if news_date not in dates:
                continue

            all_news.append({
                "title": title_span.get_text(strip=True),
                "url": full_url,
                "date": news_date.isoformat()
            })

    unique = []
    seen = set()
    for item in all_news:
        if item["url"] not in seen:
            seen.add(item["url"])
            unique.append(item)

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(unique, f, ensure_ascii=False, indent=2)
    print(f"Saved RG data to {output_file}")

# RIA scraper

def fetch_ria(dates, output_file, base_url_template="https://ria.ru/economy/"):
    print("Fetching RIA: https://ria.ru/economy/")
    soup = get_page_soup(base_url_template)
    collected = []

    # Each news item has <a itemprop="url" href="..."></a>
    for a in soup.find_all("a", itemprop="url"):
        href = a.get("href", "").strip()
        if not href:
            continue
        full_url = href if href.startswith("http") else f"https://ria.ru{href}"

        # Next meta tag with itemprop="name" holds the title
        name_meta = a.find_next("meta", itemprop="name")
        if not name_meta:
            continue
        title = name_meta.get("content", "").strip()
        if not title:
            continue

        # Extract date from the URL path: "/YYYYMMDD/..."
        parsed = urlparse(full_url)
        parts = parsed.path.lstrip("/").split("/")
        if not parts or len(parts[0]) != 8 or not parts[0].isdigit():
            continue
        y, m, d = int(parts[0][:4]), int(parts[0][4:6]), int(parts[0][6:8])
        try:
            news_date = date(y, m, d)
        except ValueError:
            continue

        # Filter by provided dates
        if news_date in dates:
            collected.append({
                "title": title,
                "url": full_url,
                "date": news_date.isoformat()
            })

    # Remove duplicates by URL
    unique = []
    seen = set()
    for item in collected:
        if item["url"] not in seen:
            seen.add(item["url"])
            unique.append(item)

    # Save to JSON
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(unique, f, ensure_ascii=False, indent=2)

    print(f"Saved RIA data to {output_file}")


# Autostat scraper

def fetch_autostat(dates, output_file,
                   rubrics=[21, 8, 13, 70, 71],
                   base_url_template="https://m.autostat.ru/news/themes-{rubric}/"):

    if dates is None:
        raise ValueError("Argument 'dates' must be provided as a list of datetime.date objects.")

    all_collected = []
    seen_urls = set()

    ru_months = {
        'января': 1, 'февраля': 2, 'марта': 3, 'апреля': 4,
        'мая': 5, 'июня': 6, 'июля': 7, 'августа': 8,
        'сентября': 9, 'октября': 10, 'ноября': 11, 'декабря': 12
    }
    today = date.today()
    yesterday = today - timedelta(days=1)

    for rubric in rubrics:
        url = base_url_template.format(rubric=rubric)
        print(f"Fetching Autostat, {rubric}: {url}")
        soup = get_page_soup(url)
        if not soup:
            print(f"  (!) Failed to retrieve or parse page for rubric {rubric}")
            continue

        titles = soup.find_all("p", class_="Block-title")
        if not titles:
            print(f"    (!) No <p class='Block-title'> elements found on {url}")
            continue

        for title_p in titles:
            title = title_p.get_text(strip=True)
            if not title:
                continue

            link_a = title_p.find_parent("a", class_="Block-link")
            if not link_a:
                continue
            href = link_a.get("href", "").strip()
            if not href:
                continue
            full_url = urljoin("https://www.autostat.ru", href)

            date_p = title_p.find_next("p", class_="Block-date")
            if not date_p:
                continue
            date_text = date_p.get_text(strip=True)  # e.g. "Сегодня, 15:48" or "28 мая, 15:48"
            date_part = date_text.split(",")[0].strip().lower()

            if date_part == "сегодня":
                news_date = today
            elif date_part == "вчера":
                news_date = yesterday
            else:
                parts = date_part.split()
                if len(parts) != 2:
                    continue
                day_str, month_str = parts
                try:
                    day = int(day_str)
                    month = ru_months.get(month_str)
                    if not month:
                        continue
                    news_date = date(today.year, month, day)
                    if news_date > today:
                        news_date = date(today.year - 1, month, day)
                except Exception:
                    continue

            if news_date in dates and full_url not in seen_urls:
                all_collected.append({
                    "title": title,
                    "url": full_url,
                    "date": news_date.isoformat()
                })
                seen_urls.add(full_url)

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_collected, f, ensure_ascii=False, indent=2)

    print(f"Saved Autostat data to {output_file}")

# Parameters
days_before = 3
dates = get_last_dates(days_before)
dates_kom = format_dates(dates, fmt="%Y-%m-%d")
dates_ved = format_dates(dates, fmt="%Y/%m/%d")

rubrics_kom_rus = [3, 4, 40]
rubrics_kom_world = [3, 5]
rubrics_kom_prices = [41]
rubrics_rbc = ["economics", "business", "finances"]
rubrics_rg = ["politekonom", "industria", "business", "finansy", "kazna", "rabota", "pensii", "vnesh", "apk", "tovary", "turizm"]
rubrics_auto = [21, 8, 13, 70, 71]

# Fetching
fetch_kom(rubrics_kom_rus, dates_kom, "kom_rus.json")
fetch_kom(rubrics_kom_world, dates_kom, "kom_world.json")
fetch_kom(rubrics_kom_prices, dates_kom, "kom_prices.json")
fetch_ved(dates_ved, "ved.json")
fetch_rbc(rubrics_rbc, dates, "rbc.json")
fetch_agro(dates, "agro.json")
fetch_rg(rubrics_rg, dates, "rg.json")
fetch_ria(dates, "ria.json")
fetch_autostat(dates, "autostat.json", rubrics_auto)

# Kommersant, Vedomosti, RBC, Agroinvestor, RG.ru, RIA, Autostat
section_to_files = {
    "world": [
        "kom_world.json",
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        "rg.json",
        "ria.json"
    ],
    "rus": [
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        "rg.json",
        "ria.json"
    ],
    "prices": [
        "kom_prices.json",
        "kom_rus.json",
        "ved.json",
        "rbc.json",
        "agro.json",
        "rg.json",
        "ria.json",
        "autostat.json"
    ]
}

#with open('autostat.json', encoding='utf-8') as f:
#    data = json.load(f)
#print(json.dumps(data, ensure_ascii=False, indent=2))

### Functions for google drive

def find_file_in_drive(file_name: str) -> str:
    # try in root
    try:
        resp_root = drive_service.files().list(
            q=f"name = '{file_name}' and 'root' in parents and trashed = false",
            spaces="drive",
            fields="files(id, name)",
            pageSize=1
        ).execute()
    except HttpError as e:
        raise RuntimeError(f"Ошибка при запросе к Drive API (root search): {e}")

    items_root = resp_root.get("files", [])
    if items_root:
        return items_root[0]["id"]

    # try in sharedWithMe
    try:
        resp_shared = drive_service.files().list(
            q=f"name = '{file_name}' and trashed = false and sharedWithMe = true",
            spaces="drive",
            fields="files(id, name, parents)",
            pageSize=1
        ).execute()
    except HttpError as e:
        raise RuntimeError(f"Ошибка при запросе к Drive API (sharedWithMe search): {e}")

    items_shared = resp_shared.get("files", [])
    if items_shared:
        return items_shared[0]["id"]
    raise FileNotFoundError(f"File '{file_name}' not found in root or sharedWithMe.")

def download_text_file(fid: str) -> str:
    request = drive_service.files().get_media(fileId=fid)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        status, done = downloader.next_chunk()
    return fh.getvalue().decode("utf-8")

def save_to_drive(file_name: str, data):
    """
    Сохраняет `data` на Google Drive в файл file_name внутри папки MY_FOLDER_ID.
    Если data — строка, файл будет сохранён как plain text;
    иначе data считается JSON-совместимой структурой и сериализуется в JSON.

    Если файл с таким именем уже есть — перезаписывает, иначе создаёт новый.
    Возвращает метаданные созданного/обновлённого файла.
    """
    # 1) Подготовим байты и mimeType в зависимости от типа data
    if isinstance(data, str):
        # Сохраняем как plain text
        content_bytes = data.encode("utf-8")
        mime_type = "text/plain"
    else:
        # Считаем, что data — это Python-структура (dict, list и т.д.), сохраняем как JSON
        json_str = json.dumps(data, ensure_ascii=False, indent=2)
        content_bytes = json_str.encode("utf-8")
        mime_type = "application/json"

    # 2) Проверим, есть ли файл с таким именем в нужной папке
    existing_file_id = None
    try:
        resp = drive_service.files().list(
            q=f"name = '{file_name}' and '{MY_FOLDER_ID}' in parents and trashed = false",
            spaces="drive",
            fields="files(id, name)",
            pageSize=1
        ).execute()
        items = resp.get("files", [])
        if items:
            existing_file_id = items[0]["id"]
    except Exception as e:
        print("Warning: не удалось проверить существование файла в Drive:", e)

    # 3) Подготовим медиаконтент
    fh = io.BytesIO(content_bytes)
    media = MediaIoBaseUpload(fh, mimetype=mime_type, resumable=False)

    if existing_file_id:
        # 4a) Если файл уже есть — перезапишем его
        try:
            updated = drive_service.files().update(
                fileId=existing_file_id,
                media_body=media
            ).execute()
            print(f"Файл '{file_name}' обновлён (ID={updated['id']}).")
            return updated
        except Exception as e:
            print(f"Ошибка при обновлении файла '{file_name}': {e}")
            raise
    else:
        # 4b) Если файла нет — создадим новый в вашей папке
        file_metadata = {
            "name": file_name,
            "parents": [MY_FOLDER_ID],
            "mimeType": mime_type
        }
        try:
            created = drive_service.files().create(
                body=file_metadata,
                media_body=media,
                fields="id, webViewLink"
            ).execute()
            print(f"Создан новый файл '{file_name}' (ID={created['id']}).")
            return created
        except Exception as e:
            print(f"Ошибка при создании файла '{file_name}': {e}")
            raise

# drive.mount('/content/drive')

### Prompts

#file_path = '/content/drive/MyDrive/news lists, prompt beginning.txt'

file_id = find_file_in_drive("news lists, prompt beginning.txt")

try:
    prompt_list_start = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    prompt_list_start = ""

#try:
#    with open(file_path, 'r', encoding='utf-8') as f:
#        propmt_list_start = f.read()
#except FileNotFoundError:
#    print(f"Error: no file found (path: {file_path})")
#except Exception as e:
#    print(f"Error while reading file: {e}")

file_id = find_file_in_drive("bullets, prompt beginning.txt")

try:
    prompt_bullets_start = download_text_file(file_id)
except Exception as e:
    print("Ошибка при скачивании файла:", e)
    prompt_bullets_start = ""

#try:
#    with open(file_path, 'r', encoding='utf-8') as f:
#        prompt_bullets_start = f.read()
#except FileNotFoundError:
#    print(f"Error: no file found (path: {file_path})")
#except Exception as e:
#    print(f"Error while reading file: {e}")

section_to_continue_prompt = {
    "world": [
        'Пожалуйста, просмотри АБСОЛЮТНО ВСЕ НОВОСТИ в приложенном файле и отбери из них только те, что строго соответствуют критериям и могут быть включены в нумерованный список для раздела по мировой экономике.'
    ],
    "rus": [
        'Пожалуйста, просмотри АБСОЛЮТНО ВСЕ НОВОСТИ в приложенном файле и отбери из них только те, что строго соответствуют критериям и могут быть включены в нумерованный список для раздела по россиийской экономике.'
    ],
    "prices": [
        'Пожалуйста, просмотри АБСОЛЮТНО ВСЕ НОВОСТИ в приложенном файле и отбери из них только те, что строго соответствуют критериям и могут быть включены в нумерованный список для раздела по новостям, релевантным для динамики российских цен.'
    ]
}
prompt_list_finish = 'Пришли мне json файл, в котором останутся только новости, соответствующие требованиям. В json оставь только название и полную ссылку. ОЧЕНЬ ВАЖНО: В ОТВЕТ НЕ ПРИСЫЛАЙ НИЧЕГО КРОМЕ JSON ФАЙЛА.'

section_to_finish_bullets_prompt = {
    "world": [
        'Пожалуйста, подготовь 3 буллита для раздела по мировой экономике в соответствии с требованиями и пришли итоговый результат в таком формате: сначала буллиты, потом нумерованный список, оформленный в соответствии с требованиями к оформлению буллитов.'
    ],
    "rus": [
        'Пожалуйста, подготовь 3 буллита для раздела по россиийской экономике в соответствии с требованиями и пришли итоговый результат в таком формате: сначала буллиты, потом нумерованный список, оформленный в соответствии с требованиями к оформлению буллитов.'
    ],
    "prices": [
        'Пожалуйста, подготовь 3 буллита для раздела по по новостям, релевантным для динамики российских цен, в соответствии с требованиями и пришли итоговый результат в таком формате: сначала буллиты, потом нумерованный список, оформленный в соответствии с требованиями к оформлению буллитов.'
    ]
}

def create_news_lists(section):
    if section not in section_to_files:
        raise ValueError(f"Section '{section}' unknown.")

    file_name = f"{section}.txt"

    # Если сегодня не суббота, пробуем прочитать существующий файл <section>.txt
    if datetime.today().weekday() != 5:  # 5 = Saturday
        folder_id_input = "root"

        try:
          file_id = find_file_in_drive(file_name)
          list_start = download_text_file(file_id)
        except Exception as e:
          print(f"Warning, no file found.")
          list_start = ""

        #drive_folder = "/content/drive/MyDrive"
        #file_name = f"{section}.txt"
        #file_path = f"{drive_folder}/{file_name}"

        #try:
        #    with open(file_path, "r", encoding="utf-8") as f:
        #        list_start = f.read()
        #except FileNotFoundError:
        #    print(f"Warning: file not found: {file_path}")
        #    list_start = ""
        #except Exception as e:
        #    print(f"Error reading file: {e}")
        #    list_start = ""

    # Достаём список JSON-файлов и соответствующий prompt_list_continue
    json_files = section_to_files[section]
    prompt_list_continue = section_to_continue_prompt[section]

    combined_text_parts = []
    for json_filename in json_files:
        base_name, ext = os.path.splitext(json_filename)
        if ext.lower() != ".json":
            print(f"Пропускаем '{json_filename}', т.к. не .json-файл.")
            continue

        try:
            with open(json_filename, 'r', encoding='utf-8') as f:
                news_data = json.load(f)
        except FileNotFoundError:
            print(f"Файл '{json_filename}' не найден. Пропускаем.")
            continue
        except json.JSONDecodeError as e:
            print(f"Ошибка JSON в '{json_filename}': {e}. Пропускаем.")
            continue

        news_json_string = json.dumps(news_data, ensure_ascii=False, indent=2)

        raw_parts = [
            prompt_list_start,
            prompt_list_continue,
            prompt_list_finish,
            news_json_string
        ]

        prompt_parts = []
        for part in raw_parts:
            if isinstance(part, list):
                # Если это список, склеиваем через переносы строк
                prompt_parts.append("\n".join(part))
            else:
                prompt_parts.append(str(part))

        try:
            response = model_obj.generate_content(prompt_parts)
        except Exception as e:
            print(f"Error in model.generate_content for '{json_filename}': {e}.")
            continue

        header = f"=== {base_name} ({section}) ===\n"
        combined_text_parts.append(header + response.text + "\n\n")

    if not combined_text_parts:
        print(f"For section '{section}', zero JSONs were successfully processed.")
        return

    # Объединяем прочитанное ранее (list_start) с новыми частями
    all_text = list_start + "".join(combined_text_parts)

    # Записываем итог в тот же файл <section>.txt на Google Drive
    save_to_drive(file_name, all_text)

create_news_lists("world")
time.sleep(60)
create_news_lists("rus")
time.sleep(60)
create_news_lists("prices")

def create_bullets(section):
    if section not in section_to_files:
        raise ValueError(f"Section '{section}' unknown.")

    list_file = f"{section}_bullets.txt"
    file_id = find_file_in_drive(list_file)

    try:
        list_content = download_text_file(file_id)
    except Exception as e:
        print("Ошибка при скачивании файла:", e)

    # Берём соответствующий prompt для завершения
    prompt_bullets_finish = section_to_finish_bullets_prompt[section]

    # Формируем prompt_parts
    raw_parts = [
        prompt_bullets_start,
        prompt_bullets_finish,
        list_content
    ]

    prompt_parts = []
    for part in raw_parts:
            if isinstance(part, list):
                # Если это список, склеиваем через переносы строк
                prompt_parts.append("\n".join(part))
            else:
                prompt_parts.append(str(part))

    try:
        response = model_obj.generate_content(prompt_parts)
    except Exception as e:
        print(f"Error in model.generate_content: {e}")
        return

    file_name = f"report_{section}.txt"
    save_to_drive(file_name, response.text)

if datetime.today().weekday() == 3:
  create_bullets("world")
  create_bullets("rus")
  create_bullets("prices")
